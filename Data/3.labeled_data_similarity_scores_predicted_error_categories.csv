#,id,library,aspect,search_word,aspect_question,so_id,so_question_title,so_question_body,so_answer,question_keyword,explanation,error reason,label,error_category,error_source,expected_answer,callid,sim_ques_ans_how,sim_ques_ans_why,sim_ques_ans_really,sim_ques_how_why,sim_ques_how_really,sim_ques_why_really,sim_ans_base_how,sim_ans_base_why,sim_ans_base_really,sim_ans_how_why,sim_ans_how_really,sim_ans_why_really,sim_ques_ans_how_qaqa,sim_ques_ans_why_qaqa,sim_ques_ans_really_qaqa,sim_ques_how_why_qaqa,sim_ques_how_really_qaqa,sim_ques_why_really_qaqa,sim_ans_base_how_qaqa,sim_ans_base_why_qaqa,sim_ans_base_really_qaqa,sim_ans_how_why_qaqa,sim_ans_how_really_qaqa,sim_ans_why_really_qaqa,id+kw,predict_probo_0,predict_probo_1,predicted_label,correct
1,1,Spacy,Ease of use,simple,Ease of use aspect,62766608,Is there a simple way to get the position of a token in sequence with spacy?,"I want to the start index and the end index of every token in a sequence. Is there a simple way to do that with spacy?
","import spacy

text = 'Brown is a nice guy'
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)

for token in doc:
  print(token.text, token.idx, token.idx + len(token.text) - 1)",installation is easy,Installing and loading the Spacy library is a straightforward process that can be accomplished in just a few lines of code.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014104560978,0.66,0.64,0.62,0.51,0.5,0.55,0.26,0.45,0.39,0.67,0.29,0.25,0.53,0.57,0.51,0.53,0.42,0.56,0.79,0.65,0.59,0.59,0.59,0.62, 1installation is easy ,0,1,1,0
2,1,Spacy,Ease of use,simple,Ease of use aspect,62766608,Is there a simple way to get the position of a token in sequence with spacy?,"I want to the start index and the end index of every token in a sequence. Is there a simple way to do that with spacy?
","import spacy

text = 'Brown is a nice guy'
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)

for token in doc:
  print(token.text, token.idx, token.idx + len(token.text) - 1)",spacy handles parsing,"Spacy's ability to automatically parse natural language text allows users to focus on specific tasks and analyses, rather than worrying about the underlying processing and parsing mechanisms.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706014105098675,0.7,0.5,0.56,0.83,0.79,0.8,0.74,0.72,0.67,0.76,0.77,0.75,0.56,0.45,0.74,0.58,0.61,0.72,0.5,0.71,0.59,0.47,0.45,0.64, 1spacy handles parsing ,0.66,0.34,0,0
3,1,Spacy,Ease of use,simple,Ease of use aspect,62766608,Is there a simple way to get the position of a token in sequence with spacy?,"I want to the start index and the end index of every token in a sequence. Is there a simple way to do that with spacy?
","import spacy

text = 'Brown is a nice guy'
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)

for token in doc:
  print(token.text, token.idx, token.idx + len(token.text) - 1)",spacy is user-friendly,"Spacy provides a simple way to get the position of a token in sequence, making it a user-friendly library for natural language processing tasks.",,1,,,,C20230706014104098379,0.59,0.53,0.66,0.83,0.53,0.58,0.35,0.86,0.83,0.33,0.31,0.79,0.62,0.73,0.72,0.48,0.46,0.75,0.9,0.64,0.73,0.64,0.7,0.71, 1spacy is user-friendly ,0.02,0.98,1,1
4,2,Spacy,Ease of use,easy,Ease of use aspect,56694713,In spaCy is there a way to extract the sentence the entity has been extracted from?,"considering the following sentence:

""the quick brown fox jumps over the lazy dog""
and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:

""brown"" , ""the quick brown fox jumps over the lazy dog""","Yes, there is a way to do this in Spacy. You have to iterate over the entity Span objects and extract the sentence from each Span object. Here is an example:

    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)",iterating over entity spans,"To extract the sentence an entity has been extracted from, one needs to iterate over entity Span objects and extract each sentence from them.",,1,,,,C20230706014106131529,0.73,0.82,0.75,0.78,0.74,0.9,0.85,0.83,0.77,0.8,0.78,0.8,0.6,0.71,0.8,0.57,0.73,0.56,0.78,0.76,0.57,0.71,0.58,0.63, 2iterating over entity spans ,0.02,0.98,1,1
5,2,Spacy,Ease of use,easy,Ease of use aspect,56694713,In spaCy is there a way to extract the sentence the entity has been extracted from?,"considering the following sentence:

""the quick brown fox jumps over the lazy dog""
and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:

""brown"" , ""the quick brown fox jumps over the lazy dog""","Yes, there is a way to do this in Spacy. You have to iterate over the entity Span objects and extract the sentence from each Span object. Here is an example:

    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)",not easy for non-programmer,"For someone with limited technical skills or no programming experience, using Spacy for entity and sentence extraction may not be easy.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014106660748,0.66,0.67,0.64,0.54,0.4,0.81,0.59,0.76,0.78,0.53,0.6,0.87,0.66,0.78,0.64,0.48,0.41,0.74,0.56,0.66,0.5,0.6,0.7,0.59, 2not easy for non-programmer ,0.11,0.89,1,0
6,2,Spacy,Ease of use,easy,Ease of use aspect,56694713,In spaCy is there a way to extract the sentence the entity has been extracted from?,"considering the following sentence:

""the quick brown fox jumps over the lazy dog""
and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:

""brown"" , ""the quick brown fox jumps over the lazy dog""","Yes, there is a way to do this in Spacy. You have to iterate over the entity Span objects and extract the sentence from each Span object. Here is an example:

    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)",requires programming skills,Using Spacy for sentence and entity extraction requires expertise in coding.,Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014105602836,0.71,0.76,0.62,0.82,0.86,0.79,0.62,0.84,0.71,0.68,0.74,0.84,0.57,0.84,0.74,0.62,0.91,0.62,0.62,0.83,0.79,0.62,0.69,0.75, 2requires programming skills ,0,1,1,0
7,2,Spacy,Ease of use,easy,Ease of use aspect,56694713,In spaCy is there a way to extract the sentence the entity has been extracted from?,"considering the following sentence:

""the quick brown fox jumps over the lazy dog""
and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:

""brown"" , ""the quick brown fox jumps over the lazy dog""","Yes, there is a way to do this in Spacy. You have to iterate over the entity Span objects and extract the sentence from each Span object. Here is an example:

    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)",useful for nlp,"For those with coding proficiency, Spacy can offer useful tools for natural language processing and text analysis.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014107386977,0.67,0.69,0.86,0.54,0.56,0.8,0.33,0.77,0.72,0.52,0.62,0.75,0.64,0.46,0.49,0.46,0.56,0.67,0.65,0.72,0.73,0.7,0.65,0.86, 2useful for nlp ,0.04,0.96,1,0
8,3,Spacy,Ease of use,easy,Ease of use aspect,74799295,How to extract cities with Spacy / Can't load French model,"I know it's perhaps an easy question but i'm not very familiar with Spacy.

So i'm trying to extract cities in a text file.","I just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. It likely does not form correct parses of your input text.

I would not use nltk or locationtagger for this task.

Instead, download a proper spaCy model for French:

python3 -m spacy download fr_core_news_{sm|md|lg|trf}
Read spaCy's documentation on named entity recognition [1]. This includes information about identifying geopolitical entities (""GPE"").

The default spaCy models will tag cities, states/provinces/districts, and countries under the ""GPE"" tag. If you are interested only in the cities, then, you should filter the found GPEs against the data in locationtagger's City-Region-Locations.csv.

Additionally, you may wish to segment the text by paragraph and use spaCy's nlp.pipe to process paragraphs in parallel.",data filtering,"When extracting cities using Spacy, filtering out non-city GPEs may be necessary, using external data sources if available.",,1,,,,C20230706014331552465,0.72,0.72,0.72,0.83,0.84,0.75,0.72,0.83,0.83,0.74,0.74,0.81,0.77,0.64,0.53,0.63,0.65,0.53,0.78,0.86,0.73,0.79,0.72,0.74, 3data filtering ,0.19,0.81,1,1
9,3,Spacy,Ease of use,easy,Ease of use aspect,74799295,How to extract cities with Spacy / Can't load French model,"I know it's perhaps an easy question but i'm not very familiar with Spacy.

So i'm trying to extract cities in a text file.","I just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. It likely does not form correct parses of your input text.

I would not use nltk or locationtagger for this task.

Instead, download a proper spaCy model for French:

python3 -m spacy download fr_core_news_{sm|md|lg|trf}
Read spaCy's documentation on named entity recognition [1]. This includes information about identifying geopolitical entities (""GPE"").

The default spaCy models will tag cities, states/provinces/districts, and countries under the ""GPE"" tag. If you are interested only in the cities, then, you should filter the found GPEs against the data in locationtagger's City-Region-Locations.csv.

Additionally, you may wish to segment the text by paragraph and use spaCy's nlp.pipe to process paragraphs in parallel.",dependency issues,"Tools like locationtagger may have hardcoded dependencies on certain Spacy models, limiting their usefulness for non-English text.",,1,,,,C20230706014330311473,0.6,0.6,0.31,0.4,0.46,0.37,0.57,0.67,0.38,0.5,0.48,0.5,0.37,0.44,0.37,0.33,0.4,0.58,0.31,0.51,0.51,0.72,0.57,0.77, 3dependency issues ,0.28,0.72,1,1
10,3,Spacy,Ease of use,easy,Ease of use aspect,74799295,How to extract cities with Spacy / Can't load French model,"I know it's perhaps an easy question but i'm not very familiar with Spacy.

So i'm trying to extract cities in a text file.","I just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. It likely does not form correct parses of your input text.

I would not use nltk or locationtagger for this task.

Instead, download a proper spaCy model for French:

python3 -m spacy download fr_core_news_{sm|md|lg|trf}
Read spaCy's documentation on named entity recognition [1]. This includes information about identifying geopolitical entities (""GPE"").

The default spaCy models will tag cities, states/provinces/districts, and countries under the ""GPE"" tag. If you are interested only in the cities, then, you should filter the found GPEs against the data in locationtagger's City-Region-Locations.csv.

Additionally, you may wish to segment the text by paragraph and use spaCy's nlp.pipe to process paragraphs in parallel.",model capabilities,"It's important to choose the appropriate Spacy model for the task at hand, based on its features and named entity recognition capabilities.",,1,,,,C20230706014330837706,0.6,0.45,0.55,0.64,0.69,0.7,0.6,0.55,0.47,0.56,0.6,0.72,0.46,0.68,0.34,0.62,0.29,0.42,0.65,0.68,0.26,0.69,0.55,0.32, 3model capabilities ,0.21,0.79,1,1
11,3,Spacy,Ease of use,easy,Ease of use aspect,74799295,How to extract cities with Spacy / Can't load French model,"I know it's perhaps an easy question but i'm not very familiar with Spacy.

So i'm trying to extract cities in a text file.","I just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. It likely does not form correct parses of your input text.

I would not use nltk or locationtagger for this task.

Instead, download a proper spaCy model for French:

python3 -m spacy download fr_core_news_{sm|md|lg|trf}
Read spaCy's documentation on named entity recognition [1]. This includes information about identifying geopolitical entities (""GPE"").

The default spaCy models will tag cities, states/provinces/districts, and countries under the ""GPE"" tag. If you are interested only in the cities, then, you should filter the found GPEs against the data in locationtagger's City-Region-Locations.csv.

Additionally, you may wish to segment the text by paragraph and use spaCy's nlp.pipe to process paragraphs in parallel.",spacy model availability,"Spacy requires a proper model for the language of interest, and these are not always readily available and may need to be downloaded.",,1,,,,C20230706014329838521,0.64,0.65,0.4,0.75,0.39,0.56,0.55,0.68,0.42,0.56,0.51,0.44,0.71,0.75,0.44,0.53,0.27,0.35,0.39,0.5,0.35,0.62,0.27,0.45, 3spacy model availability ,0.29,0.71,1,1
12,4,Spacy,Ease of use,simple,Ease of use aspect,57199811,Create a Spacy pipeline with my own Tokeniser?,"What puzzles me is that a spacy ""pipeline"" is a sequence of processing steps, but the very first step which converts the original data into a spacy document is ""hidden"". Unlike all the other steps it cannot be accessed from the pipeline, I think. So what if I want to replace it, change it, or create a pipeline with my own Tokenizer, what is the proper way to do this?","The reason the tokenizer isn't part of the regular pipeline is because it's ""special"": There can only really be one, and while all other pipeline components take a Doc and return it, the tokenizer takes a string of text and turns it into a Doc.

However, nlp.tokenizer is writable, so you can either create your own Tokenizer class from scratch, or even replace it with an entirely custom function. Here's a super simple example that shows the idea:
",creating a custom tokenizer,Developers can use a simple example provided in the answer to create their own Tokenizer that suits their needs.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706014333140120,0.74,0.64,0.59,0.81,0.51,0.45,0.59,0.77,0.6,0.51,0.37,0.6,0.38,0.64,0.64,0.4,0.3,0.53,0.57,0.57,0.6,0.44,0.69,0.52, 4creating a custom tokenizer ,0.64,0.36,0,0
13,4,Spacy,Ease of use,simple,Ease of use aspect,57199811,Create a Spacy pipeline with my own Tokeniser?,"What puzzles me is that a spacy ""pipeline"" is a sequence of processing steps, but the very first step which converts the original data into a spacy document is ""hidden"". Unlike all the other steps it cannot be accessed from the pipeline, I think. So what if I want to replace it, change it, or create a pipeline with my own Tokenizer, what is the proper way to do this?","The reason the tokenizer isn't part of the regular pipeline is because it's ""special"": There can only really be one, and while all other pipeline components take a Doc and return it, the tokenizer takes a string of text and turns it into a Doc.

However, nlp.tokenizer is writable, so you can either create your own Tokenizer class from scratch, or even replace it with an entirely custom function. Here's a super simple example that shows the idea:
",tokenizer is special,The tokenizer is separate from the regular pipeline because it takes in text as input and converts it into a Doc.,,1,,,,C20230706014332048192,0.65,0.52,0.69,0.47,0.57,0.58,0.82,0.74,0.72,0.74,0.75,0.75,0.64,0.39,0.79,0.45,0.51,0.61,0.67,0.3,0.46,0.48,0.73,0.48, 4tokenizer is special ,0.28,0.72,1,1
14,4,Spacy,Ease of use,simple,Ease of use aspect,57199811,Create a Spacy pipeline with my own Tokeniser?,"What puzzles me is that a spacy ""pipeline"" is a sequence of processing steps, but the very first step which converts the original data into a spacy document is ""hidden"". Unlike all the other steps it cannot be accessed from the pipeline, I think. So what if I want to replace it, change it, or create a pipeline with my own Tokenizer, what is the proper way to do this?","The reason the tokenizer isn't part of the regular pipeline is because it's ""special"": There can only really be one, and while all other pipeline components take a Doc and return it, the tokenizer takes a string of text and turns it into a Doc.

However, nlp.tokenizer is writable, so you can either create your own Tokenizer class from scratch, or even replace it with an entirely custom function. Here's a super simple example that shows the idea:
",writable nlp.tokenizer,The nlp.tokenizer can be overwritten with either a custom Tokenizer class or a new tokenizer function.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706014332603482,0.8,0.72,0.57,0.81,0.66,0.7,0.87,0.76,0.61,0.84,0.65,0.69,0.62,0.69,0.65,0.9,0.6,0.7,0.58,0.61,0.57,0.65,0.49,0.59, 4writable nlp.tokenizer ,0.78,0.22,0,0
15,5,Spacy,Ease of use,difficult,Ease of use aspect,67789544,Given a word can we get all possible lemmas for it using Spacy?,"The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.

Why am I doing this?

I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using en_core_web_sm. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.

So in short, I would like to replicate the behaviour of token._lemma for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.","I found it difficult to get lemmas and inflections directly out of spaCy without first constructing an example sentence to give it context. This wasn't ideal, so I looked further and found LemmaInflect did this very well.",difficult to get lemmas,The user found it difficult to get the desired output of all possible lemmas for a given word without first constructing an example sentence to provide context.,,1,,,,C20230706014333678890,0.71,0.61,0.62,0.79,0.77,0.83,0.72,0.61,0.81,0.75,0.74,0.77,0.77,0.74,0.73,0.8,0.62,0.54,0.87,0.79,0.55,0.83,0.61,0.7, 5difficult to get lemmas ,0.01,0.99,1,1
16,5,Spacy,Ease of use,difficult,Ease of use aspect,67789544,Given a word can we get all possible lemmas for it using Spacy?,"The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.

Why am I doing this?

I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using en_core_web_sm. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.

So in short, I would like to replicate the behaviour of token._lemma for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.","I found it difficult to get lemmas and inflections directly out of spaCy without first constructing an example sentence to give it context. This wasn't ideal, so I looked further and found LemmaInflect did this very well.",found workaround with lemmainflect,The user found a solution by using LemmaInflect instead of Spacy to get the desired output of lemmas and inflections.,,1,,,,C20230706014334528606,0.65,0.74,0.58,0.7,0.76,0.74,0.66,0.77,0.73,0.77,0.78,0.85,0.67,0.65,0.69,0.9,0.66,0.62,0.73,0.73,0.65,0.76,0.71,0.91, 5found workaround with lemmainflect ,0.05,0.95,1,1
17,5,Spacy,Ease of use,difficult,Ease of use aspect,67789544,Given a word can we get all possible lemmas for it using Spacy?,"The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.

Why am I doing this?

I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using en_core_web_sm. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.

So in short, I would like to replicate the behaviour of token._lemma for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.","I found it difficult to get lemmas and inflections directly out of spaCy without first constructing an example sentence to give it context. This wasn't ideal, so I looked further and found LemmaInflect did this very well.",not straightforward task,"Based on the conversation, it seems that using Spacy solely for this specific task may require some additional knowledge or tools beyond the library itself.",wrong labeling,0,Generic Error,Mixed,,C20230706014334111237,0.69,0.69,0.46,0.78,0.96,0.8,0.78,0.79,0.65,0.83,0.74,0.8,0.76,0.78,0.5,0.55,0.66,0.68,0.53,0.72,0.59,0.58,0.59,0.68, 5not straightforward task ,0.56,0.44,0,1
18,6,Spacy,Ease of use,difficult,Ease of use aspect,48980120,Is it possible to parse emojis using spaCy?,"Is it possible to tokenize emojis like :), :(, ;~( properly using the spaCy Python library? e.g. If I run the following code:

import spacy

nlp = spacy.load('en')
doc = nlp(""Hello bright world :)"")
And then visualize the doc with displaCy:
It incorrectly parses world :) as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.

edit: Found the following: https://github.com/ines/spacymoji but I think it only supports Unicode emojis like ‚Äö√Ñ√∂‚àö‚à´¬¨√Ü and not ASCII ones like :)","Yes, spaCy actually includes a pretty comprehensive list of text-based emoticons as part of its tokenizer exceptions. So using your example above and printing the individual tokens, the emoticon is tokenized correctly:

doc = nlp(""Hello bright world :)"")
print([token.text for token in doc])
# ['Hello', 'bright', 'world', ':)']
I think what happens here is that you actually came across an interesting (maybe non-ideal) edge case with the displacy defaults. To avoid very long dependency arcs for punctuation, the collapse_punct setting defaults to True. This means that when the visualisation is rendered, punctuation is merged onto the preceding token. Punctuation is identified by checking whether the token's is_punct attribute returns True ‚Äö√Ñ√∂‚àö√ë‚àö¬® which also happens to be the case for "":)"".

In your example, you can work around this by setting collapse_punct to False in the options passed to displacy.serve:

displacy.serve(doc, style='dep', options={'collapse_punct': False})
(The displaCy visualizer should probably include an exception for emoticons when merging punctuation. This is currently difficult, because spaCy doesn't have an is_emoji or is_symbol flag. However, it might be a nice addition in the future ‚Äö√Ñ√∂‚àö√ë‚àö¬® you can vote for it on this thread.)",default collapse_punct setting,The displacy default setting of collapse_punct to True caused the emoticon to be merged with the preceding token in the visualization.,,1,,,,C20230706014335560465,0.61,0.73,0.67,0.67,0.85,0.79,0.65,0.83,0.84,0.73,0.75,0.83,0.46,0.72,0.59,0.36,0.68,0.54,0.72,0.78,0.67,0.77,0.93,0.77, 6default collapse_punct setting ,0.09,0.91,1,1
19,6,Spacy,Ease of use,difficult,Ease of use aspect,48980120,Is it possible to parse emojis using spaCy?,"Is it possible to tokenize emojis like :), :(, ;~( properly using the spaCy Python library? e.g. If I run the following code:

import spacy

nlp = spacy.load('en')
doc = nlp(""Hello bright world :)"")
And then visualize the doc with displaCy:
It incorrectly parses world :) as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.

edit: Found the following: https://github.com/ines/spacymoji but I think it only supports Unicode emojis like ‚Äö√Ñ√∂‚àö‚à´¬¨√Ü and not ASCII ones like :)","Yes, spaCy actually includes a pretty comprehensive list of text-based emoticons as part of its tokenizer exceptions. So using your example above and printing the individual tokens, the emoticon is tokenized correctly:

doc = nlp(""Hello bright world :)"")
print([token.text for token in doc])
# ['Hello', 'bright', 'world', ':)']
I think what happens here is that you actually came across an interesting (maybe non-ideal) edge case with the displacy defaults. To avoid very long dependency arcs for punctuation, the collapse_punct setting defaults to True. This means that when the visualisation is rendered, punctuation is merged onto the preceding token. Punctuation is identified by checking whether the token's is_punct attribute returns True ‚Äö√Ñ√∂‚àö√ë‚àö¬® which also happens to be the case for "":)"".

In your example, you can work around this by setting collapse_punct to False in the options passed to displacy.serve:

displacy.serve(doc, style='dep', options={'collapse_punct': False})
(The displaCy visualizer should probably include an exception for emoticons when merging punctuation. This is currently difficult, because spaCy doesn't have an is_emoji or is_symbol flag. However, it might be a nice addition in the future ‚Äö√Ñ√∂‚àö√ë‚àö¬® you can vote for it on this thread.)",limitations of spacy emoji support,GitHub repository for spaCy emojis only currently supports Unicode emojis and not ASCII ones.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706014337758480,0.65,0.87,0.62,0.61,0.75,0.63,0.86,0.8,0.78,0.73,0.85,0.7,0.71,0.59,0.7,0.49,0.56,0.47,0.5,0.7,0.63,0.59,0.64,0.83, 6limitations of spacy emoji support ,0.09,0.91,1,1
20,6,Spacy,Ease of use,difficult,Ease of use aspect,48980120,Is it possible to parse emojis using spaCy?,"Is it possible to tokenize emojis like :), :(, ;~( properly using the spaCy Python library? e.g. If I run the following code:

import spacy

nlp = spacy.load('en')
doc = nlp(""Hello bright world :)"")
And then visualize the doc with displaCy:
It incorrectly parses world :) as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.

edit: Found the following: https://github.com/ines/spacymoji but I think it only supports Unicode emojis like ‚Äö√Ñ√∂‚àö‚à´¬¨√Ü and not ASCII ones like :)","Yes, spaCy actually includes a pretty comprehensive list of text-based emoticons as part of its tokenizer exceptions. So using your example above and printing the individual tokens, the emoticon is tokenized correctly:

doc = nlp(""Hello bright world :)"")
print([token.text for token in doc])
# ['Hello', 'bright', 'world', ':)']
I think what happens here is that you actually came across an interesting (maybe non-ideal) edge case with the displacy defaults. To avoid very long dependency arcs for punctuation, the collapse_punct setting defaults to True. This means that when the visualisation is rendered, punctuation is merged onto the preceding token. Punctuation is identified by checking whether the token's is_punct attribute returns True ‚Äö√Ñ√∂‚àö√ë‚àö¬® which also happens to be the case for "":)"".

In your example, you can work around this by setting collapse_punct to False in the options passed to displacy.serve:

displacy.serve(doc, style='dep', options={'collapse_punct': False})
(The displaCy visualizer should probably include an exception for emoticons when merging punctuation. This is currently difficult, because spaCy doesn't have an is_emoji or is_symbol flag. However, it might be a nice addition in the future ‚Äö√Ñ√∂‚àö√ë‚àö¬® you can vote for it on this thread.)",setting collapse_punct to false,The solution to the issue with merging punctuation in the visualization is to set collapse_punct to False in the options passed to the displacy visualizer.,,1,,,,C20230706014336082249,0.74,0.79,0.5,0.67,0.49,0.3,0.64,0.86,0.74,0.65,0.6,0.78,0.61,0.62,0.47,0.57,0.48,0.41,0.78,0.7,0.66,0.75,0.74,0.8, 6setting collapse_punct to false ,0.02,0.98,1,1
21,6,Spacy,Ease of use,difficult,Ease of use aspect,48980120,Is it possible to parse emojis using spaCy?,"Is it possible to tokenize emojis like :), :(, ;~( properly using the spaCy Python library? e.g. If I run the following code:

import spacy

nlp = spacy.load('en')
doc = nlp(""Hello bright world :)"")
And then visualize the doc with displaCy:
It incorrectly parses world :) as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.

edit: Found the following: https://github.com/ines/spacymoji but I think it only supports Unicode emojis like ‚Äö√Ñ√∂‚àö‚à´¬¨√Ü and not ASCII ones like :)","Yes, spaCy actually includes a pretty comprehensive list of text-based emoticons as part of its tokenizer exceptions. So using your example above and printing the individual tokens, the emoticon is tokenized correctly:

doc = nlp(""Hello bright world :)"")
print([token.text for token in doc])
# ['Hello', 'bright', 'world', ':)']
I think what happens here is that you actually came across an interesting (maybe non-ideal) edge case with the displacy defaults. To avoid very long dependency arcs for punctuation, the collapse_punct setting defaults to True. This means that when the visualisation is rendered, punctuation is merged onto the preceding token. Punctuation is identified by checking whether the token's is_punct attribute returns True ‚Äö√Ñ√∂‚àö√ë‚àö¬® which also happens to be the case for "":)"".

In your example, you can work around this by setting collapse_punct to False in the options passed to displacy.serve:

displacy.serve(doc, style='dep', options={'collapse_punct': False})
(The displaCy visualizer should probably include an exception for emoticons when merging punctuation. This is currently difficult, because spaCy doesn't have an is_emoji or is_symbol flag. However, it might be a nice addition in the future ‚Äö√Ñ√∂‚àö√ë‚àö¬® you can vote for it on this thread.)",spacy tokenizer exceptions,"spaCy includes a list of text-based emoticons as part of its tokenizer exceptions, which means that the emoticons in question are tokenized correctly.",,1,,,,C20230706014335035118,0.84,0.76,0.71,0.88,0.71,0.76,0.73,0.84,0.75,0.8,0.87,0.75,0.67,0.59,0.53,0.69,0.53,0.58,0.78,0.74,0.63,0.91,0.83,0.79, 6spacy tokenizer exceptions ,0.12,0.88,1,1
22,7,Spacy,Ease of use,easy,Ease of use aspect,55087101,How to get all noun phrases in Spacy(Python),"I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:","Spacy clearly defines a noun chunk as:

A base noun phrase, or ""NP chunk"", is a noun phrase that does not permit other NPs to be nested within it ‚Äö√Ñ√∂‚àö√ë‚àö¬® so no NP-level coordination, no prepositional phrases, and no relative clauses."" (https://spacy.io/api/doc#noun_chunks)
If you process the dependency parse differently, allowing prepositional modifiers and nested phrases/chunks, then you can end up with what you're looking for.

I bet you could modify the existing spacy code fairly easily to do what you want:

https://github.com/explosion/spaCy/blob/06c6dc6fbcb8fbb78a61a2e42c1b782974bd43bd/spacy/lang/en/syntax_iterators.py",definition of noun chunk in spacy,"Spacy defines a noun chunk as a base noun phrase that does not permit other NPs to be nested within it ‚Äö√Ñ√¨ so no NP-level coordination, no prepositional phrases, and no relative clauses.",,1,,,,C20230706014338419647,0.62,0.69,0.59,0.77,0.52,0.53,0.83,0.71,0.33,0.7,0.43,0.62,0.64,0.7,0.73,0.91,0.61,0.61,0.59,0.71,0.6,0.89,0.87,0.92, 7definition of noun chunk in spacy ,0.01,0.99,1,1
23,7,Spacy,Ease of use,easy,Ease of use aspect,55087101,How to get all noun phrases in Spacy(Python),"I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:","Spacy clearly defines a noun chunk as:

A base noun phrase, or ""NP chunk"", is a noun phrase that does not permit other NPs to be nested within it ‚Äö√Ñ√∂‚àö√ë‚àö¬® so no NP-level coordination, no prepositional phrases, and no relative clauses."" (https://spacy.io/api/doc#noun_chunks)
If you process the dependency parse differently, allowing prepositional modifiers and nested phrases/chunks, then you can end up with what you're looking for.

I bet you could modify the existing spacy code fairly easily to do what you want:

https://github.com/explosion/spaCy/blob/06c6dc6fbcb8fbb78a61a2e42c1b782974bd43bd/spacy/lang/en/syntax_iterators.py",modifying dependency parse to extract noun phrases,Modifying the dependency parse will allow prepositional modifiers and nested phrases to be extracted as noun phrases.,wrong labeling,1,Generic Error,Mixed,,C20230706014338848778,0.75,0.8,0.83,0.87,0.86,0.93,0.78,0.8,0.72,0.81,0.75,0.86,0.64,0.76,0.69,0.94,0.92,0.98,0.74,0.87,0.71,0.8,0.85,0.82, 7modifying dependency parse to extract noun phrases ,0.01,0.99,1,1
24,7,Spacy,Ease of use,easy,Ease of use aspect,55087101,How to get all noun phrases in Spacy(Python),"I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:","Spacy clearly defines a noun chunk as:

A base noun phrase, or ""NP chunk"", is a noun phrase that does not permit other NPs to be nested within it ‚Äö√Ñ√∂‚àö√ë‚àö¬® so no NP-level coordination, no prepositional phrases, and no relative clauses."" (https://spacy.io/api/doc#noun_chunks)
If you process the dependency parse differently, allowing prepositional modifiers and nested phrases/chunks, then you can end up with what you're looking for.

I bet you could modify the existing spacy code fairly easily to do what you want:

https://github.com/explosion/spaCy/blob/06c6dc6fbcb8fbb78a61a2e42c1b782974bd43bd/spacy/lang/en/syntax_iterators.py",modifying spacy code to extract all noun phrases,"The user can modify the existing Spacy code to extract all noun phrases, and a link to the relevant code is provided.",,1,,,,C20230706014339305504,0.67,0.87,0.75,0.82,0.82,0.72,0.65,0.68,0.54,0.85,0.67,0.77,0.72,0.6,0.75,0.87,0.51,0.55,0.82,0.73,0.72,0.84,0.71,0.86, 7modifying spacy code to extract all noun phrases ,0,1,1,1
25,8,Spacy,Ease of use,complex,Ease of use aspect,65850018,processing text with spacy nlp.pipe,"I'm procerssing 40,000 abstracts with spacy nlp.pipe using the code below and its taking 8 mins. Is there a way to speed this up further? I've also disabled ner.","Try tuning batch_size and n_process params :
Note as well, by joining on "" "" you may have some surprises, as spaCy's splitting rules are a bit more complex than that.",ease of use,Spacy library is easy to use for text processing.,,0,,,,C20230706014339793574,0.62,0.64,0.53,0.77,0.8,0.91,0.46,0.74,0.56,0.48,0.46,0.8,0.48,0.58,0.58,0.7,0.56,0.62,0.53,0.51,0.6,0.76,0.85,0.89, 8ease of use ,0.49,0.51,0,1
26,8,Spacy,Ease of use,complex,Ease of use aspect,65850018,processing text with spacy nlp.pipe,"I'm procerssing 40,000 abstracts with spacy nlp.pipe using the code below and its taking 8 mins. Is there a way to speed this up further? I've also disabled ner.","Try tuning batch_size and n_process params :
Note as well, by joining on "" "" you may have some surprises, as spaCy's splitting rules are a bit more complex than that.",parameter tuning,Performance can be improved with tuning batch_size and n_process.,,1,,,,C20230706014340466081,0.7,0.61,0.53,0.71,0.45,0.37,0.59,0.64,0.62,0.71,0.74,0.87,0.61,0.61,0.51,0.51,0.37,0.24,0.43,0.63,0.73,0.53,0.52,0.91, 8parameter tuning ,0.22,0.78,1,1
27,8,Spacy,Ease of use,complex,Ease of use aspect,65850018,processing text with spacy nlp.pipe,"I'm procerssing 40,000 abstracts with spacy nlp.pipe using the code below and its taking 8 mins. Is there a way to speed this up further? I've also disabled ner.","Try tuning batch_size and n_process params :
Note as well, by joining on "" "" you may have some surprises, as spaCy's splitting rules are a bit more complex than that.",string joining,Simple string join may not work properly due to complex splitting rules of Spacy.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706014341425378,0.72,0.62,0.67,0.87,0.83,0.85,0.75,0.69,0.8,0.8,0.82,0.89,0.68,0.66,0.57,0.68,0.63,0.58,0.77,0.69,0.68,0.8,0.87,0.9, 8string joining ,0.09,0.91,1,1
28,9,Spacy,Documentation,documentation,Documentation aspect,52855178,Discrepancy documentation and implementation of spaCy vectors for German words?,"According to documentation:

spaCy's small models (all packages that end in sm) don't ship with word vectors, and only include context-sensitive tensors. [...] individual tokens won't have any vectors assigned.
But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True.

It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?","has_vector behaves differently than you expect.

This is discussed in the comments on an issue raised on github. The gist is, since vectors are available, it is True, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.

Quote from spaCy contributor Ines:

We've been going back and forth on how the has_vector should behave in cases like this. There is a vector, so having it return False would be misleading. Similarly, if the model doesn't come with a pre-trained vocab, technically all lexemes are OOV.
Version 2.1.0 has been announced to include German word vectors.",behavior of has_vector,"has_vector attribute returns True if vectors are available, regardless of whether they are context or word vectors.",Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706014341892503,0.76,0.63,0.66,0.66,0.63,0.68,0.46,0.66,0.65,0.54,0.5,0.61,0.42,0.45,0.53,0.78,0.56,0.62,0.45,0.47,0.38,0.78,0.74,0.78, 9behavior of has_vector ,0.74,0.26,0,0
29,9,Spacy,Documentation,documentation,Documentation aspect,52855178,Discrepancy documentation and implementation of spaCy vectors for German words?,"According to documentation:

spaCy's small models (all packages that end in sm) don't ship with word vectors, and only include context-sensitive tensors. [...] individual tokens won't have any vectors assigned.
But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True.

It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?","has_vector behaves differently than you expect.

This is discussed in the comments on an issue raised on github. The gist is, since vectors are available, it is True, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.

Quote from spaCy contributor Ines:

We've been going back and forth on how the has_vector should behave in cases like this. There is a vector, so having it return False would be misleading. Similarly, if the model doesn't come with a pre-trained vocab, technically all lexemes are OOV.
Version 2.1.0 has been announced to include German word vectors.",discussions on has_vector,"Contributors have been discussing the behavior of the has_vector attribute in cases like this, where only context vectors are available.",Similarity Technique Not Enough,1,Similarity Technique Not Enough,Decider,,C20230706014343424196,0.81,0.49,0.77,0.49,0.42,0.81,0.37,0.48,0.78,0.34,0.39,0.68,0.45,0.54,0.43,0.5,0.51,0.54,0.57,0.49,0.53,0.57,0.79,0.58, 9discussions on has_vector ,0.48,0.52,0,0
30,9,Spacy,Documentation,documentation,Documentation aspect,52855178,Discrepancy documentation and implementation of spaCy vectors for German words?,"According to documentation:

spaCy's small models (all packages that end in sm) don't ship with word vectors, and only include context-sensitive tensors. [...] individual tokens won't have any vectors assigned.
But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True.

It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?","has_vector behaves differently than you expect.

This is discussed in the comments on an issue raised on github. The gist is, since vectors are available, it is True, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.

Quote from spaCy contributor Ines:

We've been going back and forth on how the has_vector should behave in cases like this. There is a vector, so having it return False would be misleading. Similarly, if the model doesn't come with a pre-trained vocab, technically all lexemes are OOV.
Version 2.1.0 has been announced to include German word vectors.",importance of understanding,Understanding the behavior of the has_vector attribute and the differences between context and word vectors is vital in using Spacy's functionality effectively.,Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706014342900550,0.81,0.58,0.7,0.69,0.82,0.84,0.85,0.72,0.76,0.74,0.73,0.74,0.56,0.61,0.6,0.64,0.56,0.76,0.73,0.74,0.66,0.67,0.69,0.68, 9importance of understanding ,0.09,0.91,1,0
31,9,Spacy,Documentation,documentation,Documentation aspect,52855178,Discrepancy documentation and implementation of spaCy vectors for German words?,"According to documentation:

spaCy's small models (all packages that end in sm) don't ship with word vectors, and only include context-sensitive tensors. [...] individual tokens won't have any vectors assigned.
But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True.

It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?","has_vector behaves differently than you expect.

This is discussed in the comments on an issue raised on github. The gist is, since vectors are available, it is True, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.

Quote from spaCy contributor Ines:

We've been going back and forth on how the has_vector should behave in cases like this. There is a vector, so having it return False would be misleading. Similarly, if the model doesn't come with a pre-trained vocab, technically all lexemes are OOV.
Version 2.1.0 has been announced to include German word vectors.",inclusion of german word vectors,"Beginning with version 2.1.0, Spacy will include German word vectors.",No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706014343950832,0.75,0.79,0.7,0.66,0.72,0.78,0.67,0.46,0.93,0.42,0.67,0.55,0.66,0.6,0.68,0.75,0.74,0.9,0.7,0.56,0.6,0.63,0.63,0.66, 9inclusion of german word vectors ,0.01,0.99,1,1
32,10,Spacy,Documentation,documentation,Documentation aspect,48200524,Named entity recognition in Spacy,"I am trying to find Named entities for a sentence as below

import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","As per spacy documentation for Name Entity Recognition here is the way to extract name entity

import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
Result
Name Entity:  (China,) ",importing spacy module,Necessary to perform name entity recognition.,,1,,,,C20230706014345093135,0.51,0.51,0.61,0.78,0.81,0.95,0.37,0.39,0.42,0.69,0.69,0.81,0.51,0.57,0.73,0.73,0.69,0.66,0.45,0.54,0.48,0.8,0.86,0.83, 10importing spacy module ,0.04,0.96,1,1
33,10,Spacy,Documentation,documentation,Documentation aspect,48200524,Named entity recognition in Spacy,"I am trying to find Named entities for a sentence as below

import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","As per spacy documentation for Name Entity Recognition here is the way to extract name entity

import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
Result
Name Entity:  (China,) ",loading 'en' model,Required for Spacy to identify English language.,,1,,,,C20230706014345533097,0.48,0.64,0.63,0.46,0.54,0.92,0.58,0.56,0.5,0.61,0.62,0.83,0.68,0.59,0.67,0.45,0.48,0.71,0.42,0.53,0.54,0.59,0.56,0.74, 10loading 'en' model ,0.19,0.81,1,1
34,10,Spacy,Documentation,documentation,Documentation aspect,48200524,Named entity recognition in Spacy,"I am trying to find Named entities for a sentence as below

import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","As per spacy documentation for Name Entity Recognition here is the way to extract name entity

import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
Result
Name Entity:  (China,) ",output of name entity recognition,Returns set of named entities in input sentence.,,1,,,,C20230706014346047065,0.54,0.63,0.57,0.55,0.46,0.66,0.47,0.59,0.46,0.77,0.72,0.72,0.64,0.64,0.6,0.66,0.64,0.57,0.46,0.31,0.5,0.5,0.64,0.34, 10output of name entity recognition ,0.2,0.8,1,1
35,10,Spacy,Documentation,documentation,Documentation aspect,48200524,Named entity recognition in Spacy,"I am trying to find Named entities for a sentence as below

import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","As per spacy documentation for Name Entity Recognition here is the way to extract name entity

import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
Result
Name Entity:  (China,) ",spacy documentation,Provides guidelines for using Spacy library.,,1,,,,C20230706014344516514,0.47,0.45,0.46,0.59,0.39,0.73,0.57,0.56,0.55,0.64,0.59,0.8,0.47,0.7,0.81,0.54,0.41,0.43,0.45,0.51,0.52,0.48,0.35,0.61, 10spacy documentation ,0.21,0.79,1,1
36,11,Spacy,Documentation,documentation,Documentation aspect,58215855,"How to get full list of POS, Tag, and Dep in spaCy?","The POS, TAG, and DEP values used in spaCy are common ones of NLP, but I believe there are some differences depending on the corpus database.

For example, Universal Dependencies Contributors has listed 37 syntactic dependencies. Does spaCy use all of these 37 dependencies? And nothing more?

Is there a command to output all POS, TAG, and DEP values, spaCy may provide?","A specific pipeline component can show its labels:

nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``')
spacy.explain(label) will give a short definition for most labels, e.g.:

spacy.explain(""NN"")
# 'noun, singular or mass'
spacy.explain(""PERSON"")
# 'People, including fictional'
The detailed documentation is here: https://spacy.io/api/annotation",accessing labels,A specific pipeline component can be used to show its labels.,,1,,,,C20230706014346572681,0.64,0.71,0.59,0.68,0.68,0.79,0.68,0.56,0.51,0.47,0.75,0.48,0.48,0.72,0.58,0.41,0.42,0.62,0.65,0.59,0.67,0.78,0.74,0.81, 11accessing labels ,0.03,0.97,1,1
37,11,Spacy,Documentation,documentation,Documentation aspect,58215855,"How to get full list of POS, Tag, and Dep in spaCy?","The POS, TAG, and DEP values used in spaCy are common ones of NLP, but I believe there are some differences depending on the corpus database.

For example, Universal Dependencies Contributors has listed 37 syntactic dependencies. Does spaCy use all of these 37 dependencies? And nothing more?

Is there a command to output all POS, TAG, and DEP values, spaCy may provide?","A specific pipeline component can show its labels:

nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``')
spacy.explain(label) will give a short definition for most labels, e.g.:

spacy.explain(""NN"")
# 'noun, singular or mass'
spacy.explain(""PERSON"")
# 'People, including fictional'
The detailed documentation is here: https://spacy.io/api/annotation",additional documentation,"For more information, the detailed documentation for spaCy's annotations can be found at https://spacy.io/api/annotation.",,1,,,,C20230706014348147508,0.74,0.5,0.64,0.58,0.7,0.74,0.61,0.51,0.53,0.29,0.66,0.54,0.47,0.57,0.57,0.43,0.75,0.51,0.62,0.5,0.55,0.62,0.82,0.61, 11additional documentation ,0.01,0.99,1,1
38,11,Spacy,Documentation,documentation,Documentation aspect,58215855,"How to get full list of POS, Tag, and Dep in spaCy?","The POS, TAG, and DEP values used in spaCy are common ones of NLP, but I believe there are some differences depending on the corpus database.

For example, Universal Dependencies Contributors has listed 37 syntactic dependencies. Does spaCy use all of these 37 dependencies? And nothing more?

Is there a command to output all POS, TAG, and DEP values, spaCy may provide?","A specific pipeline component can show its labels:

nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``')
spacy.explain(label) will give a short definition for most labels, e.g.:

spacy.explain(""NN"")
# 'noun, singular or mass'
spacy.explain(""PERSON"")
# 'People, including fictional'
The detailed documentation is here: https://spacy.io/api/annotation",label differences,The labels used by spaCy may differ based on the corpus database being used.,Unclear context,1,Generic Error,Mixed,,C20230706014346989310,0.69,0.59,0.71,0.82,0.89,0.87,0.7,0.67,0.69,0.8,0.88,0.82,0.72,0.47,0.53,0.48,0.36,0.46,0.66,0.59,0.66,0.57,0.62,0.85, 11label differences ,0.52,0.48,0,0
39,11,Spacy,Documentation,documentation,Documentation aspect,58215855,"How to get full list of POS, Tag, and Dep in spaCy?","The POS, TAG, and DEP values used in spaCy are common ones of NLP, but I believe there are some differences depending on the corpus database.

For example, Universal Dependencies Contributors has listed 37 syntactic dependencies. Does spaCy use all of these 37 dependencies? And nothing more?

Is there a command to output all POS, TAG, and DEP values, spaCy may provide?","A specific pipeline component can show its labels:

nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``')
spacy.explain(label) will give a short definition for most labels, e.g.:

spacy.explain(""NN"")
# 'noun, singular or mass'
spacy.explain(""PERSON"")
# 'People, including fictional'
The detailed documentation is here: https://spacy.io/api/annotation",outputting labels,"The nlp.get_pipe('tagger').labels command outputs all POS, TAG, and DEP values used by spaCy.",,1,,,,C20230706014347618904,0.88,0.82,0.75,0.73,0.86,0.64,0.88,0.6,0.79,0.62,0.85,0.75,0.72,0.71,0.6,0.67,0.89,0.67,0.75,0.65,0.65,0.77,0.83,0.85, 11outputting labels ,0.01,0.99,1,1
40,12,Spacy,Documentation,documentation,Documentation aspect,50189238,How to re-train an existing spacy NER model for currency,"I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around 1000 samples with the following format :

TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
Can anyone please help me out with this with the data format, training size or any other relevant information ?","The example from the documentation should work for you. I altered it a little to match your variable name.

optimizer = nlp.begin_training()

for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in TRAIN_DATA:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk('/model')
Link to Documentation",model save,"After successfully updating the model, it is then saved to disk using nlp.to_disk().",,1,,,,C20230706014349909613,0.78,0.79,0.6,0.85,0.59,0.6,0.76,0.71,0.62,0.74,0.73,0.63,0.47,0.71,0.71,0.59,0.6,0.48,0.68,0.75,0.57,0.59,0.78,0.6, 12model save ,0.3,0.7,1,1
41,12,Spacy,Documentation,documentation,Documentation aspect,50189238,How to re-train an existing spacy NER model for currency,"I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around 1000 samples with the following format :

TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
Can anyone please help me out with this with the data format, training size or any other relevant information ?","The example from the documentation should work for you. I altered it a little to match your variable name.

optimizer = nlp.begin_training()

for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in TRAIN_DATA:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk('/model')
Link to Documentation",solution overview,"The provided example shows how to retrain an existing Spacy NER model for currency, with different currency values in various formats.",,1,,,,C20230706014348657773,0.58,0.66,0.5,0.8,0.65,0.48,0.67,0.74,0.53,0.6,0.35,0.48,0.56,0.64,0.59,0.5,0.65,0.28,0.73,0.5,0.55,0.55,0.49,0.31, 12solution overview ,0.01,0.99,1,1
42,12,Spacy,Documentation,documentation,Documentation aspect,50189238,How to re-train an existing spacy NER model for currency,"I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around 1000 samples with the following format :

TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
Can anyone please help me out with this with the data format, training size or any other relevant information ?","The example from the documentation should work for you. I altered it a little to match your variable name.

optimizer = nlp.begin_training()

for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in TRAIN_DATA:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk('/model')
Link to Documentation",training method,"The solution involves iterating over a shuffled training data for 100 iterations, creating a Doc and a GoldParse object for each iteration, and updating the NER model with nlp.update() using a dropout rate of 0.5 and an optimizer.",,1,,,,C20230706014349064239,0.52,0.76,0.75,0.23,0.09,0.19,0.79,0.55,0.27,0.63,0.28,0.28,0.83,0.6,0.3,0.4,0.41,0.24,0.42,0.5,0.5,0.64,0.78,0.63, 12training method ,0.02,0.98,1,1
43,13,Spacy,Documentation,documentation,Documentation aspect,74550422,Concatenate two spacy docs together?,"How do I concatenate two spacy docs together? To merge them into one?

import spacy

nlp = spacy.load('en')
doc1 = nlp(u'This is the doc number one.')
doc2 = nlp(u'And this is the doc number two.')
new_doc = doc1+doc2
Of course that will return an error as a doc object is not concatenable by default. Is there a straightforward solution to do that?

I looked at this: https://github.com/explosion/spaCy/issues/2229 The issue seems closed so it sounds like they have implemented a solution but I cannot find a simple example of that being used.","What about this:
For ""a lot"" of different sentences, it might be better to use nlp.pipe as shown in the documentation.

Hope it helps.",efficiency,Using nlp.pipe is more efficient for a lot of different sentences.,,1,,,,C20230706014421703319,0.76,0.7,0.84,0.91,0.7,0.71,0.7,0.62,0.82,0.87,0.78,0.74,0.79,0.7,0.61,0.8,0.91,0.76,0.67,0.61,0.54,0.88,0.79,0.76, 13efficiency ,0.01,0.99,1,1
44,13,Spacy,Documentation,documentation,Documentation aspect,74550422,Concatenate two spacy docs together?,"How do I concatenate two spacy docs together? To merge them into one?

import spacy

nlp = spacy.load('en')
doc1 = nlp(u'This is the doc number one.')
doc2 = nlp(u'And this is the doc number two.')
new_doc = doc1+doc2
Of course that will return an error as a doc object is not concatenable by default. Is there a straightforward solution to do that?

I looked at this: https://github.com/explosion/spaCy/issues/2229 The issue seems closed so it sounds like they have implemented a solution but I cannot find a simple example of that being used.","What about this:
For ""a lot"" of different sentences, it might be better to use nlp.pipe as shown in the documentation.

Hope it helps.",no straightforward solution,There isn't a straightforward solution provided for concatenating two Spacy docs together in the conversation.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706014422219122,0.75,0.44,0.55,0.68,0.29,0.39,0.75,0.56,0.74,0.73,0.85,0.82,0.58,0.57,0.69,0.62,0.44,0.56,0.72,0.49,0.62,0.7,0.83,0.65, 13no straightforward solution ,0.04,0.96,1,1
45,13,Spacy,Documentation,documentation,Documentation aspect,74550422,Concatenate two spacy docs together?,"How do I concatenate two spacy docs together? To merge them into one?

import spacy

nlp = spacy.load('en')
doc1 = nlp(u'This is the doc number one.')
doc2 = nlp(u'And this is the doc number two.')
new_doc = doc1+doc2
Of course that will return an error as a doc object is not concatenable by default. Is there a straightforward solution to do that?

I looked at this: https://github.com/explosion/spaCy/issues/2229 The issue seems closed so it sounds like they have implemented a solution but I cannot find a simple example of that being used.","What about this:
For ""a lot"" of different sentences, it might be better to use nlp.pipe as shown in the documentation.

Hope it helps.",reference to github issue,The conversation references an issue on Github where the problem of concatenating Spacy docs together has been discussed by the Spacy community.,Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014422749354,0.77,0.67,0.61,0.88,0.55,0.49,0.79,0.7,0.72,0.55,0.54,0.82,0.78,0.47,0.49,0.75,0.5,0.58,0.68,0.55,0.63,0.62,0.61,0.58, 13reference to github issue ,0.08,0.92,1,0
46,13,Spacy,Documentation,documentation,Documentation aspect,74550422,Concatenate two spacy docs together?,"How do I concatenate two spacy docs together? To merge them into one?

import spacy

nlp = spacy.load('en')
doc1 = nlp(u'This is the doc number one.')
doc2 = nlp(u'And this is the doc number two.')
new_doc = doc1+doc2
Of course that will return an error as a doc object is not concatenable by default. Is there a straightforward solution to do that?

I looked at this: https://github.com/explosion/spaCy/issues/2229 The issue seems closed so it sounds like they have implemented a solution but I cannot find a simple example of that being used.","What about this:
For ""a lot"" of different sentences, it might be better to use nlp.pipe as shown in the documentation.

Hope it helps.",suggestion for official documentation,The answer suggests that the official Spacy documentation might contain more comprehensive information on the topic of concatenating Spacy docs.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014423300535,0.36,0.79,0.73,0.73,0.78,0.93,0.36,0.59,0.52,0.33,0.36,0.85,0.47,0.5,0.57,0.3,0.38,0.58,0.59,0.63,0.61,0.57,0.59,0.9, 13suggestion for official documentation ,0.02,0.98,1,0
47,14,Spacy,Documentation,documentation,Documentation aspect,57206701,Spacy tokenizer rule for exceptions that contain whitespace?,"When I create a pipeline with the default tokenizer for say English, I can then call the method for adding a special case:

tokenizer.add_special_case(""don't"", case)
The tokenizer will happily accept a special case that contains whitespace:

tokenizer.add_special_case(""some odd case"", case)
but it appears that does not actually change the behavior of the tokenizer or will never match?

More generally, what is the best way of extending an existing tokenizer so that the some patterns which normally would result in multiple tokens only create one token? For example something like [A-Za-z]+\([A-Za-z0-9]+\)[A-Za-z]+ should not result in three tokens because of the parentheses but in a single token, e.g. for asdf(a33b)xyz while the normal English rules should still apply if that pattern does not match.

Is this something that can be done somehow by augmenting an existing tokenizer or would I have to first tokenize, then find entities that match the corresponding token patterns and then merge the entity tokens?","As you found, Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace. That's for adding strings like ""o'clock"" and "":-)"", or expanding e.g. ""don't"" to ""do not"".

Modifying the prefix, suffix and infix rules (either by setting them on an existing tokenizer or creating a new tokenizer with custom parameters) also doesn't work since those are applied after whitespace splitting.

To override the whitespace splitting behavior, you have four options:

Merge after tokenization. You use Retokenizer.merge(), or possibly merge_entities or merge_noun_chunks. The relevant documentation is here: https://spacy.io/usage/linguistic-features#retokenization and https://spacy.io/api/pipeline-functions#merge_entities and https://spacy.io/api/pipeline-functions#merge_noun_chunks

This is your best bet for keeping as much of the default behavior as possible.
Subclass Tokenizer and override __call__. Sample code:

Implement a completely new tokenizer (without subclassing Tokenizer). Relevant docs here: https://spacy.io/usage/linguistic-features#custom-tokenizer-example
Tokenize externally and instantiate Doc with words. Relevant docs here: https://spacy.io/usage/linguistic-features#own-annotations
To answer the second part of your question, if you don't need to change whitespace splitting behavior, you have two other options:

Add to the default prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizer-additions

Note from https://stackoverflow.com/a/58112065/594211: ""You can add new patterns without defining a custom tokenizer, but there's no way to remove a pattern without defining a custom tokenizer.""
Instantiate Tokenizer with custom prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizers

To get the default rules, you read the existing tokenizer's attributes (as shown above) or use the nlp object‚Äö√Ñ√∂‚àö√ë‚àö¬•s Defaults. There are code samples for the latter approach in https://stackoverflow.com/a/47502839/594211 and https://stackoverflow.com/a/58112065/594211.",modify rules for patterns,"The answer suggests to add to the default prefix, suffix, and infix rules or instantiate the tokenizer with custom rules to handle cases where multiple tokens are generated from a pattern.",Too large question and answer,1,Convoluted Challenge,Enquirer,,C20230706014426412517,0.57,0.82,0.69,0.66,0.73,0.83,0.57,0.83,0.79,0.58,0.48,0.68,0.44,0.63,0.57,0.53,0.89,0.53,0.62,0.76,0.76,0.78,0.68,0.81, 14modify rules for patterns ,0.04,0.96,1,1
48,14,Spacy,Documentation,documentation,Documentation aspect,57206701,Spacy tokenizer rule for exceptions that contain whitespace?,"When I create a pipeline with the default tokenizer for say English, I can then call the method for adding a special case:

tokenizer.add_special_case(""don't"", case)
The tokenizer will happily accept a special case that contains whitespace:

tokenizer.add_special_case(""some odd case"", case)
but it appears that does not actually change the behavior of the tokenizer or will never match?

More generally, what is the best way of extending an existing tokenizer so that the some patterns which normally would result in multiple tokens only create one token? For example something like [A-Za-z]+\([A-Za-z0-9]+\)[A-Za-z]+ should not result in three tokens because of the parentheses but in a single token, e.g. for asdf(a33b)xyz while the normal English rules should still apply if that pattern does not match.

Is this something that can be done somehow by augmenting an existing tokenizer or would I have to first tokenize, then find entities that match the corresponding token patterns and then merge the entity tokens?","As you found, Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace. That's for adding strings like ""o'clock"" and "":-)"", or expanding e.g. ""don't"" to ""do not"".

Modifying the prefix, suffix and infix rules (either by setting them on an existing tokenizer or creating a new tokenizer with custom parameters) also doesn't work since those are applied after whitespace splitting.

To override the whitespace splitting behavior, you have four options:

Merge after tokenization. You use Retokenizer.merge(), or possibly merge_entities or merge_noun_chunks. The relevant documentation is here: https://spacy.io/usage/linguistic-features#retokenization and https://spacy.io/api/pipeline-functions#merge_entities and https://spacy.io/api/pipeline-functions#merge_noun_chunks

This is your best bet for keeping as much of the default behavior as possible.
Subclass Tokenizer and override __call__. Sample code:

Implement a completely new tokenizer (without subclassing Tokenizer). Relevant docs here: https://spacy.io/usage/linguistic-features#custom-tokenizer-example
Tokenize externally and instantiate Doc with words. Relevant docs here: https://spacy.io/usage/linguistic-features#own-annotations
To answer the second part of your question, if you don't need to change whitespace splitting behavior, you have two other options:

Add to the default prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizer-additions

Note from https://stackoverflow.com/a/58112065/594211: ""You can add new patterns without defining a custom tokenizer, but there's no way to remove a pattern without defining a custom tokenizer.""
Instantiate Tokenizer with custom prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizers

To get the default rules, you read the existing tokenizer's attributes (as shown above) or use the nlp object‚Äö√Ñ√∂‚àö√ë‚àö¬•s Defaults. There are code samples for the latter approach in https://stackoverflow.com/a/47502839/594211 and https://stackoverflow.com/a/58112065/594211.",override whitespace splitting,"Four options are provided, including merging after tokenization, subclassing Tokenizer, implementing a new tokenizer, or tokenizing externally and instantiating Doc with words.",,1,,,,C20230706014425567550,0.76,0.67,0.52,0.64,0.49,0.37,0.82,0.7,0.74,0.8,0.7,0.7,0.64,0.73,0.42,0.49,0.51,0.32,0.78,0.79,0.88,0.84,0.86,0.86, 14override whitespace splitting ,0.02,0.98,1,1
49,14,Spacy,Documentation,documentation,Documentation aspect,57206701,Spacy tokenizer rule for exceptions that contain whitespace?,"When I create a pipeline with the default tokenizer for say English, I can then call the method for adding a special case:

tokenizer.add_special_case(""don't"", case)
The tokenizer will happily accept a special case that contains whitespace:

tokenizer.add_special_case(""some odd case"", case)
but it appears that does not actually change the behavior of the tokenizer or will never match?

More generally, what is the best way of extending an existing tokenizer so that the some patterns which normally would result in multiple tokens only create one token? For example something like [A-Za-z]+\([A-Za-z0-9]+\)[A-Za-z]+ should not result in three tokens because of the parentheses but in a single token, e.g. for asdf(a33b)xyz while the normal English rules should still apply if that pattern does not match.

Is this something that can be done somehow by augmenting an existing tokenizer or would I have to first tokenize, then find entities that match the corresponding token patterns and then merge the entity tokens?","As you found, Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace. That's for adding strings like ""o'clock"" and "":-)"", or expanding e.g. ""don't"" to ""do not"".

Modifying the prefix, suffix and infix rules (either by setting them on an existing tokenizer or creating a new tokenizer with custom parameters) also doesn't work since those are applied after whitespace splitting.

To override the whitespace splitting behavior, you have four options:

Merge after tokenization. You use Retokenizer.merge(), or possibly merge_entities or merge_noun_chunks. The relevant documentation is here: https://spacy.io/usage/linguistic-features#retokenization and https://spacy.io/api/pipeline-functions#merge_entities and https://spacy.io/api/pipeline-functions#merge_noun_chunks

This is your best bet for keeping as much of the default behavior as possible.
Subclass Tokenizer and override __call__. Sample code:

Implement a completely new tokenizer (without subclassing Tokenizer). Relevant docs here: https://spacy.io/usage/linguistic-features#custom-tokenizer-example
Tokenize externally and instantiate Doc with words. Relevant docs here: https://spacy.io/usage/linguistic-features#own-annotations
To answer the second part of your question, if you don't need to change whitespace splitting behavior, you have two other options:

Add to the default prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizer-additions

Note from https://stackoverflow.com/a/58112065/594211: ""You can add new patterns without defining a custom tokenizer, but there's no way to remove a pattern without defining a custom tokenizer.""
Instantiate Tokenizer with custom prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizers

To get the default rules, you read the existing tokenizer's attributes (as shown above) or use the nlp object‚Äö√Ñ√∂‚àö√ë‚àö¬•s Defaults. There are code samples for the latter approach in https://stackoverflow.com/a/47502839/594211 and https://stackoverflow.com/a/58112065/594211.",rules modification limitation,"Modifying the prefix, suffix and infix rules doesn't work since those are applied after whitespace splitting.",,1,,,,C20230706014424847126,0.66,0.74,0.6,0.54,0.67,0.37,0.8,0.75,0.44,0.84,0.51,0.51,0.66,0.74,0.6,0.55,0.59,0.57,0.62,0.66,0.47,0.93,0.79,0.83, 14rules modification limitation ,0.04,0.96,1,1
50,14,Spacy,Documentation,documentation,Documentation aspect,57206701,Spacy tokenizer rule for exceptions that contain whitespace?,"When I create a pipeline with the default tokenizer for say English, I can then call the method for adding a special case:

tokenizer.add_special_case(""don't"", case)
The tokenizer will happily accept a special case that contains whitespace:

tokenizer.add_special_case(""some odd case"", case)
but it appears that does not actually change the behavior of the tokenizer or will never match?

More generally, what is the best way of extending an existing tokenizer so that the some patterns which normally would result in multiple tokens only create one token? For example something like [A-Za-z]+\([A-Za-z0-9]+\)[A-Za-z]+ should not result in three tokens because of the parentheses but in a single token, e.g. for asdf(a33b)xyz while the normal English rules should still apply if that pattern does not match.

Is this something that can be done somehow by augmenting an existing tokenizer or would I have to first tokenize, then find entities that match the corresponding token patterns and then merge the entity tokens?","As you found, Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace. That's for adding strings like ""o'clock"" and "":-)"", or expanding e.g. ""don't"" to ""do not"".

Modifying the prefix, suffix and infix rules (either by setting them on an existing tokenizer or creating a new tokenizer with custom parameters) also doesn't work since those are applied after whitespace splitting.

To override the whitespace splitting behavior, you have four options:

Merge after tokenization. You use Retokenizer.merge(), or possibly merge_entities or merge_noun_chunks. The relevant documentation is here: https://spacy.io/usage/linguistic-features#retokenization and https://spacy.io/api/pipeline-functions#merge_entities and https://spacy.io/api/pipeline-functions#merge_noun_chunks

This is your best bet for keeping as much of the default behavior as possible.
Subclass Tokenizer and override __call__. Sample code:

Implement a completely new tokenizer (without subclassing Tokenizer). Relevant docs here: https://spacy.io/usage/linguistic-features#custom-tokenizer-example
Tokenize externally and instantiate Doc with words. Relevant docs here: https://spacy.io/usage/linguistic-features#own-annotations
To answer the second part of your question, if you don't need to change whitespace splitting behavior, you have two other options:

Add to the default prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizer-additions

Note from https://stackoverflow.com/a/58112065/594211: ""You can add new patterns without defining a custom tokenizer, but there's no way to remove a pattern without defining a custom tokenizer.""
Instantiate Tokenizer with custom prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizers

To get the default rules, you read the existing tokenizer's attributes (as shown above) or use the nlp object‚Äö√Ñ√∂‚àö√ë‚àö¬•s Defaults. There are code samples for the latter approach in https://stackoverflow.com/a/47502839/594211 and https://stackoverflow.com/a/58112065/594211.",special case limitation,Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace.,,1,,,,C20230706014424327637,0.67,0.58,0.69,0.79,0.89,0.74,0.68,0.65,0.71,0.64,0.71,0.67,0.67,0.67,0.62,0.93,0.73,0.65,0.79,0.73,0.7,0.83,0.84,0.82, 14special case limitation ,0.23,0.77,1,1
51,15,Spacy,Documentation,documentation,Documentation aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",example code provided,The documentation provides an example code that shows how to load a Spacy model and process multiple documents in parallel.,Original explanation is not conclusive,0,Generic Error,Mixed,,C20230706014427470367,0.7,0.63,0.63,0.6,0.55,0.69,0.49,0.74,0.72,0.52,0.57,0.73,0.59,0.68,0.93,0.42,0.57,0.59,0.33,0.68,0.66,0.6,0.39,0.6, 15example code provided ,0.01,0.99,1,0
52,15,Spacy,Documentation,documentation,Documentation aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",further documentation available,The Spacy library provides further information on how to use multiprocessing for efficient and speedy processing.,Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706014427993632,0.74,0.5,0.62,0.79,0.74,0.69,0.77,0.7,0.75,0.66,0.7,0.87,0.61,0.67,0.57,0.83,0.63,0.73,0.68,0.64,0.71,0.85,0.83,0.79, 15further documentation available ,0.24,0.76,1,0
53,15,Spacy,Documentation,documentation,Documentation aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",safety of single spacy object,It is safe to create a single Spacy object and access it from multiple processes for named entity recognition as long as it is done through nlp.pipe and the number of processes is specified to avoid any interference.,wrong labeling,1,Generic Error,Mixed,,C20230706014428609838,0.66,0.74,0.59,0.76,0.74,0.56,0.86,0.9,0.84,0.86,0.73,0.78,0.58,0.67,0.43,0.65,0.49,0.38,0.74,0.85,0.76,0.79,0.75,0.89, 15safety of single spacy object ,0.32,0.68,1,1
54,15,Spacy,Documentation,documentation,Documentation aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",solution for multiprocessing,The documentation of the Spacy library provides a solution to sharing Spacy models between processes by passing the n_process argument to nlp.pipe.,wrong labeling,1,Generic Error,Mixed,,C20230706014426944256,0.79,0.67,0.74,0.58,0.66,0.87,0.62,0.76,0.8,0.63,0.66,0.8,0.79,0.63,0.8,0.56,0.57,0.75,0.64,0.72,0.89,0.76,0.73,0.81, 15solution for multiprocessing ,0.01,0.99,1,1
55,16,Spacy,Documentation,documentation,Documentation aspect,67198877,Cannot import biluo_tags_from_offsets from spacy.gold,"I am trying to import biluo_tags_from_offsets from spacy.gold but cannot do it. Getting ModuleNotFoundError.

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-21-d17a54331c7a> in <module>
      3 
      4 import spacy
----> 5 from spacy.gold import biluo_tags_from_offsets
      6 nlp = spacy.load(""en_core_web_lg"")
      7 

ModuleNotFoundError: No module named 'spacy.gold'
How can I use this spacy.gold module. I have already installed spacy. I am an amateur, sorry if this question does not make any sense. Any kind of help is appreciated. Thanks!","As the documentation says, spacy.gold was disabled in spaCy 3.0. If you have the latest spaCy version, that is why you are getting this error.

You need to replace from spacy.gold import biluo_tags_from_offsets with from spacy.training import offsets_to_biluo_tags.",module disabled,Spacy.gold was disabled in spaCy 3.0 which resulted in the error message 'No module named 'spacy.gold'.',,1,,,,C20230706014429048055,0.67,0.82,0.68,0.64,0.78,0.67,0.49,0.76,0.63,0.51,0.6,0.58,0.58,0.56,0.59,0.54,0.62,0.88,0.58,0.73,0.65,0.72,0.7,0.67, 16module disabled ,0.02,0.98,1,1
56,16,Spacy,Documentation,documentation,Documentation aspect,67198877,Cannot import biluo_tags_from_offsets from spacy.gold,"I am trying to import biluo_tags_from_offsets from spacy.gold but cannot do it. Getting ModuleNotFoundError.

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-21-d17a54331c7a> in <module>
      3 
      4 import spacy
----> 5 from spacy.gold import biluo_tags_from_offsets
      6 nlp = spacy.load(""en_core_web_lg"")
      7 

ModuleNotFoundError: No module named 'spacy.gold'
How can I use this spacy.gold module. I have already installed spacy. I am an amateur, sorry if this question does not make any sense. Any kind of help is appreciated. Thanks!","As the documentation says, spacy.gold was disabled in spaCy 3.0. If you have the latest spaCy version, that is why you are getting this error.

You need to replace from spacy.gold import biluo_tags_from_offsets with from spacy.training import offsets_to_biluo_tags.",replace with new module,The recommended solution is to replace from spacy.gold import biluo_tags_from_offsets with from spacy.training import offsets_to_biluo_tags to import the equivalent functionality from a new module.,,1,,,,C20230706014429557161,0.8,0.62,0.51,0.66,0.28,0.35,0.76,0.73,0.51,0.7,0.41,0.65,0.69,0.59,0.4,0.48,0.44,0.36,0.69,0.76,0.73,0.82,0.74,0.81, 16replace with new module ,0,1,1,1
57,17,Spacy,Documentation,documentation,Documentation aspect,61241351,"Install specific version of spacy - working with pip, but not with conda","I am revisiting some code I wrote months ago with Spacy Version 2.1.8. I need this version of spacy. pip install is working but Anaconda does not even show that spacy is installed. And

conda install spacy=2.1.8
gives the PackagesNotFoundError error.","As explained in spaCy documentation, you can take advantage of the conda-forge community repository to install spacy

$ conda install -c conda-forge spacy=2.1.8
This version should be available according to spaCy's page on Conda Forge.",available version,Documentation assures that the required version of Spacy should be available on the conda-forge repository.,,1,,,,C20230706014431142992,0.65,0.66,0.68,0.87,0.7,0.79,0.8,0.78,0.68,0.93,0.74,0.69,0.46,0.45,0.79,0.47,0.52,0.6,0.74,0.64,0.49,0.86,0.59,0.52, 17available version ,0.34,0.66,1,1
58,17,Spacy,Documentation,documentation,Documentation aspect,61241351,"Install specific version of spacy - working with pip, but not with conda","I am revisiting some code I wrote months ago with Spacy Version 2.1.8. I need this version of spacy. pip install is working but Anaconda does not even show that spacy is installed. And

conda install spacy=2.1.8
gives the PackagesNotFoundError error.","As explained in spaCy documentation, you can take advantage of the conda-forge community repository to install spacy

$ conda install -c conda-forge spacy=2.1.8
This version should be available according to spaCy's page on Conda Forge.",clear explanation,The provided explanation in the documentation is succinct and focuses on solving the issue of installing specific Spacy versions.,Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706014431657945,0.53,0.64,0.61,0.75,0.52,0.52,0.73,0.71,0.65,0.85,0.69,0.67,0.46,0.31,0.64,0.62,0.66,0.62,0.58,0.41,0.61,0.7,0.62,0.59, 17clear explanation ,0.06,0.94,1,0
59,17,Spacy,Documentation,documentation,Documentation aspect,61241351,"Install specific version of spacy - working with pip, but not with conda","I am revisiting some code I wrote months ago with Spacy Version 2.1.8. I need this version of spacy. pip install is working but Anaconda does not even show that spacy is installed. And

conda install spacy=2.1.8
gives the PackagesNotFoundError error.","As explained in spaCy documentation, you can take advantage of the conda-forge community repository to install spacy

$ conda install -c conda-forge spacy=2.1.8
This version should be available according to spaCy's page on Conda Forge.",command line code,"Documentation provides a command-line code to install spacy package, which specifies the version number and the conda-forge community repository.",,1,,,,C20230706014430720078,0.55,0.59,0.56,0.81,0.68,0.72,0.7,0.64,0.62,0.53,0.73,0.54,0.38,0.51,0.56,0.92,0.86,0.9,0.5,0.5,0.53,0.41,0.67,0.64, 17command line code ,0.27,0.73,1,1
60,17,Spacy,Documentation,documentation,Documentation aspect,61241351,"Install specific version of spacy - working with pip, but not with conda","I am revisiting some code I wrote months ago with Spacy Version 2.1.8. I need this version of spacy. pip install is working but Anaconda does not even show that spacy is installed. And

conda install spacy=2.1.8
gives the PackagesNotFoundError error.","As explained in spaCy documentation, you can take advantage of the conda-forge community repository to install spacy

$ conda install -c conda-forge spacy=2.1.8
This version should be available according to spaCy's page on Conda Forge.",use of conda-forge,Documentation emphasizes the use of conda-forge community repository to install Spacy packages.,,0,,,,C20230706014429993143,0.73,0.38,0.63,0.68,0.75,0.61,0.84,0.65,0.67,0.7,0.78,0.72,0.69,0.44,0.32,0.34,0.61,0.6,0.83,0.71,0.7,0.66,0.64,0.54, 17use of conda-forge ,0.5,0.5,0,1
61,18,Spacy,Documentation,documentation,Documentation aspect,68083466,How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.

I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.

For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ Using spacy train command for spacy v3.0.

The spacy train command that I am using currently is as follows:

> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
I load the model for prediction using:

> nlp1 = spacy.load(R"".\output\model-best"")
As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).","""I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.""
What you are describing is called ""online learning"" and the default spaCy models don't support it. Most modern neural NER methods, even outside of spaCy, have no support for it at all.

You cannot fix this by using a custom training loop.

Your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.

Rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.

Training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. What you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",online learning not supported,"The default spaCy models do not support online learning, where an existing model can be updated with new entities without training the whole model again.",,1,,,,C20230706014826555306,0.71,0.79,0.55,0.65,0.5,0.71,0.61,0.77,0.75,0.62,0.84,0.82,0.6,0.71,0.51,0.43,0.36,0.72,0.63,0.79,0.74,0.81,0.89,0.87, 18online learning not supported ,0.02,0.98,1,1
62,18,Spacy,Documentation,documentation,Documentation aspect,68083466,How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.

I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.

For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ Using spacy train command for spacy v3.0.

The spacy train command that I am using currently is as follows:

> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
I load the model for prediction using:

> nlp1 = spacy.load(R"".\output\model-best"")
As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).","""I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.""
What you are describing is called ""online learning"" and the default spaCy models don't support it. Most modern neural NER methods, even outside of spaCy, have no support for it at all.

You cannot fix this by using a custom training loop.

Your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.

Rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.

Training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. What you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",options: retrain models on the fly,"Retraining models on the fly may seem time-consuming, but a small model can be trained quickly and then used to train a larger model with longer training.",Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706014827542569,0.68,0.83,0.61,0.76,0.63,0.63,0.83,0.91,0.92,0.83,0.83,0.95,0.53,0.59,0.42,0.44,0.42,0.43,0.84,0.66,0.52,0.72,0.61,0.72, 18options: retrain models on the fly ,0.1,0.9,1,0
63,18,Spacy,Documentation,documentation,Documentation aspect,68083466,How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.

I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.

For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc‚Äö√Ñ√∂‚àö√ë¬¨‚àÇ Using spacy train command for spacy v3.0.

The spacy train command that I am using currently is as follows:

> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
I load the model for prediction using:

> nlp1 = spacy.load(R"".\output\model-best"")
As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).","""I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.""
What you are describing is called ""online learning"" and the default spaCy models don't support it. Most modern neural NER methods, even outside of spaCy, have no support for it at all.

You cannot fix this by using a custom training loop.

Your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.

Rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.

Training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. What you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",options: rule-based matching,Rule-based matching is an option for adding new entities by explicitly matching things in a predefined list.,wrong labeling,1,Generic Error,Mixed,,C20230706014827071956,0.56,0.71,0.61,0.7,0.68,0.78,0.73,0.71,0.63,0.9,0.66,0.71,0.58,0.52,0.64,0.65,0.59,0.62,0.6,0.56,0.67,0.82,0.79,0.86, 18options: rule-based matching ,0.1,0.9,1,1
64,19,Spacy,Performance,performance,Performance aspect,58724562,When creating a Doc using the standard constructor the model is not loaded ,"I'm trying to use SpaCY and instantiate the Doc object using the constructor:
but when I do that, if I try to use the dependency parser:
I get the error:

ValueError: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:
While if I use the method nlp(""Hello world!"") that does not happens.

The reason I do that, is because I use the entity extraction from a third party application I want to supply to SpaCy my tokenisation and my entities.
What should I do? load the pipeline by myself in the document, and exclude the tokeniser for example?","nlp() returns a Doc where the tokenizer and all the pipeline components in nlp.pipeline have been applied to the document.

If you create a Doc by hand, the tokenizer and the pipeline components are not loaded or applied at any point.

After creating a Doc by hand, you can still apply individual pipeline components from a loaded model:

nlp = spacy.load('en_core_web_sm')
nlp.tagger(doc)
nlp.parser(doc)
Then you can add your own entities to the document. (Note that if your tokenizer is very different from the default tokenizer used when training a model, the performance may not be as good.)",doc instantiation affects performance,"Using the standard constructor to create a Doc object in Spacy negatively affects its performance since the model and pipeline components are not loaded. Hence, errors may occur when using certain functions like the dependency parser.",Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706014828076581,0.6,0.54,0.52,0.66,0.6,0.69,0.5,0.71,0.76,0.62,0.58,0.55,0.59,0.48,0.56,0.92,0.63,0.62,0.7,0.74,0.76,0.87,0.83,0.82, 19doc instantiation affects performance ,0.35,0.65,1,0
65,19,Spacy,Performance,performance,Performance aspect,58724562,When creating a Doc using the standard constructor the model is not loaded ,"I'm trying to use SpaCY and instantiate the Doc object using the constructor:
but when I do that, if I try to use the dependency parser:
I get the error:

ValueError: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:
While if I use the method nlp(""Hello world!"") that does not happens.

The reason I do that, is because I use the entity extraction from a third party application I want to supply to SpaCy my tokenisation and my entities.
What should I do? load the pipeline by myself in the document, and exclude the tokeniser for example?","nlp() returns a Doc where the tokenizer and all the pipeline components in nlp.pipeline have been applied to the document.

If you create a Doc by hand, the tokenizer and the pipeline components are not loaded or applied at any point.

After creating a Doc by hand, you can still apply individual pipeline components from a loaded model:

nlp = spacy.load('en_core_web_sm')
nlp.tagger(doc)
nlp.parser(doc)
Then you can add your own entities to the document. (Note that if your tokenizer is very different from the default tokenizer used when training a model, the performance may not be as good.)",nlp() applies tokenizer and pipeline components,"When using the nlp() method to instantiate a Doc object in Spacy, the tokenizer and all pipeline components in nlp.pipeline are loaded and applied to the document, resulting in optimal performance and error-free use of functions like the dependency parser.",,1,,,,C20230706014828758730,0.71,0.81,0.74,0.86,0.56,0.6,0.87,0.84,0.73,0.85,0.6,0.73,0.64,0.74,0.73,0.73,0.65,0.62,0.66,0.68,0.64,0.56,0.87,0.58, 19nlp() applies tokenizer and pipeline components ,0.09,0.91,1,1
66,19,Spacy,Performance,performance,Performance aspect,58724562,When creating a Doc using the standard constructor the model is not loaded ,"I'm trying to use SpaCY and instantiate the Doc object using the constructor:
but when I do that, if I try to use the dependency parser:
I get the error:

ValueError: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:
While if I use the method nlp(""Hello world!"") that does not happens.

The reason I do that, is because I use the entity extraction from a third party application I want to supply to SpaCy my tokenisation and my entities.
What should I do? load the pipeline by myself in the document, and exclude the tokeniser for example?","nlp() returns a Doc where the tokenizer and all the pipeline components in nlp.pipeline have been applied to the document.

If you create a Doc by hand, the tokenizer and the pipeline components are not loaded or applied at any point.

After creating a Doc by hand, you can still apply individual pipeline components from a loaded model:

nlp = spacy.load('en_core_web_sm')
nlp.tagger(doc)
nlp.parser(doc)
Then you can add your own entities to the document. (Note that if your tokenizer is very different from the default tokenizer used when training a model, the performance may not be as good.)",pipeline components can be applied individually to a doc object,"After manually creating a Doc object in Spacy, it is possible to apply individual pipeline components from a loaded model like the tagger and parser to the document. One can also add their own entities to the document.",,1,,,,C20230706014829643970,0.68,0.84,0.77,0.72,0.64,0.82,0.46,0.73,0.66,0.5,0.6,0.66,0.61,0.64,0.81,0.53,0.53,0.59,0.51,0.82,0.76,0.67,0.59,0.86, 19pipeline components can be applied individually to a doc object ,0.13,0.87,1,1
67,19,Spacy,Performance,performance,Performance aspect,58724562,When creating a Doc using the standard constructor the model is not loaded ,"I'm trying to use SpaCY and instantiate the Doc object using the constructor:
but when I do that, if I try to use the dependency parser:
I get the error:

ValueError: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:
While if I use the method nlp(""Hello world!"") that does not happens.

The reason I do that, is because I use the entity extraction from a third party application I want to supply to SpaCy my tokenisation and my entities.
What should I do? load the pipeline by myself in the document, and exclude the tokeniser for example?","nlp() returns a Doc where the tokenizer and all the pipeline components in nlp.pipeline have been applied to the document.

If you create a Doc by hand, the tokenizer and the pipeline components are not loaded or applied at any point.

After creating a Doc by hand, you can still apply individual pipeline components from a loaded model:

nlp = spacy.load('en_core_web_sm')
nlp.tagger(doc)
nlp.parser(doc)
Then you can add your own entities to the document. (Note that if your tokenizer is very different from the default tokenizer used when training a model, the performance may not be as good.)",tokenizer differences can affect performance,"If the tokenizer used to create a Doc object in Spacy differs significantly from the default tokenizer used when training the model, its performance may be negatively affected, resulting in suboptimal performance.",Limitation of similarity calculation (large text),0,Similary Calculation Issue ,Decider,,C20230706014830146637,0.71,0.76,0.78,0.74,0.82,0.74,0.8,0.85,0.71,0.74,0.83,0.72,0.57,0.57,0.71,0.53,0.61,0.82,0.65,0.78,0.68,0.52,0.88,0.57, 19tokenizer differences can affect performance ,0.01,0.99,1,0
68,20,Spacy,Performance,performance,Performance aspect,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",entity description output,Use intermediary file for displaying entity descriptions.,,1,,,,C20230706014831193636,0.66,0.76,0.66,0.86,0.77,0.8,0.52,0.64,0.56,0.65,0.76,0.66,0.64,0.62,0.7,0.87,0.7,0.66,0.67,0.59,0.36,0.83,0.7,0.76, 20entity description output ,0.08,0.92,1,1
69,20,Spacy,Performance,performance,Performance aspect,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",kb performance issue,Descriptions not stored in KB due to performance.,,1,,,,C20230706014830664743,0.67,0.75,0.8,0.73,0.62,0.55,0.51,0.68,0.55,0.69,0.51,0.49,0.74,0.68,0.54,0.72,0.57,0.58,0.54,0.66,0.68,0.79,0.73,0.84, 20kb performance issue ,0.04,0.96,1,1
70,20,Spacy,Performance,performance,Performance aspect,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",spacy entity linking capability,"Spacy is capable of entity linking, as evidenced by the provided answer.",,1,,,,C20230706014831834027,0.69,0.79,0.79,0.79,0.93,0.77,0.52,0.61,0.42,0.83,0.75,0.8,0.51,0.64,0.66,0.61,0.59,0.67,0.48,0.45,0.59,0.7,0.76,0.71, 20spacy entity linking capability ,0.02,0.98,1,1
71,21,Spacy,Performance,performance,Performance aspect,48169545,Does spacy take as input a list of tokens?,"I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization. Is this possible at all, either with spacy or any other NLP package ?

For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:
I would like to do the same while having as input a list of tokens...","You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.

Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.",input flexibility,Spacy can take an input list of tokens to perform tasks.,,1,,,,C20230706014832772915,0.56,0.65,0.67,0.62,0.63,0.74,0.59,0.58,0.62,0.71,0.67,0.64,0.45,0.67,0.74,0.58,0.37,0.43,0.55,0.56,0.5,0.84,0.64,0.68, 21input flexibility ,0.13,0.87,1,1
72,21,Spacy,Performance,performance,Performance aspect,48169545,Does spacy take as input a list of tokens?,"I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization. Is this possible at all, either with spacy or any other NLP package ?

For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:
I would like to do the same while having as input a list of tokens...","You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.

Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.",performance degradation,Performance degradation may occur if the user's tokenization strategy is significantly different from the reference corpus used in Spacy's statistical models.,,1,,,,C20230706014833291870,0.8,0.66,0.67,0.79,0.47,0.38,0.83,0.81,0.69,0.76,0.87,0.63,0.61,0.53,0.4,0.59,0.34,0.26,0.75,0.83,0.73,0.88,0.67,0.69, 21performance degradation ,0.05,0.95,1,1
73,21,Spacy,Performance,performance,Performance aspect,48169545,Does spacy take as input a list of tokens?,"I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization. Is this possible at all, either with spacy or any other NLP package ?

For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:
I would like to do the same while having as input a list of tokens...","You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.

Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.",processing pipeline,The processing pipeline can be run against already tokenized text in Spacy.,wrong labeling,1,Generic Error,Mixed,,C20230706014905472280,0.74,0.61,0.75,0.79,0.74,0.79,0.58,0.67,0.63,0.87,0.85,0.83,0.57,0.51,0.59,0.7,0.66,0.88,0.45,0.57,0.56,0.51,0.54,0.91, 21processing pipeline ,0.04,0.96,1,1
74,21,Spacy,Performance,performance,Performance aspect,48169545,Does spacy take as input a list of tokens?,"I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization. Is this possible at all, either with spacy or any other NLP package ?

For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:
I would like to do the same while having as input a list of tokens...","You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.

Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.",version compatibility,Spacy 2.0.5 and Python 3 can be used for processing already tokenized text.,,1,,,,C20230706014906069514,0.8,0.74,0.79,0.66,0.57,0.6,0.51,0.75,0.42,0.59,0.63,0.58,0.83,0.72,0.55,0.72,0.53,0.56,0.52,0.72,0.77,0.75,0.68,0.77, 21version compatibility ,0.01,0.99,1,1
75,22,Spacy,Performance,performance,Performance aspect,57049798,Can we find sentences around an entity tagged via NER?,"We have a model ready which identifies a custom named entity. The problem is if the whole doc is given then the model does not work as per expecation if only a few sentences are given, it is giving amazing results.

I want to select two sentences before and after a tagged entity.

eg. If a part of the doc has world Colombo(which is tagged as GPE), I need to select two sentences before the tag and 2 sentences after the tag. I tried a couple of approaches but the complexity is too high.

Is there a built-in way in spacy with which we can address this problem?

I am using python and spacy.

I have tried parsing the doc by identifying the index of the tag. But that approach is really slow.","It might be worth it to see if you can improve the custom named entity recognizer, because it should be unusual for extra context to hurt performance and potentially if you fix that issue it will work better overall.

However, regarding your concrete question about surrounding sentences:

A Token or a Span (an entity is a Span) has a .sent attribute that gives you the covering sentence as a Span. If you look at the tokens right before/after a given sentence's start/end tokens, you can get the previous/next sentences for any token in a document.",built-in functionality,Spacy has a built-in functionality that allows access to the previous and next sentences for any token in a document.,Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014907128270,0.68,0.7,0.71,0.57,0.37,0.64,0.41,0.65,0.82,0.74,0.51,0.77,0.53,0.71,0.52,0.53,0.42,0.58,0.45,0.73,0.78,0.65,0.59,0.84, 22built-in functionality ,0.04,0.96,1,0
76,22,Spacy,Performance,performance,Performance aspect,57049798,Can we find sentences around an entity tagged via NER?,"We have a model ready which identifies a custom named entity. The problem is if the whole doc is given then the model does not work as per expecation if only a few sentences are given, it is giving amazing results.

I want to select two sentences before and after a tagged entity.

eg. If a part of the doc has world Colombo(which is tagged as GPE), I need to select two sentences before the tag and 2 sentences after the tag. I tried a couple of approaches but the complexity is too high.

Is there a built-in way in spacy with which we can address this problem?

I am using python and spacy.

I have tried parsing the doc by identifying the index of the tag. But that approach is really slow.","It might be worth it to see if you can improve the custom named entity recognizer, because it should be unusual for extra context to hurt performance and potentially if you fix that issue it will work better overall.

However, regarding your concrete question about surrounding sentences:

A Token or a Span (an entity is a Span) has a .sent attribute that gives you the covering sentence as a Span. If you look at the tokens right before/after a given sentence's start/end tokens, you can get the previous/next sentences for any token in a document.",improvement suggestion,Improving the named entity recognizer may lead to better performance.,,1,,,,C20230706014906606986,0.74,0.67,0.74,0.9,0.91,0.84,0.63,0.8,0.59,0.65,0.91,0.63,0.44,0.73,0.62,0.87,0.64,0.57,0.62,0.61,0.68,0.71,0.93,0.72, 22improvement suggestion ,0.03,0.97,1,1
77,22,Spacy,Performance,performance,Performance aspect,57049798,Can we find sentences around an entity tagged via NER?,"We have a model ready which identifies a custom named entity. The problem is if the whole doc is given then the model does not work as per expecation if only a few sentences are given, it is giving amazing results.

I want to select two sentences before and after a tagged entity.

eg. If a part of the doc has world Colombo(which is tagged as GPE), I need to select two sentences before the tag and 2 sentences after the tag. I tried a couple of approaches but the complexity is too high.

Is there a built-in way in spacy with which we can address this problem?

I am using python and spacy.

I have tried parsing the doc by identifying the index of the tag. But that approach is really slow.","It might be worth it to see if you can improve the custom named entity recognizer, because it should be unusual for extra context to hurt performance and potentially if you fix that issue it will work better overall.

However, regarding your concrete question about surrounding sentences:

A Token or a Span (an entity is a Span) has a .sent attribute that gives you the covering sentence as a Span. If you look at the tokens right before/after a given sentence's start/end tokens, you can get the previous/next sentences for any token in a document.",previous approach slow,The previous approach of parsing the document by identifying the index of tags is slow.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014908964762,0.78,0.74,0.85,0.64,0.72,0.87,0.77,0.63,0.74,0.74,0.86,0.86,0.68,0.44,0.56,0.61,0.56,0.47,0.73,0.69,0.74,0.62,0.77,0.51, 22previous approach slow ,0.03,0.97,1,0
78,23,Spacy,Performance,performance,Performance aspect,72413618,How can I make it so that I process each sentence to look for and replace with a synonym for the words that match?,"I am currently working with spacy and have a corpus (containing 960,256 words) that looks like this:
I have a function that looks for the synonym of a word (using spacy):

Which returns an array of answers like so:
What I want is to grab the corpus, feed it sentence by sentence and word by word to most_similar so I can save the list of words to replace and do so by using replace_wordthe thing is that I'm not sure how to do this. I've tried for a while but it always fails somehow (either won't take batches so I can't do it at once, the words end up being empty vectors if I simply split each sentence by .split("" "") so...could you help me out please?","I hope I understood what you need correctly. I'm guessing you want to:

Iterate over a corpus
Find specific tokens using the matcher
Find synonyms of the matched tokens
Return a new list of sentences but with the replaced tokens.
If that's the case then what you need is a valid similarity function (I tried the one above but it didn't work for me properly) but you can try this:
You also mentioned that you want this to run on a corpus. I recommend that you use the nlp.pipe() method for performance gains combined with the set_extension method. You can do it like this:",extension with set_extension,User mentions using set_extension to further optimize the nlp.pipe() method.,,1,,,,C20230706014911568190,0.71,0.57,0.48,0.93,0.58,0.59,0.75,0.61,0.64,0.8,0.78,0.76,0.68,0.56,0.66,0.7,0.58,0.73,0.7,0.66,0.61,0.77,0.67,0.77, 23extension with set_extension ,0.27,0.73,1,1
79,23,Spacy,Performance,performance,Performance aspect,72413618,How can I make it so that I process each sentence to look for and replace with a synonym for the words that match?,"I am currently working with spacy and have a corpus (containing 960,256 words) that looks like this:
I have a function that looks for the synonym of a word (using spacy):

Which returns an array of answers like so:
What I want is to grab the corpus, feed it sentence by sentence and word by word to most_similar so I can save the list of words to replace and do so by using replace_wordthe thing is that I'm not sure how to do this. I've tried for a while but it always fails somehow (either won't take batches so I can't do it at once, the words end up being empty vectors if I simply split each sentence by .split("" "") so...could you help me out please?","I hope I understood what you need correctly. I'm guessing you want to:

Iterate over a corpus
Find specific tokens using the matcher
Find synonyms of the matched tokens
Return a new list of sentences but with the replaced tokens.
If that's the case then what you need is a valid similarity function (I tried the one above but it didn't work for me properly) but you can try this:
You also mentioned that you want this to run on a corpus. I recommend that you use the nlp.pipe() method for performance gains combined with the set_extension method. You can do it like this:",issues with similarity function,User reported issues with the similarity function they tried.,,1,,,,C20230706014910516754,0.65,0.61,0.76,0.8,0.82,0.93,0.78,0.57,0.66,0.64,0.76,0.62,0.5,0.45,0.69,0.55,0.5,0.51,0.51,0.54,0.57,0.63,0.74,0.71, 23issues with similarity function ,0.07,0.93,1,1
80,23,Spacy,Performance,performance,Performance aspect,72413618,How can I make it so that I process each sentence to look for and replace with a synonym for the words that match?,"I am currently working with spacy and have a corpus (containing 960,256 words) that looks like this:
I have a function that looks for the synonym of a word (using spacy):

Which returns an array of answers like so:
What I want is to grab the corpus, feed it sentence by sentence and word by word to most_similar so I can save the list of words to replace and do so by using replace_wordthe thing is that I'm not sure how to do this. I've tried for a while but it always fails somehow (either won't take batches so I can't do it at once, the words end up being empty vectors if I simply split each sentence by .split("" "") so...could you help me out please?","I hope I understood what you need correctly. I'm guessing you want to:

Iterate over a corpus
Find specific tokens using the matcher
Find synonyms of the matched tokens
Return a new list of sentences but with the replaced tokens.
If that's the case then what you need is a valid similarity function (I tried the one above but it didn't work for me properly) but you can try this:
You also mentioned that you want this to run on a corpus. I recommend that you use the nlp.pipe() method for performance gains combined with the set_extension method. You can do it like this:",optimization with nlp.pipe(),User seeks to optimize the process by using nlp.pipe().,,1,,,,C20230706014911037638,0.77,0.73,0.74,0.69,0.7,0.77,0.57,0.6,0.6,0.8,0.6,0.64,0.91,0.66,0.82,0.64,0.72,0.65,0.63,0.66,0.67,0.78,0.6,0.67, 23optimization with nlp.pipe() ,0.01,0.99,1,1
81,23,Spacy,Performance,performance,Performance aspect,72413618,How can I make it so that I process each sentence to look for and replace with a synonym for the words that match?,"I am currently working with spacy and have a corpus (containing 960,256 words) that looks like this:
I have a function that looks for the synonym of a word (using spacy):

Which returns an array of answers like so:
What I want is to grab the corpus, feed it sentence by sentence and word by word to most_similar so I can save the list of words to replace and do so by using replace_wordthe thing is that I'm not sure how to do this. I've tried for a while but it always fails somehow (either won't take batches so I can't do it at once, the words end up being empty vectors if I simply split each sentence by .split("" "") so...could you help me out please?","I hope I understood what you need correctly. I'm guessing you want to:

Iterate over a corpus
Find specific tokens using the matcher
Find synonyms of the matched tokens
Return a new list of sentences but with the replaced tokens.
If that's the case then what you need is a valid similarity function (I tried the one above but it didn't work for me properly) but you can try this:
You also mentioned that you want this to run on a corpus. I recommend that you use the nlp.pipe() method for performance gains combined with the set_extension method. You can do it like this:",successful task execution,User is able to find synonyms of words and apply them to their corpus.,Unclear context,0,Generic Error,Mixed,,C20230706014909486757,0.7,0.78,0.66,0.72,0.6,0.58,0.68,0.5,0.65,0.64,0.86,0.65,0.49,0.86,0.53,0.58,0.44,0.57,0.61,0.41,0.43,0.49,0.82,0.49, 23successful task execution ,0.02,0.98,1,0
82,24,Spacy,Performance,performance,Performance aspect,66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy, which is when I process string,

in case the string is a single string object, I have to use nlp(string),

while I have to use nlp.pipe(a list) for a list made of strings elements.
Why is this? I assume this must be something to do with nlp.pipe() behavior which is generator.

What is the reason?","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.

According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.

Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in batches, instead of one-by-one. This is usually much more efficient.
If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.",batch size optimization,Configuring batch size in nlp.pipe optimizes performance according to the system being used.,,1,,,,C20230706014913135871,0.76,0.73,0.67,0.6,0.6,0.97,0.52,0.78,0.71,0.79,0.83,0.93,0.59,0.62,0.58,0.63,0.47,0.73,0.68,0.71,0.73,0.94,0.94,0.96, 24batch size optimization ,0.02,0.98,1,1
83,24,Spacy,Performance,performance,Performance aspect,66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy, which is when I process string,

in case the string is a single string object, I have to use nlp(string),

while I have to use nlp.pipe(a list) for a list made of strings elements.
Why is this? I assume this must be something to do with nlp.pipe() behavior which is generator.

What is the reason?","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.

According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.

Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in batches, instead of one-by-one. This is usually much more efficient.
If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.",efficient generators,Spacy uses generators to efficiently process texts in batches instead of one-by-one.,unknown,1,Generic Error,Mixed,,C20230706014912095974,0.61,0.7,0.49,0.6,0.79,0.69,0.76,0.71,0.68,0.86,0.86,0.88,0.68,0.57,0.61,0.66,0.84,0.8,0.71,0.68,0.76,0.76,0.81,0.89, 24efficient generators ,0.02,0.98,1,1
84,24,Spacy,Performance,performance,Performance aspect,66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy, which is when I process string,

in case the string is a single string object, I have to use nlp(string),

while I have to use nlp.pipe(a list) for a list made of strings elements.
Why is this? I assume this must be something to do with nlp.pipe() behavior which is generator.

What is the reason?","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.

According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.

Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in batches, instead of one-by-one. This is usually much more efficient.
If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.",memory efficiency,"Using generators consumes less memory than processing texts one-by-one, making it more efficient.",,1,,,,C20230706014912610734,0.53,0.66,0.7,0.65,0.67,0.95,0.71,0.83,0.84,0.86,0.74,0.86,0.49,0.6,0.49,0.54,0.56,0.56,0.62,0.69,0.75,0.89,0.85,0.92, 24memory efficiency ,0.16,0.84,1,1
85,24,Spacy,Performance,performance,Performance aspect,66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy, which is when I process string,

in case the string is a single string object, I have to use nlp(string),

while I have to use nlp.pipe(a list) for a list made of strings elements.
Why is this? I assume this must be something to do with nlp.pipe() behavior which is generator.

What is the reason?","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.

According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.

Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in batches, instead of one-by-one. This is usually much more efficient.
If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.",streamlining data processing,Writing a streamer or generator to produce results as needed is recommended for efficient processing of large streams of data.,Too large question and answer,1,Convoluted Challenge,Enquirer,,C20230706014913850593,0.66,0.69,0.54,0.88,0.87,0.86,0.68,0.75,0.62,0.93,0.82,0.83,0.58,0.77,0.67,0.51,0.92,0.53,0.69,0.34,0.61,0.38,0.89,0.36, 24streamlining data processing ,0.01,0.99,1,1
86,25,Spacy,Performance,fast,Performance aspect,58212589,How to check if a sentence is a question with spacy?,"I am using spacy library to build a chat bot. How do I check if a document is a question with a certain confidence? I know how to do relevance, but not sure how to filter statements from questions.

I am looking for something like below:

spacy.load('en_core_web_lg')('Is this a question?').is_question","My first response is to suggest looking for question marks at the end of the sentence.

Otherwise, most questions start with {is, does, do, what, when, where, who, why, what, how}.

There is a more complex answer involving the inclusion of auxiliary verbs and their placement relative to the verb, but if your data is well-formed, this may be sufficient (and fast).",finding common question words,Suggests identifying frequently used question words to identify a question,Unclear context,0,Generic Error,Mixed,,C20230706014915229802,0.55,0.71,0.55,0.75,0.58,0.69,0.53,0.6,0.57,0.69,0.79,0.68,0.49,0.42,0.39,0.52,0.29,0.38,0.6,0.52,0.59,0.64,0.63,0.82, 25finding common question words ,0.32,0.68,1,0
87,25,Spacy,Performance,fast,Performance aspect,58212589,How to check if a sentence is a question with spacy?,"I am using spacy library to build a chat bot. How do I check if a document is a question with a certain confidence? I know how to do relevance, but not sure how to filter statements from questions.

I am looking for something like below:

spacy.load('en_core_web_lg')('Is this a question?').is_question","My first response is to suggest looking for question marks at the end of the sentence.

Otherwise, most questions start with {is, does, do, what, when, where, who, why, what, how}.

There is a more complex answer involving the inclusion of auxiliary verbs and their placement relative to the verb, but if your data is well-formed, this may be sufficient (and fast).",identifying question marks,Suggests looking for question marks at the end of the sentence,,1,,,,C20230706014914703734,0.64,0.59,0.63,0.87,0.56,0.6,0.56,0.6,0.53,0.78,0.72,0.72,0.77,0.63,0.42,0.39,0.81,0.41,0.57,0.26,0.47,0.48,0.7,0.66, 25identifying question marks ,0.35,0.65,1,1
88,25,Spacy,Performance,fast,Performance aspect,58212589,How to check if a sentence is a question with spacy?,"I am using spacy library to build a chat bot. How do I check if a document is a question with a certain confidence? I know how to do relevance, but not sure how to filter statements from questions.

I am looking for something like below:

spacy.load('en_core_web_lg')('Is this a question?').is_question","My first response is to suggest looking for question marks at the end of the sentence.

Otherwise, most questions start with {is, does, do, what, when, where, who, why, what, how}.

There is a more complex answer involving the inclusion of auxiliary verbs and their placement relative to the verb, but if your data is well-formed, this may be sufficient (and fast).",well-formed data,"Acknowledges that identifying questions can be more complex with auxiliary verbs, but might not be necessary depending on the data being well-formed",,1,,,,C20230706014915790700,0.53,0.79,0.73,0.33,0.37,0.45,0.37,0.76,0.61,0.45,0.53,0.58,0.59,0.77,0.68,0.5,0.56,0.51,0.41,0.73,0.65,0.58,0.57,0.71, 25well-formed data ,0.16,0.84,1,1
89,26,Spacy,Performance,fast,Performance aspect,59830899,sense2vec performance improvemets,"I am using Sense2Vec (builds upon Spacy, a fantastic NLP library/tool) and testing the similarity between various words. Unfortunately the performance is quite slow (execution is over a second on a regular MacBook Pro).

Does anyone have a way to speed this up?

Here is the code:

term = ""dog|NOUN""
sense2vec = Sense2Vec().from_disk(""path-to-s2v_reddit_2019_lg"")
sims = sense2vec.most_similar(term, n=top)
I suspect some caching would speed things, but I am not sure which elements should be cached.
","There is indeed a way to speed up Sense2Vec.most_similar. There is a script available from https://github.com/explosion/sense2vec/blob/master/scripts/06_precompute_cache.py, which precomputes a cache of the nearest neighbors. It will then save that cache with the component on disk, making the data larger but the queries fast. If a query is not covered by the cache, most_similar falls back to using the normal computation.",fallback to normal computation,"If a query is not covered by the cache, most_similar still falls back to using the normal computation.",,1,,,,C20230706014917856289,0.73,0.73,0.74,0.62,0.43,0.75,0.75,0.76,0.69,0.67,0.84,0.67,0.52,0.69,0.61,0.29,0.29,0.72,0.4,0.76,0.67,0.45,0.45,0.84, 26fallback to normal computation ,0.2,0.8,1,1
90,26,Spacy,Performance,fast,Performance aspect,59830899,sense2vec performance improvemets,"I am using Sense2Vec (builds upon Spacy, a fantastic NLP library/tool) and testing the similarity between various words. Unfortunately the performance is quite slow (execution is over a second on a regular MacBook Pro).

Does anyone have a way to speed this up?

Here is the code:

term = ""dog|NOUN""
sense2vec = Sense2Vec().from_disk(""path-to-s2v_reddit_2019_lg"")
sims = sense2vec.most_similar(term, n=top)
I suspect some caching would speed things, but I am not sure which elements should be cached.
","There is indeed a way to speed up Sense2Vec.most_similar. There is a script available from https://github.com/explosion/sense2vec/blob/master/scripts/06_precompute_cache.py, which precomputes a cache of the nearest neighbors. It will then save that cache with the component on disk, making the data larger but the queries fast. If a query is not covered by the cache, most_similar falls back to using the normal computation.",precomputing cache,Precomputing a cache of the nearest neighbors can speed up the Sense2Vec.most_similar function.,,1,,,,C20230706014916811414,0.75,0.76,0.6,0.98,0.71,0.69,0.79,0.82,0.75,0.96,0.8,0.81,0.62,0.62,0.6,0.98,0.53,0.49,0.75,0.8,0.72,0.92,0.83,0.81, 26precomputing cache ,0.07,0.93,1,1
91,26,Spacy,Performance,fast,Performance aspect,59830899,sense2vec performance improvemets,"I am using Sense2Vec (builds upon Spacy, a fantastic NLP library/tool) and testing the similarity between various words. Unfortunately the performance is quite slow (execution is over a second on a regular MacBook Pro).

Does anyone have a way to speed this up?

Here is the code:

term = ""dog|NOUN""
sense2vec = Sense2Vec().from_disk(""path-to-s2v_reddit_2019_lg"")
sims = sense2vec.most_similar(term, n=top)
I suspect some caching would speed things, but I am not sure which elements should be cached.
","There is indeed a way to speed up Sense2Vec.most_similar. There is a script available from https://github.com/explosion/sense2vec/blob/master/scripts/06_precompute_cache.py, which precomputes a cache of the nearest neighbors. It will then save that cache with the component on disk, making the data larger but the queries fast. If a query is not covered by the cache, most_similar falls back to using the normal computation.",script available on github,"A script for precomputing the cache is available on Github, which, when saved on disk with the component, can make queries faster.",,1,,,,C20230706014917331461,0.81,0.55,0.77,0.71,0.54,0.6,0.59,0.75,0.36,0.45,0.69,0.31,0.39,0.46,0.55,0.47,0.57,0.49,0.71,0.76,0.75,0.74,0.9,0.79, 26script available on github ,0,1,1,1
92,26,Spacy,Performance,fast,Performance aspect,59830899,sense2vec performance improvemets,"I am using Sense2Vec (builds upon Spacy, a fantastic NLP library/tool) and testing the similarity between various words. Unfortunately the performance is quite slow (execution is over a second on a regular MacBook Pro).

Does anyone have a way to speed this up?

Here is the code:

term = ""dog|NOUN""
sense2vec = Sense2Vec().from_disk(""path-to-s2v_reddit_2019_lg"")
sims = sense2vec.most_similar(term, n=top)
I suspect some caching would speed things, but I am not sure which elements should be cached.
","There is indeed a way to speed up Sense2Vec.most_similar. There is a script available from https://github.com/explosion/sense2vec/blob/master/scripts/06_precompute_cache.py, which precomputes a cache of the nearest neighbors. It will then save that cache with the component on disk, making the data larger but the queries fast. If a query is not covered by the cache, most_similar falls back to using the normal computation.",sense2vec performance issue,The user is experiencing slow performance while testing the similarity between various words using the Sense2Vec library.,,1,,,,C20230706014916296473,0.8,0.66,0.68,0.65,0.7,0.63,0.53,0.67,0.55,0.48,0.7,0.68,0.88,0.79,0.46,0.71,0.56,0.54,0.6,0.36,0.57,0.4,0.52,0.21, 26sense2vec performance issue ,0.05,0.95,1,1
93,27,Spacy,Performance,fast,Performance aspect,48199353,How to use spacy in large dataset with short sentences efficiently?,"I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed.

Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!","You can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.

Rewriting your code block and functionality using the nlp.pipe method would look something like this:
This way puts all your filtering into the token_filter function, which takes in a spacy token and returns True only if it is not punctuation, a space, a stopword, and 4 or less characters. Then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. Then, filtered_tokens is a list of your tokenized documents.

Some helpful references for customizing this pipeline would be:

Token attributes
Language.pipe",multithreading solution,Multithreading in Spacy can create a faster pipeline for tokenization and data ingestion.,,1,,,,C20230706014919007335,0.75,0.61,0.53,0.62,0.71,0.68,0.83,0.2,0.77,0.2,0.77,0.21,0.56,0.54,0.58,0.55,0.59,0.58,0.8,0.54,0.76,0.65,0.82,0.65, 27multithreading solution ,0,1,1,1
94,27,Spacy,Performance,fast,Performance aspect,48199353,How to use spacy in large dataset with short sentences efficiently?,"I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed.

Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!","You can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.

Rewriting your code block and functionality using the nlp.pipe method would look something like this:
This way puts all your filtering into the token_filter function, which takes in a spacy token and returns True only if it is not punctuation, a space, a stopword, and 4 or less characters. Then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. Then, filtered_tokens is a list of your tokenized documents.

Some helpful references for customizing this pipeline would be:

Token attributes
Language.pipe",references for customization,Spacy's token attributes and language.pipe provide helpful reference points for customizing the pipeline.,,1,,,,C20230706014920624687,0.76,0.7,0.74,0.61,0.81,0.65,0.67,0.63,0.59,0.55,0.76,0.53,0.79,0.53,0.51,0.55,0.7,0.66,0.75,0.36,0.54,0.32,0.68,0.18, 27references for customization ,0.02,0.98,1,1
95,27,Spacy,Performance,fast,Performance aspect,48199353,How to use spacy in large dataset with short sentences efficiently?,"I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed.

Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!","You can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.

Rewriting your code block and functionality using the nlp.pipe method would look something like this:
This way puts all your filtering into the token_filter function, which takes in a spacy token and returns True only if it is not punctuation, a space, a stopword, and 4 or less characters. Then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. Then, filtered_tokens is a list of your tokenized documents.

Some helpful references for customizing this pipeline would be:

Token attributes
Language.pipe",spacy performance issues,"Spacy struggles with large datasets of short sentences, causing high memory consumption and system crashes.",,1,,,,C20230706014918530804,0.76,0.77,0.84,0.71,0.6,0.67,0.43,0.58,0.79,0.66,0.61,0.7,0.72,0.8,0.71,0.72,0.51,0.55,0.58,0.54,0.74,0.68,0.65,0.45, 27spacy performance issues ,0.05,0.95,1,1
96,27,Spacy,Performance,fast,Performance aspect,48199353,How to use spacy in large dataset with short sentences efficiently?,"I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed.

Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!","You can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.

Rewriting your code block and functionality using the nlp.pipe method would look something like this:
This way puts all your filtering into the token_filter function, which takes in a spacy token and returns True only if it is not punctuation, a space, a stopword, and 4 or less characters. Then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. Then, filtered_tokens is a list of your tokenized documents.

Some helpful references for customizing this pipeline would be:

Token attributes
Language.pipe",token filtering customization,Customizing the token filtering process can further enhance Spacy's multithreaded performance.,ChatGPT giving out of context response,0,Generic Error,Mixed,,C20230706014920127221,0.77,0.65,0.62,0.77,0.66,0.76,0.73,0.7,0.65,0.9,0.67,0.74,0.63,0.56,0.58,0.65,0.52,0.56,0.72,0.73,0.77,0.86,0.84,0.86, 27token filtering customization ,0.03,0.97,1,0
97,28,Spacy,Performance,fast,Performance aspect,40615717,Spacy.io multithreading with custom pipelines,"I'm trying to use Spacy.io with custom pipelines for language processing, but it seems that when I add a custom function to the pipeline the process runs only on one thread. With default pipeline process uses all the threads specified.

This is how I defined pipeline:

nlp = spacy.load(language, create_pipeline=custom_pipeline)
This is custom_pipeline function:

def custom_pipeline(nlp):
    return (nlp.tagger, score_vocab_pipe)
This is how I run the pipeline:

nlp.pipe(texts, batch_size=1000, n_threads=8)
Are there any requisites for custom pipeline functions to support multithreading with spacy?","This post might give you a better understanding of how the multi-threading is implemented. The multi-threading is described in the docs here.

In short: the tagger doesn't currently release the GIL, so the tagger.pipe() method is just a generator that applies the tagger one-by-one. The tagger should be quite fast for most workloads with one thread per process, especially since it doesn't use much memory. You can see the recipe for multi-processing batch jobs here.

We could release the GIL around the tagger as well, to allow efficient multi-threading. If you'd like to work on this, we can talk about it on the tracker or the spaCy Gitter.",gil not released,"The tagger in Spacy does not currently release the GIL, which means that the tagger.pipe() method applies the tagger one-by-one instead of allowing for efficient multi-threading.",No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706014921055923,0.6,0.69,0.67,0.67,0.66,0.58,0.5,0.9,0.81,0.55,0.61,0.93,0.73,0.54,0.64,0.46,0.57,0.51,0.8,0.78,0.84,0.91,0.88,0.84, 28gil not released ,0,1,1,1
98,28,Spacy,Performance,fast,Performance aspect,40615717,Spacy.io multithreading with custom pipelines,"I'm trying to use Spacy.io with custom pipelines for language processing, but it seems that when I add a custom function to the pipeline the process runs only on one thread. With default pipeline process uses all the threads specified.

This is how I defined pipeline:

nlp = spacy.load(language, create_pipeline=custom_pipeline)
This is custom_pipeline function:

def custom_pipeline(nlp):
    return (nlp.tagger, score_vocab_pipe)
This is how I run the pipeline:

nlp.pipe(texts, batch_size=1000, n_threads=8)
Are there any requisites for custom pipeline functions to support multithreading with spacy?","This post might give you a better understanding of how the multi-threading is implemented. The multi-threading is described in the docs here.

In short: the tagger doesn't currently release the GIL, so the tagger.pipe() method is just a generator that applies the tagger one-by-one. The tagger should be quite fast for most workloads with one thread per process, especially since it doesn't use much memory. You can see the recipe for multi-processing batch jobs here.

We could release the GIL around the tagger as well, to allow efficient multi-threading. If you'd like to work on this, we can talk about it on the tracker or the spaCy Gitter.",multi-processing recipe,There is a recipe for multi-processing batch jobs in Spacy that can help improve performance for multi-threading with custom pipelines.,,1,,,,C20230706014921525223,0.81,0.68,0.62,0.93,0.84,0.84,0.74,0.77,0.62,0.58,0.78,0.56,0.62,0.54,0.57,0.67,0.66,0.67,0.62,0.63,0.58,0.89,0.83,0.8, 28multi-processing recipe ,0.18,0.82,1,1
99,28,Spacy,Performance,fast,Performance aspect,40615717,Spacy.io multithreading with custom pipelines,"I'm trying to use Spacy.io with custom pipelines for language processing, but it seems that when I add a custom function to the pipeline the process runs only on one thread. With default pipeline process uses all the threads specified.

This is how I defined pipeline:

nlp = spacy.load(language, create_pipeline=custom_pipeline)
This is custom_pipeline function:

def custom_pipeline(nlp):
    return (nlp.tagger, score_vocab_pipe)
This is how I run the pipeline:

nlp.pipe(texts, batch_size=1000, n_threads=8)
Are there any requisites for custom pipeline functions to support multithreading with spacy?","This post might give you a better understanding of how the multi-threading is implemented. The multi-threading is described in the docs here.

In short: the tagger doesn't currently release the GIL, so the tagger.pipe() method is just a generator that applies the tagger one-by-one. The tagger should be quite fast for most workloads with one thread per process, especially since it doesn't use much memory. You can see the recipe for multi-processing batch jobs here.

We could release the GIL around the tagger as well, to allow efficient multi-threading. If you'd like to work on this, we can talk about it on the tracker or the spaCy Gitter.",potential optimization,"Releasing the GIL around the tagger in Spacy could allow for efficient multi-threading, which may help optimize the performance for multithreading with custom pipelines.",,1,,,,C20230706014922236947,0.8,0.82,0.74,0.93,0.81,0.82,0.91,0.8,0.76,0.73,0.7,0.73,0.66,0.71,0.57,0.71,0.7,0.65,0.84,0.85,0.57,0.79,0.6,0.69, 28potential optimization ,0.22,0.78,1,1
100,29,Spacy,Performance,fast,Performance aspect,69738938,How to use existing huggingface-transformers model into spacy?,"I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.

My first naive attempt was to load it via spacy.load('bert-base-uncased'), it didn't work because spacy demands a certain structure, which is understandable.

Now I'm trying to figure out how to use the spacy-transformers library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.

I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.
","What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is covered in the docs, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.

To pull from the docs, this is how you specify a custom model in a config:

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
# XXX You can change the model name here
name = ""bert-base-cased""
tokenizer_config = {""use_fast"": true}
Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features. If your HuggingFace model has an NER head or something it will not work. So if you use a custom model, you'll need to train other components, like NER, on top of it.

Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.

all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?",creating spacy/en_core_web_sm model,"The pre-trained model spacy/en_core_web_sm was created through training on a large corpus of data and adjusting the model's parameters until it performed well on defined tasks like part-of-speech tagging, dependency parsing, and named entity recognition.",ChatGPT giving out of context response,0,Generic Error,Mixed,,C20230706014924545034,0.72,0.46,0.71,0.4,0.74,0.29,0.84,0.66,0.77,0.85,0.78,0.65,0.64,0.46,0.63,0.51,0.62,0.45,0.75,0.7,0.67,0.73,0.72,0.68, 29creating spacy/en_core_web_sm model ,0.05,0.95,1,0
101,29,Spacy,Performance,fast,Performance aspect,69738938,How to use existing huggingface-transformers model into spacy?,"I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.

My first naive attempt was to load it via spacy.load('bert-base-uncased'), it didn't work because spacy demands a certain structure, which is understandable.

Now I'm trying to figure out how to use the spacy-transformers library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.

I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.
","What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is covered in the docs, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.

To pull from the docs, this is how you specify a custom model in a config:

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
# XXX You can change the model name here
name = ""bert-base-cased""
tokenizer_config = {""use_fast"": true}
Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features. If your HuggingFace model has an NER head or something it will not work. So if you use a custom model, you'll need to train other components, like NER, on top of it.

Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.

all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?",custom model training,"Even if you use a custom trained HuggingFace model with spacy-transformers, you may still need to train other components, such as NER, that are included in spaCy's built-in models. Training your own custom models from scratch may be necessary if your use case or domain differs significantly from what existing models are prepped for.",wrong labeling,1,Generic Error,Mixed,,C20230706014923290434,0.7,0.76,0.61,0.35,0.44,0.4,0.6,0.74,0.47,0.53,0.53,0.54,0.54,0.41,0.42,0.62,0.56,0.44,0.52,0.41,0.42,0.67,0.79,0.63, 29custom model training ,0.26,0.74,1,1
102,29,Spacy,Performance,fast,Performance aspect,69738938,How to use existing huggingface-transformers model into spacy?,"I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.

My first naive attempt was to load it via spacy.load('bert-base-uncased'), it didn't work because spacy demands a certain structure, which is understandable.

Now I'm trying to figure out how to use the spacy-transformers library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.

I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.
","What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is covered in the docs, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.

To pull from the docs, this is how you specify a custom model in a config:

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
# XXX You can change the model name here
name = ""bert-base-cased""
tokenizer_config = {""use_fast"": true}
Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features. If your HuggingFace model has an NER head or something it will not work. So if you use a custom model, you'll need to train other components, like NER, on top of it.

Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.

all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?",spacy's built-in models,Many situations may not require custom models and spaCy's built-in models are fast to train and can provide comparable performance. Using built-in models as a starting point and then swapping in a Transformer later is also an option.,Diversified question and answers,0,Convoluted Challenge,Enquirer,,C20230706014923957819,0.75,0.6,0.53,0.55,0.6,0.76,0.41,0.64,0.4,0.69,0.79,0.7,0.54,0.38,0.37,0.43,0.53,0.55,0.46,0.45,0.47,0.81,0.68,0.74, 29spacy's built-in models ,0.2,0.8,1,0
103,29,Spacy,Performance,fast,Performance aspect,69738938,How to use existing huggingface-transformers model into spacy?,"I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.

My first naive attempt was to load it via spacy.load('bert-base-uncased'), it didn't work because spacy demands a certain structure, which is understandable.

Now I'm trying to figure out how to use the spacy-transformers library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.

I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.
","What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is covered in the docs, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.

To pull from the docs, this is how you specify a custom model in a config:

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
# XXX You can change the model name here
name = ""bert-base-cased""
tokenizer_config = {""use_fast"": true}
Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features. If your HuggingFace model has an NER head or something it will not work. So if you use a custom model, you'll need to train other components, like NER, on top of it.

Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.

all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?",using transformer component,"Adding a Transformer component to spacy by specifying the name of the HuggingFace model allows you to load your custom model in spacy, but it's important to understand that Transformers are only sources of features, not complete models. ",Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014922768159,0.6,0.49,0.7,0.4,0.52,0.51,0.7,0.74,0.7,0.76,0.79,0.79,0.43,0.44,0.51,0.43,0.27,0.48,0.37,0.32,0.61,0.57,0.68,0.53, 29using transformer component ,0.36,0.64,1,0
104,30,Spacy,Performance,speed,Performance aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",multiprocessing compatibility,Spacy is compatible with Python's multiprocessing module for parallel computation. This means that it's possible to use multiple processes to speed up certain tasks.,,1,,,,C20230706014926247135,0.7,0.45,0.6,0.51,0.69,0.61,0.28,0.33,0.42,0.73,0.74,0.79,0.62,0.67,0.58,0.58,0.58,0.68,0.59,0.79,0.53,0.72,0.78,0.65, 30multiprocessing compatibility ,0.09,0.91,1,1
105,30,Spacy,Performance,speed,Performance aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",n_process argument,"Spacy provides the n_process argument to the nlp.pipe method, which allows users to specify the number of processes to use for a given task. This can further improve performance and make multiprocessing more efficient.",,1,,,,C20230706014926677944,0.7,0.67,0.77,0.8,0.75,0.82,0.67,0.66,0.75,0.83,0.7,0.74,0.43,0.66,0.84,0.55,0.67,0.64,0.57,0.77,0.59,0.41,0.49,0.57, 30n_process argument ,0.23,0.77,1,1
106,30,Spacy,Performance,speed,Performance aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",spacy documentation,The answerer recommends checking the Spacy documentation for more information on using multiprocessing. This suggests that Spacy has provided resources and support on this topic to help users make the most of the library's capabilities.,,0,,,,C20230706014927289054,0.73,0.79,0.61,0.62,0.78,0.52,0.72,0.82,0.62,0.67,0.78,0.77,0.55,0.7,0.63,0.53,0.87,0.47,0.58,0.52,0.31,0.34,0.35,0.48, 30spacy documentation ,0.52,0.48,0,1
107,30,Spacy,Performance,speed,Performance aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",speed faq,The answerer also recommends checking the Speed FAQ for more information on using multiprocessing with Spacy. This resource likely provides additional tips and best practices for improving performance with multiprocessing.,,1,,,,C20230706014927822933,0.68,0.75,0.74,0.69,0.66,0.69,0.64,0.89,0.69,0.61,0.72,0.65,0.73,0.78,0.78,0.72,0.72,0.77,0.57,0.8,0.75,0.51,0.52,0.75, 30speed faq ,0.04,0.96,1,1
108,31,Spacy,Performance,speed,Performance aspect,52897492,spaCy: optimizing tokenization,"I'm currently trying to tokenize a text file where each line is the body text of a tweet:
The file is 59,397 lines long (a day's worth of data) and I'm using spaCy for pre-processing/tokenization. It's currently taking me around 8.5 minutes and I was wondering if there were any way of optimising the following code to be quicker as 8.5 minutes seems awfully long for this process:Although it says files, it's currently only looping over 1 file.

Just to note, I only need this to tokenize the content; I don't need any extra tagging etc.","It sounds like you haven't optimised the pipeline yet. You'll get a significant speed up from disabling the pipeline components you don't need, like so:

nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])    
This should get you down to about the two-minute mark, or better, on its own.

If you need a further speed up, you can look at multi-threading using nlp.pipe. Docs for multi-threading are here: https://spacy.io/usage/processing-pipelines#section-multithreading",disable parser/tagger/ner,"The user can disable the parser, tagger, and NER to improve performance.",,1,,,,C20230706014928864783,0.65,0.5,0.62,0.68,0.94,0.69,0.72,0.73,0.69,0.83,0.88,0.76,0.64,0.77,0.5,0.44,0.51,0.61,0.79,0.28,0.68,0.32,0.85,0.3, 31disable parser/tagger/ner ,0.3,0.7,1,1
109,31,Spacy,Performance,speed,Performance aspect,52897492,spaCy: optimizing tokenization,"I'm currently trying to tokenize a text file where each line is the body text of a tweet:
The file is 59,397 lines long (a day's worth of data) and I'm using spaCy for pre-processing/tokenization. It's currently taking me around 8.5 minutes and I was wondering if there were any way of optimising the following code to be quicker as 8.5 minutes seems awfully long for this process:Although it says files, it's currently only looping over 1 file.

Just to note, I only need this to tokenize the content; I don't need any extra tagging etc.","It sounds like you haven't optimised the pipeline yet. You'll get a significant speed up from disabling the pipeline components you don't need, like so:

nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])    
This should get you down to about the two-minute mark, or better, on its own.

If you need a further speed up, you can look at multi-threading using nlp.pipe. Docs for multi-threading are here: https://spacy.io/usage/processing-pipelines#section-multithreading",optimizing pipeline,Disabling unnecessary components in the pipeline improves efficiency.,,1,,,,C20230706014928342595,0.53,0.59,0.71,0.39,0.48,0.91,0.57,0.7,0.79,0.62,0.65,0.78,0.58,0.73,0.6,0.49,0.67,0.63,0.64,0.71,0.75,0.75,0.75,0.71, 31optimizing pipeline ,0.07,0.93,1,1
110,31,Spacy,Performance,speed,Performance aspect,52897492,spaCy: optimizing tokenization,"I'm currently trying to tokenize a text file where each line is the body text of a tweet:
The file is 59,397 lines long (a day's worth of data) and I'm using spaCy for pre-processing/tokenization. It's currently taking me around 8.5 minutes and I was wondering if there were any way of optimising the following code to be quicker as 8.5 minutes seems awfully long for this process:Although it says files, it's currently only looping over 1 file.

Just to note, I only need this to tokenize the content; I don't need any extra tagging etc.","It sounds like you haven't optimised the pipeline yet. You'll get a significant speed up from disabling the pipeline components you don't need, like so:

nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])    
This should get you down to about the two-minute mark, or better, on its own.

If you need a further speed up, you can look at multi-threading using nlp.pipe. Docs for multi-threading are here: https://spacy.io/usage/processing-pipelines#section-multithreading",speed using multi-threading,The user can also utilize multi-threading with nlp.pipe to further speed up the tokenization process.,unknown/long answer?,1,,,,C20230706014929391515,0.66,0.73,0.66,0.98,0.98,0.98,0.75,0.83,0.75,0.84,0.82,0.8,0.68,0.72,0.75,0.84,0.95,0.79,0.69,0.88,0.8,0.82,0.81,0.88, 31speed using multi-threading ,0.01,0.99,1,1
111,32,Spacy,Performance,speed,Performance aspect,57697374,List most similar words in spaCy in pretrained model,"With Gensim, after I've trained my own model, I can use model.wv.most_similar('cat', topn=5) and get a list of the 5 words that are closest to cat in the vector space. For example:

With spaCy, as per the documentation, I can do:

which gives similarity for tokens in a specified string. But combing through the docs and searching, I can't figure out if there is a gensim-type way of listing all similar words for a preloaded model with either nlp = spacy.load('en_core_web_lg') or nlp = spacy.load('en_vectors_web_lg'). Is there a way to do this?","I used Andy's response and it worked correctly but slowly. To resolve that I took the approach below.

SpaCy uses the cosine similarity, in the backend, to compute .similarity. Therefore, I decided to replace word.similarity(w) with its optimized counterpart. The optimized method that I worked with was cosine_similarity_numba(w.vector, word.vector), shown below, that uses the Numba library to speed up computations. You should replace line 12 in the most_similar method with the line below.
The method became 2-3 times faster which was essential for me.
I explained it in more details in this article: How to Build a Fast ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Most-Similar Words‚Äö√Ñ√∂‚àö√ë‚àöœÄ Method in SpaCy",insufficient context provided,"Without further context and evaluation of the library's performance in other areas, it is difficult to make a strict assessment of Spacy's performance.",Diversified question and answers,1,Convoluted Challenge,Enquirer,,C20230706014931488338,0.68,0.67,0.65,0.76,0.82,0.7,0.86,0.63,0.61,0.68,0.68,0.7,0.64,0.76,0.49,0.59,0.87,0.58,0.7,0.59,0.47,0.54,0.5,0.55, 32insufficient context provided ,0.45,0.55,0,0
112,32,Spacy,Performance,speed,Performance aspect,57697374,List most similar words in spaCy in pretrained model,"With Gensim, after I've trained my own model, I can use model.wv.most_similar('cat', topn=5) and get a list of the 5 words that are closest to cat in the vector space. For example:

With spaCy, as per the documentation, I can do:

which gives similarity for tokens in a specified string. But combing through the docs and searching, I can't figure out if there is a gensim-type way of listing all similar words for a preloaded model with either nlp = spacy.load('en_core_web_lg') or nlp = spacy.load('en_vectors_web_lg'). Is there a way to do this?","I used Andy's response and it worked correctly but slowly. To resolve that I took the approach below.

SpaCy uses the cosine similarity, in the backend, to compute .similarity. Therefore, I decided to replace word.similarity(w) with its optimized counterpart. The optimized method that I worked with was cosine_similarity_numba(w.vector, word.vector), shown below, that uses the Numba library to speed up computations. You should replace line 12 in the most_similar method with the line below.
The method became 2-3 times faster which was essential for me.
I explained it in more details in this article: How to Build a Fast ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Most-Similar Words‚Äö√Ñ√∂‚àö√ë‚àöœÄ Method in SpaCy",limited functionality discussed,The conversation is centered around a specific functionality of the library: finding the most similar words in a preloaded model.,,1,,,,C20230706014930474349,0.74,0.65,0.4,0.44,0.64,0.37,0.46,0.56,0.61,0.47,0.63,0.31,0.56,0.55,0.34,0.61,0.47,0.41,0.42,0.61,0.62,0.65,0.44,0.57, 32limited functionality discussed ,0.31,0.69,1,1
113,32,Spacy,Performance,speed,Performance aspect,57697374,List most similar words in spaCy in pretrained model,"With Gensim, after I've trained my own model, I can use model.wv.most_similar('cat', topn=5) and get a list of the 5 words that are closest to cat in the vector space. For example:

With spaCy, as per the documentation, I can do:

which gives similarity for tokens in a specified string. But combing through the docs and searching, I can't figure out if there is a gensim-type way of listing all similar words for a preloaded model with either nlp = spacy.load('en_core_web_lg') or nlp = spacy.load('en_vectors_web_lg'). Is there a way to do this?","I used Andy's response and it worked correctly but slowly. To resolve that I took the approach below.

SpaCy uses the cosine similarity, in the backend, to compute .similarity. Therefore, I decided to replace word.similarity(w) with its optimized counterpart. The optimized method that I worked with was cosine_similarity_numba(w.vector, word.vector), shown below, that uses the Numba library to speed up computations. You should replace line 12 in the most_similar method with the line below.
The method became 2-3 times faster which was essential for me.
I explained it in more details in this article: How to Build a Fast ‚Äö√Ñ√∂‚àö√ë‚àö‚à´Most-Similar Words‚Äö√Ñ√∂‚àö√ë‚àöœÄ Method in SpaCy",optimization method suggested,The respondent suggests using a faster optimization method for the cosine similarity calculation to improve the speed of similarity computation.,,1,,,,C20230706014930968323,0.55,0.48,0.55,0.63,0.6,0.49,0.58,0.66,0.58,0.87,0.53,0.59,0.7,0.64,0.49,0.52,0.59,0.47,0.67,0.62,0.64,0.67,0.76,0.72, 32optimization method suggested ,0.21,0.79,1,1
114,33,Spacy,Stability,bug,Stability aspect,40595828,"Retrieving the start and end character indices in the original document, for those sentences returned by Spacy","I am using something similar to the following pattern to retrieve the start and end indices of Spacy's sentences in the original document:
This seems to work for those few test docs I used. But worried if I am overlooking any 'gotchas' like spacy sometimes stripping off some characters that I am not aware of . Am I?

PS: If it helps, I need these indices to compare with indices I have from Brat's annotation file.","You should just be able to use the sent.start_char and sent.end_char attributes. These give exactly the indices you're after: https://spacy.io/docs/api/span#attributes

Also doc.text should always equal the original full text. If it doesn't, please submit a bug report.",documentation available,Spacy provides documentation that directly addresses how to retrieve start and end character indices.,,1,,,,C20230706014932685002,0.57,0.58,0.57,0.46,0.57,0.56,0.55,0.44,0.47,0.54,0.78,0.63,0.77,0.69,0.53,0.48,0.58,0.48,0.6,0.55,0.64,0.63,0.69,0.52, 33documentation available ,0.02,0.98,1,1
115,33,Spacy,Stability,bug,Stability aspect,40595828,"Retrieving the start and end character indices in the original document, for those sentences returned by Spacy","I am using something similar to the following pattern to retrieve the start and end indices of Spacy's sentences in the original document:
This seems to work for those few test docs I used. But worried if I am overlooking any 'gotchas' like spacy sometimes stripping off some characters that I am not aware of . Am I?

PS: If it helps, I need these indices to compare with indices I have from Brat's annotation file.","You should just be able to use the sent.start_char and sent.end_char attributes. These give exactly the indices you're after: https://spacy.io/docs/api/span#attributes

Also doc.text should always equal the original full text. If it doesn't, please submit a bug report.",original full text provided,Spacy is able to provide the original full text which suggests it will preserve important details and is unlikely to strip off any characters.,,1,,,,C20230706014933164049,0.36,0.73,0.54,0.5,0.67,0.56,0.46,0.62,0.65,0.4,0.69,0.47,0.41,0.66,0.49,0.42,0.26,0.43,0.55,0.55,0.71,0.66,0.58,0.49, 33original full text provided ,0.1,0.9,1,1
116,33,Spacy,Stability,bug,Stability aspect,40595828,"Retrieving the start and end character indices in the original document, for those sentences returned by Spacy","I am using something similar to the following pattern to retrieve the start and end indices of Spacy's sentences in the original document:
This seems to work for those few test docs I used. But worried if I am overlooking any 'gotchas' like spacy sometimes stripping off some characters that I am not aware of . Am I?

PS: If it helps, I need these indices to compare with indices I have from Brat's annotation file.","You should just be able to use the sent.start_char and sent.end_char attributes. These give exactly the indices you're after: https://spacy.io/docs/api/span#attributes

Also doc.text should always equal the original full text. If it doesn't, please submit a bug report.",relevant attributes available,Spacy provides relevant attributes for retrieving start and end character indices.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706014932209293,0.68,0.56,0.35,0.79,0.58,0.61,0.41,0.39,0.65,0.57,0.61,0.66,0.48,0.52,0.39,0.69,0.43,0.51,0.58,0.59,0.51,0.79,0.73,0.79, 33relevant attributes available ,0.66,0.34,0,0
117,34,Spacy,Stability,bug,Stability aspect,42215762,How to get Spacy to tokenize am/pm expressions correctly?,"Spacy seems unable to correctly tokenize am/pm expressions where the number and the am/pm part is not separated by whitespace.

Example: ""Something happened at 9am"" produces an unexpected token set: List of Token objects: [Something, happened, at, LIKE_URL, am]

While ""Something happened at 9 am"" works just fine: List of Token objects: [Something, happened, at, 9, am]

I notice that if you drill down into the Token object, the 'orth' representation of the token is a 9. Likewise, if I try '8am', the token reads 'IS_UPPER' and the 'orth' representation of the token is 8.

In particular this is a problem for me because I need to track token spans accross a wider context, and a token that shows up as ""8"" in the text but ""IS_UPPER"" in token form causes my spans to be off by 7. (IE the actual document-level index (token.idx) of the next token is off by 7)

How can I process a document with spacy that contains am/pm expressions sometimes denoted in the format described above, and retrieve accurate token spans?

I'm using Python 2.7.12 with Spacy 1.4.0","Turns out, this was reported as a bug about a month ago: https://github.com/explosion/spaCy/issues/736

Upgrading to Spacy 1.6.0 resolves the issue.",active community,"The bug was reported and fixed within a month, indicating an engaged user base.",,0,,,,C20230706014933601721,0.67,0.66,0.42,0.76,0.71,0.87,0.75,0.68,0.63,0.75,0.65,0.65,0.48,0.73,0.5,0.47,0.55,0.62,0.64,0.59,0.71,0.68,0.75,0.56, 34active community ,0.7,0.3,0,1
118,34,Spacy,Stability,bug,Stability aspect,42215762,How to get Spacy to tokenize am/pm expressions correctly?,"Spacy seems unable to correctly tokenize am/pm expressions where the number and the am/pm part is not separated by whitespace.

Example: ""Something happened at 9am"" produces an unexpected token set: List of Token objects: [Something, happened, at, LIKE_URL, am]

While ""Something happened at 9 am"" works just fine: List of Token objects: [Something, happened, at, 9, am]

I notice that if you drill down into the Token object, the 'orth' representation of the token is a 9. Likewise, if I try '8am', the token reads 'IS_UPPER' and the 'orth' representation of the token is 8.

In particular this is a problem for me because I need to track token spans accross a wider context, and a token that shows up as ""8"" in the text but ""IS_UPPER"" in token form causes my spans to be off by 7. (IE the actual document-level index (token.idx) of the next token is off by 7)

How can I process a document with spacy that contains am/pm expressions sometimes denoted in the format described above, and retrieve accurate token spans?

I'm using Python 2.7.12 with Spacy 1.4.0","Turns out, this was reported as a bug about a month ago: https://github.com/explosion/spaCy/issues/736

Upgrading to Spacy 1.6.0 resolves the issue.",limitations and bugs,It's important to thoroughly test and evaluate Spacy or any software before using it in critical applications.,ChatGPT giving out of context response,0,Generic Error,Mixed,,C20230706014934625308,0.72,0.71,0.66,0.4,0.54,0.52,0.58,0.62,0.76,0.45,0.65,0.62,0.43,0.68,0.46,0.28,0.52,0.29,0.57,0.52,0.66,0.6,0.61,0.59, 34limitations and bugs ,0.12,0.88,1,0
119,34,Spacy,Stability,bug,Stability aspect,42215762,How to get Spacy to tokenize am/pm expressions correctly?,"Spacy seems unable to correctly tokenize am/pm expressions where the number and the am/pm part is not separated by whitespace.

Example: ""Something happened at 9am"" produces an unexpected token set: List of Token objects: [Something, happened, at, LIKE_URL, am]

While ""Something happened at 9 am"" works just fine: List of Token objects: [Something, happened, at, 9, am]

I notice that if you drill down into the Token object, the 'orth' representation of the token is a 9. Likewise, if I try '8am', the token reads 'IS_UPPER' and the 'orth' representation of the token is 8.

In particular this is a problem for me because I need to track token spans accross a wider context, and a token that shows up as ""8"" in the text but ""IS_UPPER"" in token form causes my spans to be off by 7. (IE the actual document-level index (token.idx) of the next token is off by 7)

How can I process a document with spacy that contains am/pm expressions sometimes denoted in the format described above, and retrieve accurate token spans?

I'm using Python 2.7.12 with Spacy 1.4.0","Turns out, this was reported as a bug about a month ago: https://github.com/explosion/spaCy/issues/736

Upgrading to Spacy 1.6.0 resolves the issue.",regular updates,"Spacy is regularly updated, suggesting that it is well-tested and constantly being improved.",,0,,,,C20230706014934106832,0.61,0.56,0.47,0.51,0.53,0.98,0.54,0.76,0.79,0.51,0.61,0.69,0.59,0.8,0.5,0.43,0.38,0.53,0.46,0.6,0.55,0.58,0.46,0.72, 34regular updates ,0.87,0.13,0,1
120,35,Spacy,Stability,bug,Stability aspect,52263757,Spacy is_stop doesn't identify stop words?,"When I use SpaCy to identify stopwords, it doesn't work if I use the en_core_web_lg corpus, but it does work when I use en_core_web_sm. Is this a bug, or am I doing something wrong?
However, when I change this line to use the en_core_web_smcorpus, I get different results:","The issue you have is a documented bug. The suggested workaround is the following:

",documented bug,The issue the user experienced is a documented bug in Spacy.,,1,,,,C20230706014935157811,0.82,0.83,0.64,0.79,0.64,0.59,0.56,0.54,0.54,0.85,0.71,0.76,0.78,0.44,0.26,0.8,0.73,0.83,0.6,0.34,0.42,0.58,0.56,0.42, 35documented bug ,0.39,0.61,1,1
121,35,Spacy,Stability,bug,Stability aspect,52263757,Spacy is_stop doesn't identify stop words?,"When I use SpaCy to identify stopwords, it doesn't work if I use the en_core_web_lg corpus, but it does work when I use en_core_web_sm. Is this a bug, or am I doing something wrong?
However, when I change this line to use the en_core_web_smcorpus, I get different results:","The issue you have is a documented bug. The suggested workaround is the following:

",stopwords variation,"The bug only affects stopword identification with en_core_web_lg, while it works with en_core_web_sm.",,1,,,,C20230706014935680274,0.7,0.59,0.72,0.57,0.66,0.71,0.71,0.74,0.52,0.71,0.62,0.57,0.57,0.51,0.6,0.45,0.49,0.67,0.78,0.39,0.7,0.47,0.77,0.43, 35stopwords variation ,0.13,0.87,1,1
122,35,Spacy,Stability,bug,Stability aspect,52263757,Spacy is_stop doesn't identify stop words?,"When I use SpaCy to identify stopwords, it doesn't work if I use the en_core_web_lg corpus, but it does work when I use en_core_web_sm. Is this a bug, or am I doing something wrong?
However, when I change this line to use the en_core_web_smcorpus, I get different results:","The issue you have is a documented bug. The suggested workaround is the following:

",workaround available,The Spacy community has suggested a workaround for the issue.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706014936205527,0.71,0.69,0.69,0.76,0.61,0.54,0.57,0.44,0.52,0.61,0.63,0.58,0.65,0.5,0.5,0.55,0.64,0.47,0.44,0.3,0.58,0.66,0.65,0.61, 35workaround available ,0.56,0.44,0,0
123,36,Spacy,Stability,bug,Stability aspect,62796437,Spacy French langage gives NoneType error,"Here is the code :

import spacy
nlp = spacy.load('fr_core_news_sm')
doc = nlp('Demain je travaille ‚Äö√†√∂‚Äö√Ñ‚Ä† la maison')
for token in doc:
    print(token.text)
It gives the error :

File ""c:\users\ab\appdata\local\programs\python\python37\lib\site-packages\spacy\lang\fr\lemmatizer.py"", line 49, in call if self.is_base_form(univ_pos, morphology): TypeError: 'NoneType' object is not callable
If I change 'fr_core_news_sm' to 'en_core_web_sm', it works well.

My python version is 3.7.7. And Spacy version is 2.3.1.","This seems to be a bug at spaCy 2.3.1: https://github.com/explosion/spaCy/issues/5728

Downgrade to 2.3.0 and it should work: pip install spacy==2.3.0",active support,The quick identification of the issue and the provision of a workaround by the developers suggest that the library is well-supported and actively maintained.,,1,,,,C20230706014937256590,0.83,0.34,0.61,0.32,0.76,0.35,0.55,0.68,0.45,0.59,0.68,0.61,0.49,0.65,0.36,0.56,0.5,0.63,0.59,0.74,0.22,0.6,0.52,0.26, 36active support ,0.1,0.9,1,1
124,36,Spacy,Stability,bug,Stability aspect,62796437,Spacy French langage gives NoneType error,"Here is the code :

import spacy
nlp = spacy.load('fr_core_news_sm')
doc = nlp('Demain je travaille ‚Äö√†√∂‚Äö√Ñ‚Ä† la maison')
for token in doc:
    print(token.text)
It gives the error :

File ""c:\users\ab\appdata\local\programs\python\python37\lib\site-packages\spacy\lang\fr\lemmatizer.py"", line 49, in call if self.is_base_form(univ_pos, morphology): TypeError: 'NoneType' object is not callable
If I change 'fr_core_news_sm' to 'en_core_web_sm', it works well.

My python version is 3.7.7. And Spacy version is 2.3.1.","This seems to be a bug at spaCy 2.3.1: https://github.com/explosion/spaCy/issues/5728

Downgrade to 2.3.0 and it should work: pip install spacy==2.3.0",buggy version,The issue with spaCy French language module that resulted in a NoneType error indicates that the library is not entirely free from bugs.,,1,,,,C20230706014936732650,0.62,0.66,0.54,0.3,0.64,0.43,0.37,0.57,0.68,0.55,0.45,0.45,0.84,0.62,0.42,0.42,0.39,0.26,0.43,0.53,0.6,0.32,0.46,0.78, 36buggy version ,0.02,0.98,1,1
125,36,Spacy,Stability,bug,Stability aspect,62796437,Spacy French langage gives NoneType error,"Here is the code :

import spacy
nlp = spacy.load('fr_core_news_sm')
doc = nlp('Demain je travaille ‚Äö√†√∂‚Äö√Ñ‚Ä† la maison')
for token in doc:
    print(token.text)
It gives the error :

File ""c:\users\ab\appdata\local\programs\python\python37\lib\site-packages\spacy\lang\fr\lemmatizer.py"", line 49, in call if self.is_base_form(univ_pos, morphology): TypeError: 'NoneType' object is not callable
If I change 'fr_core_news_sm' to 'en_core_web_sm', it works well.

My python version is 3.7.7. And Spacy version is 2.3.1.","This seems to be a bug at spaCy 2.3.1: https://github.com/explosion/spaCy/issues/5728

Downgrade to 2.3.0 and it should work: pip install spacy==2.3.0",multilingual modules,"The availability of spaCy modules for multiple languages, including French, demonstrates the library's testing and development for use in different languages.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014937782194,0.78,0.82,0.53,0.73,0.85,0.76,0.41,0.71,0.55,0.71,0.55,0.59,0.49,0.77,0.48,0.62,0.71,0.71,0.62,0.54,0.51,0.61,0.74,0.51, 36multilingual modules ,0.21,0.79,1,0
126,36,Spacy,Stability,bug,Stability aspect,62796437,Spacy French langage gives NoneType error,"Here is the code :

import spacy
nlp = spacy.load('fr_core_news_sm')
doc = nlp('Demain je travaille ‚Äö√†√∂‚Äö√Ñ‚Ä† la maison')
for token in doc:
    print(token.text)
It gives the error :

File ""c:\users\ab\appdata\local\programs\python\python37\lib\site-packages\spacy\lang\fr\lemmatizer.py"", line 49, in call if self.is_base_form(univ_pos, morphology): TypeError: 'NoneType' object is not callable
If I change 'fr_core_news_sm' to 'en_core_web_sm', it works well.

My python version is 3.7.7. And Spacy version is 2.3.1.","This seems to be a bug at spaCy 2.3.1: https://github.com/explosion/spaCy/issues/5728

Downgrade to 2.3.0 and it should work: pip install spacy==2.3.0",overall stability,"Although the conversation indicates that spaCy may have occasional issues, it seems that the library is generally stable and well-tested, based on the available evidence.",Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014938195714,0.81,0.66,0.28,0.64,0.72,0.56,0.53,0.69,0.51,0.57,0.33,0.55,0.4,0.61,0.45,0.63,0.51,0.48,0.29,0.65,0.42,0.6,0.54,0.57, 36overall stability ,0.2,0.8,1,0
127,37,Spacy,Stability,bug,Stability aspect,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.",alternative solution provided,The answer provided an alternative approach to using the lemmatizer that did not address the user's question directly.,wrong labeling,1,Generic Error,Mixed,,C20230706014939876069,0.64,0.64,0.57,0.6,0.91,0.57,0.68,0.64,0.67,0.86,0.93,0.8,0.64,0.77,0.66,0.5,0.82,0.55,0.6,0.63,0.66,0.68,0.83,0.59, 37alternative solution provided ,0.1,0.9,1,1
128,37,Spacy,Stability,bug,Stability aspect,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.",community engagement,"User found a similar question on the topic, indicating a level of community support and engagement with the library.",,1,,,,C20230706014939356483,0.69,0.77,0.72,0.91,0.86,0.85,0.81,0.73,0.75,0.8,0.92,0.78,0.53,0.63,0.75,0.98,0.96,0.95,0.75,0.76,0.37,0.8,0.24,0.37, 37community engagement ,0.02,0.98,1,1
129,37,Spacy,Stability,bug,Stability aspect,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.",insufficient information for stability assessment,"It is difficult to determine the stability or thorough testing of the SpaCy library based on this conversation alone without a thorough analysis of its documentation, source code, and usage in various applications.",Original explanation is not conclusive,1,Generic Error,Mixed,,C20230706014940286001,0.58,0.84,0.47,0.53,0.53,0.7,0.78,0.78,0.48,0.75,0.54,0.51,0.41,0.88,0.5,0.82,0.57,0.6,0.52,0.6,0.51,0.43,0.6,0.41, 37insufficient information for stability assessment ,0.28,0.72,1,1
130,37,Spacy,Stability,bug,Stability aspect,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.",modular design,"SpaCy lemmatizer can be used as a standalone component, suggesting a modular design which may indicate reliability.",,1,,,,C20230706014938831474,0.38,0.46,0.61,0.49,0.7,0.81,0.79,0.6,0.74,0.82,0.79,0.69,0.5,0.73,0.78,0.53,0.63,0.61,0.47,0.71,0.75,0.46,0.51,0.62, 37modular design ,0.01,0.99,1,1
131,38,Spacy,Stability,bug,Stability aspect,64164360,How can I add a specific substring to tokenize on in spaCy?,"I am using spaCy to tokenize a string, and the string is likely to contain a specific substring. If the substring is present, I would like spaCy to treat the substring as a token, regardless of any other rules it has. I would like to keep all other rules intact. Is this possible?

To provide a concrete example, suppose the substring of interest is 'banana'; I want 'I like bananabread.' to be tokenized as ['I', 'like', 'banana', 'bread', '.'].

Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)? I have tried adding 'banana' to the prefixes, suffixes, and infixes, with no success.","Adding the string as a prefix, suffix, and infix should work, but depending on which version of spacy you're using, you may have run into a caching bug while testing. This bug is fixed in v2.2+.

With spacy v2.3.2:

(In v2.1 or earlier, the tokenizer customization still works on a newly loaded nlp, but if you've already processed some texts with the nlp pipeline and then modify the settings, the bug was that it would use the stored tokenization from the cache rather than the new settings.)",active developer community,The clear and helpful answer suggests an active developer community that is knowledgeable about the library and eager to help with issues.,,1,,,,C20230706014942549445,0.59,0.43,0.56,0.43,0.47,0.71,0.77,0.69,0.62,0.86,0.83,0.81,0.61,0.59,0.6,0.38,0.43,0.53,0.83,0.59,0.59,0.66,0.61,0.77, 38active developer community ,0.02,0.98,1,1
132,38,Spacy,Stability,bug,Stability aspect,64164360,How can I add a specific substring to tokenize on in spaCy?,"I am using spaCy to tokenize a string, and the string is likely to contain a specific substring. If the substring is present, I would like spaCy to treat the substring as a token, regardless of any other rules it has. I would like to keep all other rules intact. Is this possible?

To provide a concrete example, suppose the substring of interest is 'banana'; I want 'I like bananabread.' to be tokenized as ['I', 'like', 'banana', 'bread', '.'].

Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)? I have tried adding 'banana' to the prefixes, suffixes, and infixes, with no success.","Adding the string as a prefix, suffix, and infix should work, but depending on which version of spacy you're using, you may have run into a caching bug while testing. This bug is fixed in v2.2+.

With spacy v2.3.2:

(In v2.1 or earlier, the tokenizer customization still works on a newly loaded nlp, but if you've already processed some texts with the nlp pipeline and then modify the settings, the bug was that it would use the stored tokenization from the cache rather than the new settings.)",bug fix in latest version,The answer mentions a caching bug in earlier versions of Spacy that has been fixed in v2.2+.,,1,,,,C20230706014941613282,0.71,0.44,0.49,0.49,0.59,0.53,0.61,0.84,0.73,0.53,0.44,0.84,0.6,0.42,0.52,0.33,0.42,0.39,0.56,0.6,0.71,0.69,0.61,0.77, 38bug fix in latest version ,0.35,0.65,1,1
133,38,Spacy,Stability,bug,Stability aspect,64164360,How can I add a specific substring to tokenize on in spaCy?,"I am using spaCy to tokenize a string, and the string is likely to contain a specific substring. If the substring is present, I would like spaCy to treat the substring as a token, regardless of any other rules it has. I would like to keep all other rules intact. Is this possible?

To provide a concrete example, suppose the substring of interest is 'banana'; I want 'I like bananabread.' to be tokenized as ['I', 'like', 'banana', 'bread', '.'].

Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)? I have tried adding 'banana' to the prefixes, suffixes, and infixes, with no success.","Adding the string as a prefix, suffix, and infix should work, but depending on which version of spacy you're using, you may have run into a caching bug while testing. This bug is fixed in v2.2+.

With spacy v2.3.2:

(In v2.1 or earlier, the tokenizer customization still works on a newly loaded nlp, but if you've already processed some texts with the nlp pipeline and then modify the settings, the bug was that it would use the stored tokenization from the cache rather than the new settings.)",clear solution provided,The answer provides a clear solution to the questioner's problem of adding a specific substring to the tokenizer rules.,,1,,,,C20230706014940738928,0.74,0.39,0.32,0.32,0.33,0.6,0.77,0.7,0.47,0.46,0.29,0.67,0.61,0.43,0.47,0.48,0.31,0.47,0.71,0.78,0.63,0.8,0.68,0.8, 38clear solution provided ,0,1,1,1
134,38,Spacy,Stability,bug,Stability aspect,64164360,How can I add a specific substring to tokenize on in spaCy?,"I am using spaCy to tokenize a string, and the string is likely to contain a specific substring. If the substring is present, I would like spaCy to treat the substring as a token, regardless of any other rules it has. I would like to keep all other rules intact. Is this possible?

To provide a concrete example, suppose the substring of interest is 'banana'; I want 'I like bananabread.' to be tokenized as ['I', 'like', 'banana', 'bread', '.'].

Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)? I have tried adding 'banana' to the prefixes, suffixes, and infixes, with no success.","Adding the string as a prefix, suffix, and infix should work, but depending on which version of spacy you're using, you may have run into a caching bug while testing. This bug is fixed in v2.2+.

With spacy v2.3.2:

(In v2.1 or earlier, the tokenizer customization still works on a newly loaded nlp, but if you've already processed some texts with the nlp pipeline and then modify the settings, the bug was that it would use the stored tokenization from the cache rather than the new settings.)",specific version numbers given,"The answer provides specific version numbers for Spacy, indicating that the library is being actively maintained and updated.",,1,,,,C20230706014942062635,0.59,0.53,0.56,0.42,0.85,0.44,0.65,0.33,0.71,0.52,0.91,0.53,0.7,0.24,0.56,0.47,0.58,0.42,0.74,0.33,0.48,0.64,0.72,0.76, 38specific version numbers given ,0.11,0.89,1,1
135,39,Spacy,Stability,bug,Stability aspect,75724665,How to maximize performance of spaCy on an M1 Mac (currently much slower than Intel),"I've observed that nlp.pipe is 30-40% slower on my almost brand new M1 Pro Macbook than on my old Macbook Pro from 2017. Most other functions are faster on the M1 by a similar margin, so this is not the performance I would expect.

For a benchmark, I'm running the following code (with scispacy):

My M1 Mac takes over 75 seconds to complete the task, while my 2017 Intel Mac can do it in 46 seconds.

I don't know whether spacy uses numpy, but I installed a fast version of numpy using ABarrier's answer to this question. That made numpy faster, but made no difference for spacy. I'm assuming that somewhere there is an unoptimized binary being used, but I don't know how to figure out what it is.
","Install the package thinc-apple-ops:

pip install thinc-apple-ops
Or through the spacy extra:

pip install 'spacy[apple]'
(The unoptimized package is blis. We'd like to be able to switch to a newer version of BLIS with M1 support, but there are still some open bugs.)",limited evidence,The conversation doesn't provide enough evidence to make a definitive assessment of the stability and testing of Spacy.,Original explanation is not conclusive,0,Generic Error,Mixed,,C20230706014943097920,0.51,0.54,0.48,0.87,0.42,0.47,0.39,0.62,0.77,0.47,0.54,0.71,0.53,0.44,0.35,0.44,0.32,0.47,0.43,0.52,0.27,0.57,0.57,0.38, 39limited evidence ,0.52,0.48,0,1
136,39,Spacy,Stability,bug,Stability aspect,75724665,How to maximize performance of spaCy on an M1 Mac (currently much slower than Intel),"I've observed that nlp.pipe is 30-40% slower on my almost brand new M1 Pro Macbook than on my old Macbook Pro from 2017. Most other functions are faster on the M1 by a similar margin, so this is not the performance I would expect.

For a benchmark, I'm running the following code (with scispacy):

My M1 Mac takes over 75 seconds to complete the task, while my 2017 Intel Mac can do it in 46 seconds.

I don't know whether spacy uses numpy, but I installed a fast version of numpy using ABarrier's answer to this question. That made numpy faster, but made no difference for spacy. I'm assuming that somewhere there is an unoptimized binary being used, but I don't know how to figure out what it is.
","Install the package thinc-apple-ops:

pip install thinc-apple-ops
Or through the spacy extra:

pip install 'spacy[apple]'
(The unoptimized package is blis. We'd like to be able to switch to a newer version of BLIS with M1 support, but there are still some open bugs.)",recommendation from spacy team,The recommended solution from the Spacy team implies that the library is generally stable and the team is actively working to address performance issues.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014944065290,0.71,0.61,0.53,0.65,0.69,0.67,0.44,0.64,0.51,0.71,0.82,0.76,0.73,0.74,0.53,0.46,0.49,0.52,0.39,0.54,0.42,0.84,0.8,0.8, 39recommendation from spacy team ,0.36,0.64,1,0
137,39,Spacy,Stability,bug,Stability aspect,75724665,How to maximize performance of spaCy on an M1 Mac (currently much slower than Intel),"I've observed that nlp.pipe is 30-40% slower on my almost brand new M1 Pro Macbook than on my old Macbook Pro from 2017. Most other functions are faster on the M1 by a similar margin, so this is not the performance I would expect.

For a benchmark, I'm running the following code (with scispacy):

My M1 Mac takes over 75 seconds to complete the task, while my 2017 Intel Mac can do it in 46 seconds.

I don't know whether spacy uses numpy, but I installed a fast version of numpy using ABarrier's answer to this question. That made numpy faster, but made no difference for spacy. I'm assuming that somewhere there is an unoptimized binary being used, but I don't know how to figure out what it is.
","Install the package thinc-apple-ops:

pip install thinc-apple-ops
Or through the spacy extra:

pip install 'spacy[apple]'
(The unoptimized package is blis. We'd like to be able to switch to a newer version of BLIS with M1 support, but there are still some open bugs.)",reliability of spacy,The fact that the user is able to run the code successfully on both machines suggests that Spacy is generally reliable.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014943551055,0.69,0.28,0.76,0.72,0.55,0.47,0.3,0.4,0.63,0.39,0.58,0.63,0.43,0.44,0.48,0.54,0.44,0.39,0.33,0.38,0.3,0.87,0.78,0.71, 39reliability of spacy ,0.03,0.97,1,0
138,40,Spacy,Stability,bug,Stability aspect,57109492,PhraseMatcher to match in a different token attribute,"We would like to match a set of phrases using PhraseMatcher. However we whould like to match not only on the verbatim text, but a normalized version of the input. For instance, lower case, with the accents removed, etc.

We have tried to add a custom attibute to the Token, and use it in the init of the PhraseMatcher to match it but, it did not work.

We could transform the text using a custom pipeline but we want to keep the original text to be able to use other components of spacy.
What is the way to do this?","Since EntityRuler is based on PhraseMatcher, I copy here a working example with Spacy v2.2.0. Follow the comments to understand how to work with ""NORM"" attribute from tokens.

At the end, you can see how the word ""F‚Äö√†√∂‚àö√ñCIL"" matches the pattern ""facil"", since it has been normalized.
This bug was fixed in release v2.1.8 https://github.com/explosion/spaCy/issues/4002",active maintenance,"The Spacy team referenced a specific version number, indicating active maintenance and updates.",,1,,,,C20230706014945805788,0.67,0.51,0.66,0.53,0.75,0.46,0.67,0.62,0.63,0.85,0.86,0.87,0.81,0.69,0.68,0.56,0.61,0.33,0.68,0.56,0.71,0.81,0.63,0.66, 40active maintenance ,0.04,0.96,1,1
139,40,Spacy,Stability,bug,Stability aspect,57109492,PhraseMatcher to match in a different token attribute,"We would like to match a set of phrases using PhraseMatcher. However we whould like to match not only on the verbatim text, but a normalized version of the input. For instance, lower case, with the accents removed, etc.

We have tried to add a custom attibute to the Token, and use it in the init of the PhraseMatcher to match it but, it did not work.

We could transform the text using a custom pipeline but we want to keep the original text to be able to use other components of spacy.
What is the way to do this?","Since EntityRuler is based on PhraseMatcher, I copy here a working example with Spacy v2.2.0. Follow the comments to understand how to work with ""NORM"" attribute from tokens.

At the end, you can see how the word ""F‚Äö√†√∂‚àö√ñCIL"" matches the pattern ""facil"", since it has been normalized.
This bug was fixed in release v2.1.8 https://github.com/explosion/spaCy/issues/4002",thorough response,The Spacy team provided specific code examples and referred to a recent bug fix.,,1,,,,C20230706014945394213,0.65,0.63,0.75,0.81,0.88,0.75,0.55,0.61,0.66,0.81,0.95,0.86,0.56,0.55,0.67,0.44,0.62,0.54,0.6,0.71,0.76,0.74,0.65,0.86, 40thorough response ,0.01,0.99,1,1
140,40,Spacy,Stability,bug,Stability aspect,57109492,PhraseMatcher to match in a different token attribute,"We would like to match a set of phrases using PhraseMatcher. However we whould like to match not only on the verbatim text, but a normalized version of the input. For instance, lower case, with the accents removed, etc.

We have tried to add a custom attibute to the Token, and use it in the init of the PhraseMatcher to match it but, it did not work.

We could transform the text using a custom pipeline but we want to keep the original text to be able to use other components of spacy.
What is the way to do this?","Since EntityRuler is based on PhraseMatcher, I copy here a working example with Spacy v2.2.0. Follow the comments to understand how to work with ""NORM"" attribute from tokens.

At the end, you can see how the word ""F‚Äö√†√∂‚àö√ñCIL"" matches the pattern ""facil"", since it has been normalized.
This bug was fixed in release v2.1.8 https://github.com/explosion/spaCy/issues/4002",user experience,The user has already been experimenting with Spacy.,,1,,,,C20230706014944830206,0.5,0.77,0.68,0.57,0.85,0.76,0.31,0.46,0.37,0.56,0.83,0.64,0.64,0.63,0.61,0.64,0.62,0.58,0.43,0.39,0.52,0.43,0.6,0.65, 40user experience ,0.23,0.77,1,1
141,41,Spacy,Stability,bug,Stability aspect,68559878,Force 'parser' to not segment sentences?,"Is there an easy way to tell the ""parser"" pipe not to change the value of Token.is_sent_start ?

So, here is the story: I am working with documents that are pre-sentencized (1 line = 1 sentence), this segmentation is all I need. I realized the parser's segmentation is not always the same as in my documents, so I don't want to rely on the segmentation made by it.

I can't change the segmentation after the parser has done it, so I cannot correct it when it makes mistakes (you get an error). And if I segment the text myself and then apply the parser, it overrules the segmentation I've just made, so it doesn't work.

So, to force keeping the original segmentation and still use a pretrained transformer model (fr_dep_news_trf), I either :

disable the parser,
add a custom Pipe to nlp to set Token.is_sent_start how I want,
create the Doc with nlp(""an example"")
or, I simply create a Doc with

doc = Doc(words=[""an"", ""example""], sent_starts=[True, False])
and then I apply every element of the pipeline except the parser.

However, if I still do need the parser at some point (which I do, because I need to know some subtrees), If I simply apply it on my Doc, it overrules the segmentation already in place, so, in some cases, the segmentation is incorrect. So I do the following workaround:

Keep the correct segmentation in a list sentences = list(doc.sents)
Apply the parser on the doc
Work with whatever syntactic information the parser computed
Retrieve whatever sentencial information I need from the list I previously made, as I now cannot trust Token.is_sent_start.
It works, but it doesn't really feel right imho, it feels a bit messy. Is there an easier, cleaner way I missed ?

Something else I am considering is setting a custom extension, so that I would, for instance, use Token._.is_sent_start instead of the default Token.is_sent_start, and a custom Doc._.sents, but I fear it might be more confusing than helpful ...

Some user suggested using span.merge() for a pretty similar topic, but the function doesn't seem to exist in recent releases of spaCy (Preventing spaCy splitting paragraph numbers into sentences)","The parser is supposed to respect sentence boundaries if they are set in advance. There is one outstanding bug where this doesn't happen, but that was only in the case where some tokens had their sentence boundaries left unset.

If you set all the token boundaries to True or False (not None) and then run the parser, does it overwrite your values? If so it'd be great to have a specific example of that, because that sounds like a bug.

Given that, if you use a custom component to set your true sentence boundaries before the parser, it should work.

Regarding some of your other points...

I don't think it makes any sense to keep your sentence boundaries separate from the parser's - if you do that you can end up with subtrees that span multiple sentences, which will just be weird and unhelpful.

You didn't mention this in your question, but is treating each sentence/line as a separate doc an option? (It's not clear if you're combining multiple lines and the sentence boundaries are wrong, or if you're passing in a single line but it's turning into multiple sentences.)

Thanks a million for you answer! It made me go back to my documents and check every output of every element in the pipeline... Well, I feel terribly stupid, but I simply cannot reproduce my observation of a ""deffective parser"", everything seems fine now. I guess after my initial struggle with forcing my own sentence boundaries (which wasn't even actually that complicated ...), I convinced myself somehow it wasn't working even after my own boundaries were set ? Or I fixed something else in my code ? Terribly sorry. I'll keep an eye on it, might post here again if I find a mistake again",custom component workaround,Using a custom component to set sentence boundaries before running the parser should work if the user wants to override default segmentation.,,1,,,,C20230706014947492313,0.7,0.52,0.52,0.46,0.53,0.8,0.69,0.66,0.63,0.75,0.73,0.85,0.53,0.58,0.47,0.56,0.55,0.8,0.76,0.7,0.74,0.79,0.79,0.8, 41custom component workaround ,0.15,0.85,1,1
142,41,Spacy,Stability,bug,Stability aspect,68559878,Force 'parser' to not segment sentences?,"Is there an easy way to tell the ""parser"" pipe not to change the value of Token.is_sent_start ?

So, here is the story: I am working with documents that are pre-sentencized (1 line = 1 sentence), this segmentation is all I need. I realized the parser's segmentation is not always the same as in my documents, so I don't want to rely on the segmentation made by it.

I can't change the segmentation after the parser has done it, so I cannot correct it when it makes mistakes (you get an error). And if I segment the text myself and then apply the parser, it overrules the segmentation I've just made, so it doesn't work.

So, to force keeping the original segmentation and still use a pretrained transformer model (fr_dep_news_trf), I either :

disable the parser,
add a custom Pipe to nlp to set Token.is_sent_start how I want,
create the Doc with nlp(""an example"")
or, I simply create a Doc with

doc = Doc(words=[""an"", ""example""], sent_starts=[True, False])
and then I apply every element of the pipeline except the parser.

However, if I still do need the parser at some point (which I do, because I need to know some subtrees), If I simply apply it on my Doc, it overrules the segmentation already in place, so, in some cases, the segmentation is incorrect. So I do the following workaround:

Keep the correct segmentation in a list sentences = list(doc.sents)
Apply the parser on the doc
Work with whatever syntactic information the parser computed
Retrieve whatever sentencial information I need from the list I previously made, as I now cannot trust Token.is_sent_start.
It works, but it doesn't really feel right imho, it feels a bit messy. Is there an easier, cleaner way I missed ?

Something else I am considering is setting a custom extension, so that I would, for instance, use Token._.is_sent_start instead of the default Token.is_sent_start, and a custom Doc._.sents, but I fear it might be more confusing than helpful ...

Some user suggested using span.merge() for a pretty similar topic, but the function doesn't seem to exist in recent releases of spaCy (Preventing spaCy splitting paragraph numbers into sentences)","The parser is supposed to respect sentence boundaries if they are set in advance. There is one outstanding bug where this doesn't happen, but that was only in the case where some tokens had their sentence boundaries left unset.

If you set all the token boundaries to True or False (not None) and then run the parser, does it overwrite your values? If so it'd be great to have a specific example of that, because that sounds like a bug.

Given that, if you use a custom component to set your true sentence boundaries before the parser, it should work.

Regarding some of your other points...

I don't think it makes any sense to keep your sentence boundaries separate from the parser's - if you do that you can end up with subtrees that span multiple sentences, which will just be weird and unhelpful.

You didn't mention this in your question, but is treating each sentence/line as a separate doc an option? (It's not clear if you're combining multiple lines and the sentence boundaries are wrong, or if you're passing in a single line but it's turning into multiple sentences.)

Thanks a million for you answer! It made me go back to my documents and check every output of every element in the pipeline... Well, I feel terribly stupid, but I simply cannot reproduce my observation of a ""deffective parser"", everything seems fine now. I guess after my initial struggle with forcing my own sentence boundaries (which wasn't even actually that complicated ...), I convinced myself somehow it wasn't working even after my own boundaries were set ? Or I fixed something else in my code ? Terribly sorry. I'll keep an eye on it, might post here again if I find a mistake again",no major concerns,"Overall, the conversation does not indicate any major issues or concerns with the stability or reliability of Spacy's parser.",,1,,,,C20230706014948095449,0.65,0.61,0.65,0.73,0.59,0.65,0.79,0.52,0.76,0.74,0.86,0.63,0.45,0.47,0.55,0.55,0.4,0.51,0.35,0.33,0.35,0.86,0.78,0.78, 41no major concerns ,0.13,0.87,1,1
143,41,Spacy,Stability,bug,Stability aspect,68559878,Force 'parser' to not segment sentences?,"Is there an easy way to tell the ""parser"" pipe not to change the value of Token.is_sent_start ?

So, here is the story: I am working with documents that are pre-sentencized (1 line = 1 sentence), this segmentation is all I need. I realized the parser's segmentation is not always the same as in my documents, so I don't want to rely on the segmentation made by it.

I can't change the segmentation after the parser has done it, so I cannot correct it when it makes mistakes (you get an error). And if I segment the text myself and then apply the parser, it overrules the segmentation I've just made, so it doesn't work.

So, to force keeping the original segmentation and still use a pretrained transformer model (fr_dep_news_trf), I either :

disable the parser,
add a custom Pipe to nlp to set Token.is_sent_start how I want,
create the Doc with nlp(""an example"")
or, I simply create a Doc with

doc = Doc(words=[""an"", ""example""], sent_starts=[True, False])
and then I apply every element of the pipeline except the parser.

However, if I still do need the parser at some point (which I do, because I need to know some subtrees), If I simply apply it on my Doc, it overrules the segmentation already in place, so, in some cases, the segmentation is incorrect. So I do the following workaround:

Keep the correct segmentation in a list sentences = list(doc.sents)
Apply the parser on the doc
Work with whatever syntactic information the parser computed
Retrieve whatever sentencial information I need from the list I previously made, as I now cannot trust Token.is_sent_start.
It works, but it doesn't really feel right imho, it feels a bit messy. Is there an easier, cleaner way I missed ?

Something else I am considering is setting a custom extension, so that I would, for instance, use Token._.is_sent_start instead of the default Token.is_sent_start, and a custom Doc._.sents, but I fear it might be more confusing than helpful ...

Some user suggested using span.merge() for a pretty similar topic, but the function doesn't seem to exist in recent releases of spaCy (Preventing spaCy splitting paragraph numbers into sentences)","The parser is supposed to respect sentence boundaries if they are set in advance. There is one outstanding bug where this doesn't happen, but that was only in the case where some tokens had their sentence boundaries left unset.

If you set all the token boundaries to True or False (not None) and then run the parser, does it overwrite your values? If so it'd be great to have a specific example of that, because that sounds like a bug.

Given that, if you use a custom component to set your true sentence boundaries before the parser, it should work.

Regarding some of your other points...

I don't think it makes any sense to keep your sentence boundaries separate from the parser's - if you do that you can end up with subtrees that span multiple sentences, which will just be weird and unhelpful.

You didn't mention this in your question, but is treating each sentence/line as a separate doc an option? (It's not clear if you're combining multiple lines and the sentence boundaries are wrong, or if you're passing in a single line but it's turning into multiple sentences.)

Thanks a million for you answer! It made me go back to my documents and check every output of every element in the pipeline... Well, I feel terribly stupid, but I simply cannot reproduce my observation of a ""deffective parser"", everything seems fine now. I guess after my initial struggle with forcing my own sentence boundaries (which wasn't even actually that complicated ...), I convinced myself somehow it wasn't working even after my own boundaries were set ? Or I fixed something else in my code ? Terribly sorry. I'll keep an eye on it, might post here again if I find a mistake again",one outstanding bug,There is one outstanding bug where sentence boundaries are not always respected by the parser.,,1,,,,C20230706014946340865,0.67,0.28,0.68,0.47,0.5,0.63,0.86,0.69,0.72,0.78,0.76,0.67,0.59,0.36,0.35,0.48,0.38,0.61,0.72,0.82,0.65,0.82,0.73,0.73, 41one outstanding bug ,0.12,0.88,1,1
144,41,Spacy,Stability,bug,Stability aspect,68559878,Force 'parser' to not segment sentences?,"Is there an easy way to tell the ""parser"" pipe not to change the value of Token.is_sent_start ?

So, here is the story: I am working with documents that are pre-sentencized (1 line = 1 sentence), this segmentation is all I need. I realized the parser's segmentation is not always the same as in my documents, so I don't want to rely on the segmentation made by it.

I can't change the segmentation after the parser has done it, so I cannot correct it when it makes mistakes (you get an error). And if I segment the text myself and then apply the parser, it overrules the segmentation I've just made, so it doesn't work.

So, to force keeping the original segmentation and still use a pretrained transformer model (fr_dep_news_trf), I either :

disable the parser,
add a custom Pipe to nlp to set Token.is_sent_start how I want,
create the Doc with nlp(""an example"")
or, I simply create a Doc with

doc = Doc(words=[""an"", ""example""], sent_starts=[True, False])
and then I apply every element of the pipeline except the parser.

However, if I still do need the parser at some point (which I do, because I need to know some subtrees), If I simply apply it on my Doc, it overrules the segmentation already in place, so, in some cases, the segmentation is incorrect. So I do the following workaround:

Keep the correct segmentation in a list sentences = list(doc.sents)
Apply the parser on the doc
Work with whatever syntactic information the parser computed
Retrieve whatever sentencial information I need from the list I previously made, as I now cannot trust Token.is_sent_start.
It works, but it doesn't really feel right imho, it feels a bit messy. Is there an easier, cleaner way I missed ?

Something else I am considering is setting a custom extension, so that I would, for instance, use Token._.is_sent_start instead of the default Token.is_sent_start, and a custom Doc._.sents, but I fear it might be more confusing than helpful ...

Some user suggested using span.merge() for a pretty similar topic, but the function doesn't seem to exist in recent releases of spaCy (Preventing spaCy splitting paragraph numbers into sentences)","The parser is supposed to respect sentence boundaries if they are set in advance. There is one outstanding bug where this doesn't happen, but that was only in the case where some tokens had their sentence boundaries left unset.

If you set all the token boundaries to True or False (not None) and then run the parser, does it overwrite your values? If so it'd be great to have a specific example of that, because that sounds like a bug.

Given that, if you use a custom component to set your true sentence boundaries before the parser, it should work.

Regarding some of your other points...

I don't think it makes any sense to keep your sentence boundaries separate from the parser's - if you do that you can end up with subtrees that span multiple sentences, which will just be weird and unhelpful.

You didn't mention this in your question, but is treating each sentence/line as a separate doc an option? (It's not clear if you're combining multiple lines and the sentence boundaries are wrong, or if you're passing in a single line but it's turning into multiple sentences.)

Thanks a million for you answer! It made me go back to my documents and check every output of every element in the pipeline... Well, I feel terribly stupid, but I simply cannot reproduce my observation of a ""deffective parser"", everything seems fine now. I guess after my initial struggle with forcing my own sentence boundaries (which wasn't even actually that complicated ...), I convinced myself somehow it wasn't working even after my own boundaries were set ? Or I fixed something else in my code ? Terribly sorry. I'll keep an eye on it, might post here again if I find a mistake again",user's issue resolved,"The user's issue with the parser seems to have been resolved, possibly due to some error or misunderstanding.",,1,,,,C20230706014946913242,0.8,0.51,0.64,0.48,0.47,0.58,0.77,0.73,0.7,0.95,0.73,0.71,0.5,0.44,0.58,0.34,0.46,0.44,0.52,0.5,0.55,0.89,0.72,0.75, 41user's issue resolved ,0.39,0.61,1,1
145,42,Spacy,Stability,bug,Stability aspect,62737680,Spacy tagger loss is zero while training,"I use this snippet of code to train a tagger in spacy 2.3.0.
The problem is that the loss value is always zero. What am I doing wrong?","Sorry, this is a bug in v2.3.0. It will be fixed in the upcoming v2.3.1. You can train a tagger with spacy train instead or use v2.2.4 in the meanwhile.

If you'd like to have this fix sooner, you can also install from source in the current master branch (the fix is in commit b7107ac8).",active open-source community,The recommendation to install the fix from the current master branch highlights the active community of developers who are contributing to Spacy's stability and reliability.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014950557098,0.73,0.71,0.53,0.52,0.31,0.61,0.27,0.71,0.65,0.41,0.52,0.78,0.54,0.61,0.43,0.83,0.19,0.37,0.42,0.72,0.51,0.39,0.4,0.61, 42active open-source community ,0.14,0.86,1,0
146,42,Spacy,Stability,bug,Stability aspect,62737680,Spacy tagger loss is zero while training,"I use this snippet of code to train a tagger in spacy 2.3.0.
The problem is that the loss value is always zero. What am I doing wrong?","Sorry, this is a bug in v2.3.0. It will be fixed in the upcoming v2.3.1. You can train a tagger with spacy train instead or use v2.2.4 in the meanwhile.

If you'd like to have this fix sooner, you can also install from source in the current master branch (the fix is in commit b7107ac8).",alternative solutions provided,Solutions such as using the previous version or alternative methods to train taggers demonstrate a responsive development team and their attention to user feedback.,,0,,,,C20230706014950030347,0.4,0.54,0.46,0.53,0.48,0.34,0.47,0.58,0.53,0.59,0.74,0.64,0.38,0.5,0.63,0.34,0.3,0.36,0.4,0.43,0.34,0.7,0.68,0.62, 42alternative solutions provided ,0.73,0.27,0,1
147,42,Spacy,Stability,bug,Stability aspect,62737680,Spacy tagger loss is zero while training,"I use this snippet of code to train a tagger in spacy 2.3.0.
The problem is that the loss value is always zero. What am I doing wrong?","Sorry, this is a bug in v2.3.0. It will be fixed in the upcoming v2.3.1. You can train a tagger with spacy train instead or use v2.2.4 in the meanwhile.

If you'd like to have this fix sooner, you can also install from source in the current master branch (the fix is in commit b7107ac8).",bug in v2.3.0,The conversation highlights a specific bug that affects Spacy 2.3.0 and its tagger training functionality.,,1,,,,C20230706014948661418,0.58,0.85,0.65,0.61,0.48,0.44,0.68,0.59,0.57,0.59,0.57,0.59,0.57,0.51,0.48,0.56,0.56,0.59,0.57,0.58,0.52,0.8,0.51,0.29, 42bug in v2.3.0 ,0.16,0.84,1,1
148,42,Spacy,Stability,bug,Stability aspect,62737680,Spacy tagger loss is zero while training,"I use this snippet of code to train a tagger in spacy 2.3.0.
The problem is that the loss value is always zero. What am I doing wrong?","Sorry, this is a bug in v2.3.0. It will be fixed in the upcoming v2.3.1. You can train a tagger with spacy train instead or use v2.2.4 in the meanwhile.

If you'd like to have this fix sooner, you can also install from source in the current master branch (the fix is in commit b7107ac8).",upcoming v2.3.1 release,"The Spacy development team is actively working on a fix for the bug, which indicates regular maintenance and updates of the library.",Similarity Technique Not Enough,1,Similarity Technique Not Enough,Decider,,C20230706014949420980,0.89,0.69,0.48,0.52,0.58,0.3,0.65,0.57,0.55,0.63,0.58,0.66,0.65,0.7,0.41,0.49,0.47,0.5,0.5,0.48,0.44,0.71,0.58,0.63, 42upcoming v2.3.1 release ,0.96,0.04,0,0
149,43,Spacy,Stability,backwards compatibility,Stability aspect,54201004,Multithreading with spacy: Is joblib necessary?,"In this part of the documentation, it is mentioned that nlp.pipe() works in parallel and the following example is given:

for doc in nlp.pipe(texts, batch_size=10000, n_threads=3):
    pass
After that, another, longer example is given that makes use of joblib. I don't quite understand the relationship between the two. As I understand the documentation, if I simply want to parallelize the tokenisation of many documents, the above, simple for loop will work and I do not have to use joblib, right?

My pipeline looks like this:

nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])
When would I need to use joblib?","Based on an answer in Spacy github issues:

We kept the n_threads argument to avoid breaking people's code, but unfortunately the implementation doesn't currently release the GIL, the way we did in v1. In v2 the neural network model is more complicated and more subject to change, so we haven't implemented it in Cython. We might at a later date.

In v2.1.0 (you can get an alpha by installing spacy-nightly, the matrix multiplications are now single-threaded. This makes it safe to launch multiple processes for the pipeline, so we can look at doing that internally. In the meantime, the n_threads argument sits idle...Which I agree is confusing, but removing it and breaking backwards compatibility seems worse.
Thus, to summarize: n_threads doesn't work in v2.1. What I'm doing now is using Spacy with joblib to read a dataset in minibatches.

Spacy released an example for that: Spacy Multiprocessing, and it works perfectly.

I have a dataset with almost ~4M short text. Without using the example they released, it took almost 23 hours to finish parsing them, but using joblib with spacy, it took 1 hour and half to finish!

To reference the readers of this question to Spacy Multiprocessing example: Spacy Multiprocessing",idle n_threads argument,"Despite the update, the n_threads argument remains idle and joblib is still necessary for efficient parallel processing.",,1,,,,C20230706014953903432,0.65,0.54,0.37,0.38,0.61,0.5,0.42,0.72,0.58,0.78,0.67,0.81,0.59,0.52,0.48,0.37,0.53,0.38,0.72,0.67,0.59,0.75,0.79,0.7, 43idle n_threads argument ,0.36,0.64,1,1
150,43,Spacy,Stability,backwards compatibility,Stability aspect,54201004,Multithreading with spacy: Is joblib necessary?,"In this part of the documentation, it is mentioned that nlp.pipe() works in parallel and the following example is given:

for doc in nlp.pipe(texts, batch_size=10000, n_threads=3):
    pass
After that, another, longer example is given that makes use of joblib. I don't quite understand the relationship between the two. As I understand the documentation, if I simply want to parallelize the tokenisation of many documents, the above, simple for loop will work and I do not have to use joblib, right?

My pipeline looks like this:

nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])
When would I need to use joblib?","Based on an answer in Spacy github issues:

We kept the n_threads argument to avoid breaking people's code, but unfortunately the implementation doesn't currently release the GIL, the way we did in v1. In v2 the neural network model is more complicated and more subject to change, so we haven't implemented it in Cython. We might at a later date.

In v2.1.0 (you can get an alpha by installing spacy-nightly, the matrix multiplications are now single-threaded. This makes it safe to launch multiple processes for the pipeline, so we can look at doing that internally. In the meantime, the n_threads argument sits idle...Which I agree is confusing, but removing it and breaking backwards compatibility seems worse.
Thus, to summarize: n_threads doesn't work in v2.1. What I'm doing now is using Spacy with joblib to read a dataset in minibatches.

Spacy released an example for that: Spacy Multiprocessing, and it works perfectly.

I have a dataset with almost ~4M short text. Without using the example they released, it took almost 23 hours to finish parsing them, but using joblib with spacy, it took 1 hour and half to finish!

To reference the readers of this question to Spacy Multiprocessing example: Spacy Multiprocessing",joblib efficacy,"Users report significant reductions in processing time, such as 1 hour and a half for parsing almost 4M short texts, making joblib a reliable alternative to Spacy's multithreading functionality.",,1,,,,C20230706014954559329,0.5,0.52,0.49,0.6,0.56,0.79,0.81,0.55,0.72,0.59,0.6,0.63,0.57,0.55,0.48,0.47,0.46,0.46,0.61,0.45,0.63,0.72,0.85,0.8, 43joblib efficacy ,0.05,0.95,1,1
151,43,Spacy,Stability,backwards compatibility,Stability aspect,54201004,Multithreading with spacy: Is joblib necessary?,"In this part of the documentation, it is mentioned that nlp.pipe() works in parallel and the following example is given:

for doc in nlp.pipe(texts, batch_size=10000, n_threads=3):
    pass
After that, another, longer example is given that makes use of joblib. I don't quite understand the relationship between the two. As I understand the documentation, if I simply want to parallelize the tokenisation of many documents, the above, simple for loop will work and I do not have to use joblib, right?

My pipeline looks like this:

nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])
When would I need to use joblib?","Based on an answer in Spacy github issues:

We kept the n_threads argument to avoid breaking people's code, but unfortunately the implementation doesn't currently release the GIL, the way we did in v1. In v2 the neural network model is more complicated and more subject to change, so we haven't implemented it in Cython. We might at a later date.

In v2.1.0 (you can get an alpha by installing spacy-nightly, the matrix multiplications are now single-threaded. This makes it safe to launch multiple processes for the pipeline, so we can look at doing that internally. In the meantime, the n_threads argument sits idle...Which I agree is confusing, but removing it and breaking backwards compatibility seems worse.
Thus, to summarize: n_threads doesn't work in v2.1. What I'm doing now is using Spacy with joblib to read a dataset in minibatches.

Spacy released an example for that: Spacy Multiprocessing, and it works perfectly.

I have a dataset with almost ~4M short text. Without using the example they released, it took almost 23 hours to finish parsing them, but using joblib with spacy, it took 1 hour and half to finish!

To reference the readers of this question to Spacy Multiprocessing example: Spacy Multiprocessing",limited stability,"Spacy's multithreading functionality is not as stable as it could be, with the n_threads argument not releasing the GIL in version 2.",Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706014951416605,0.44,0.41,0.34,0.61,0.7,0.71,0.49,0.53,0.67,0.53,0.55,0.78,0.54,0.56,0.34,0.77,0.81,0.85,0.44,0.66,0.51,0.67,0.51,0.69, 43limited stability ,0.09,0.91,1,0
152,43,Spacy,Stability,backwards compatibility,Stability aspect,54201004,Multithreading with spacy: Is joblib necessary?,"In this part of the documentation, it is mentioned that nlp.pipe() works in parallel and the following example is given:

for doc in nlp.pipe(texts, batch_size=10000, n_threads=3):
    pass
After that, another, longer example is given that makes use of joblib. I don't quite understand the relationship between the two. As I understand the documentation, if I simply want to parallelize the tokenisation of many documents, the above, simple for loop will work and I do not have to use joblib, right?

My pipeline looks like this:

nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])
When would I need to use joblib?","Based on an answer in Spacy github issues:

We kept the n_threads argument to avoid breaking people's code, but unfortunately the implementation doesn't currently release the GIL, the way we did in v1. In v2 the neural network model is more complicated and more subject to change, so we haven't implemented it in Cython. We might at a later date.

In v2.1.0 (you can get an alpha by installing spacy-nightly, the matrix multiplications are now single-threaded. This makes it safe to launch multiple processes for the pipeline, so we can look at doing that internally. In the meantime, the n_threads argument sits idle...Which I agree is confusing, but removing it and breaking backwards compatibility seems worse.
Thus, to summarize: n_threads doesn't work in v2.1. What I'm doing now is using Spacy with joblib to read a dataset in minibatches.

Spacy released an example for that: Spacy Multiprocessing, and it works perfectly.

I have a dataset with almost ~4M short text. Without using the example they released, it took almost 23 hours to finish parsing them, but using joblib with spacy, it took 1 hour and half to finish!

To reference the readers of this question to Spacy Multiprocessing example: Spacy Multiprocessing",matrix multiplications,"In version 2.1.0, matrix multiplications are single-threaded, making it safe to launch multiple processes for the pipeline.",unknown,1,Generic Error,Mixed,,C20230706014953239715,0.65,0.52,0.55,0.56,0.47,0.75,0.86,0.7,0.63,0.81,0.73,0.78,0.67,0.6,0.7,0.65,0.76,0.78,0.73,0.67,0.73,0.71,0.94,0.75, 43matrix multiplications ,0.01,0.99,1,1
153,44,Spacy,Stability,backwards,Stability aspect,52598788,spaCy: custom attributes not matching correctly?,"I have been having problems using custom extension attributes with the recently improved Matcher (spaCy 2.012). Even a simple example (mostly copied from here) is not working as I expected:
In other words, the rule correctly matches on the span 'have' (1, 2), but incorrectly matches 'have' (5, 6) and 'had' (6, 7). Furthermore, the callback function is not called. The custom attribute appears to be ignored.
The first rule functions as above. Then the second rule triggers, along with the callback function (which prints the message). There is an additional correct match for the new pattern along with the correct and erroneous matches from the first rule.

So, I have a few questions:

why does pattern1 match incorrectly? (i.e. why does the _ custom attribute constraint not apply?)
why does the callback function not work on the first call?
why does it work upon addition of a new rule?
In my own code, when using custom attributes as constraints in subsequent patterns, these patterns match on ALL tokens. I assume this is related to the behaviour exhibited by the code above.","Sorry if this was confusing ‚Äö√Ñ√∂‚àö√ë‚àö¬® but the GitHub thread you're referring to is still only the spec and proposal, i.e. the planned implementation. The changes will hopefully ship with spaCy v2.1.0 (since some of the changes to the Matcher internals are not fully backwards compatible).

While the custom attribute matching isn't implemented yet, the basic improvements to the Matcher engine are already available on the develop branch and in the alpha version via spacy-nightly (pip install spacy-nightly). Those updates likely also resolve the inconsistent behaviour you observed with the callback function.",active development and improvement,"Overall, while the specific feature being discussed may not be fully stable or well tested yet, spaCy as a whole seems to be in active development and improving.",,1,,,,C20230706014956669212,0.61,0.68,0.47,0.68,0.58,0.52,0.62,0.79,0.61,0.8,0.73,0.81,0.45,0.62,0.41,0.54,0.43,0.46,0.68,0.77,0.52,0.93,0.68,0.71, 44active development and improvement ,0.36,0.64,1,1
154,44,Spacy,Stability,backwards,Stability aspect,52598788,spaCy: custom attributes not matching correctly?,"I have been having problems using custom extension attributes with the recently improved Matcher (spaCy 2.012). Even a simple example (mostly copied from here) is not working as I expected:
In other words, the rule correctly matches on the span 'have' (1, 2), but incorrectly matches 'have' (5, 6) and 'had' (6, 7). Furthermore, the callback function is not called. The custom attribute appears to be ignored.
The first rule functions as above. Then the second rule triggers, along with the callback function (which prints the message). There is an additional correct match for the new pattern along with the correct and erroneous matches from the first rule.

So, I have a few questions:

why does pattern1 match incorrectly? (i.e. why does the _ custom attribute constraint not apply?)
why does the callback function not work on the first call?
why does it work upon addition of a new rule?
In my own code, when using custom attributes as constraints in subsequent patterns, these patterns match on ALL tokens. I assume this is related to the behaviour exhibited by the code above.","Sorry if this was confusing ‚Äö√Ñ√∂‚àö√ë‚àö¬® but the GitHub thread you're referring to is still only the spec and proposal, i.e. the planned implementation. The changes will hopefully ship with spaCy v2.1.0 (since some of the changes to the Matcher internals are not fully backwards compatible).

While the custom attribute matching isn't implemented yet, the basic improvements to the Matcher engine are already available on the develop branch and in the alpha version via spacy-nightly (pip install spacy-nightly). Those updates likely also resolve the inconsistent behaviour you observed with the callback function.",custom attribute not implemented,"Based on the conversation, it seems that the custom attribute matching feature in spaCy is still in the planning and development stage, and is not currently implemented.",,1,,,,C20230706014955136282,0.69,0.66,0.54,0.72,0.49,0.51,0.76,0.77,0.85,0.59,0.81,0.78,0.76,0.6,0.55,0.89,0.49,0.47,0.66,0.74,0.63,0.8,0.71,0.84, 44custom attribute not implemented ,0.04,0.96,1,1
155,44,Spacy,Stability,backwards,Stability aspect,52598788,spaCy: custom attributes not matching correctly?,"I have been having problems using custom extension attributes with the recently improved Matcher (spaCy 2.012). Even a simple example (mostly copied from here) is not working as I expected:
In other words, the rule correctly matches on the span 'have' (1, 2), but incorrectly matches 'have' (5, 6) and 'had' (6, 7). Furthermore, the callback function is not called. The custom attribute appears to be ignored.
The first rule functions as above. Then the second rule triggers, along with the callback function (which prints the message). There is an additional correct match for the new pattern along with the correct and erroneous matches from the first rule.

So, I have a few questions:

why does pattern1 match incorrectly? (i.e. why does the _ custom attribute constraint not apply?)
why does the callback function not work on the first call?
why does it work upon addition of a new rule?
In my own code, when using custom attributes as constraints in subsequent patterns, these patterns match on ALL tokens. I assume this is related to the behaviour exhibited by the code above.","Sorry if this was confusing ‚Äö√Ñ√∂‚àö√ë‚àö¬® but the GitHub thread you're referring to is still only the spec and proposal, i.e. the planned implementation. The changes will hopefully ship with spaCy v2.1.0 (since some of the changes to the Matcher internals are not fully backwards compatible).

While the custom attribute matching isn't implemented yet, the basic improvements to the Matcher engine are already available on the develop branch and in the alpha version via spacy-nightly (pip install spacy-nightly). Those updates likely also resolve the inconsistent behaviour you observed with the callback function.",improvements to matcher engine available,"The improvements to the Matcher engine are already available, indicating that other features of spaCy are being actively developed and tested.",,1,,,,C20230706014955645032,0.68,0.64,0.57,0.76,0.5,0.5,0.68,0.74,0.68,0.6,0.76,0.62,0.55,0.53,0.62,0.87,0.36,0.39,0.61,0.64,0.59,0.83,0.6,0.73, 44improvements to matcher engine available ,0.09,0.91,1,1
156,44,Spacy,Stability,backwards,Stability aspect,52598788,spaCy: custom attributes not matching correctly?,"I have been having problems using custom extension attributes with the recently improved Matcher (spaCy 2.012). Even a simple example (mostly copied from here) is not working as I expected:
In other words, the rule correctly matches on the span 'have' (1, 2), but incorrectly matches 'have' (5, 6) and 'had' (6, 7). Furthermore, the callback function is not called. The custom attribute appears to be ignored.
The first rule functions as above. Then the second rule triggers, along with the callback function (which prints the message). There is an additional correct match for the new pattern along with the correct and erroneous matches from the first rule.

So, I have a few questions:

why does pattern1 match incorrectly? (i.e. why does the _ custom attribute constraint not apply?)
why does the callback function not work on the first call?
why does it work upon addition of a new rule?
In my own code, when using custom attributes as constraints in subsequent patterns, these patterns match on ALL tokens. I assume this is related to the behaviour exhibited by the code above.","Sorry if this was confusing ‚Äö√Ñ√∂‚àö√ë‚àö¬® but the GitHub thread you're referring to is still only the spec and proposal, i.e. the planned implementation. The changes will hopefully ship with spaCy v2.1.0 (since some of the changes to the Matcher internals are not fully backwards compatible).

While the custom attribute matching isn't implemented yet, the basic improvements to the Matcher engine are already available on the develop branch and in the alpha version via spacy-nightly (pip install spacy-nightly). Those updates likely also resolve the inconsistent behaviour you observed with the callback function.",updates may not be backwards compatible,"Some of the updates to spaCy may not be fully backwards compatible, indicating that care is being taken to ensure compatibility with existing code.",,1,,,,C20230706014956133774,0.69,0.79,0.52,0.8,0.62,0.66,0.85,0.81,0.75,0.88,0.81,0.78,0.6,0.72,0.48,0.57,0.86,0.54,0.65,0.75,0.58,0.87,0.81,0.88, 44updates may not be backwards compatible ,0.38,0.62,1,1
157,45,Spacy,Stability,backwards,Stability aspect,66503956,"spaCy 3.0, installed files are different from the files in github repo","I installed spaCy 3.0 on my ubuntu. I use ctrl+B to find the definition of class ""sentencizer"" which is in sentencizer.py file:
Why there is no content in the functions defined in sentencizer.py. In the spaCy github repo, there is no sentencizer.py file and the class ""sentencizer"" is defined in sentencizer.pyx:
Why the installed files are different from the github repo? Thanks!","When developing a Python library, small changes are saved in Git as they're made, but they're only released to PyPI when the maintainer intentionally makes a release. So it's normal for the files on your computer to be a little different from the files in a git repo, even if you have a very recent release.

I was really confused about the sentencizer.py code you posted, since there doesn't seem to have ever been a file with that name in spaCy, but it looks like that is a magic PyCharm feature - it's not showing you the actual source code, it's doing some sort of decompilation.

You noticed spaCy has the sentencizer.pyx file. That's compiled into a binary .so file that Python runs when you use the code. PyCharm is presumably working backwards from the .so file.",compiled .so files,The 'sentencizer.pyx' file is compiled into a binary .so file that Python runs when the code is used.,,1,,,,C20230706014958224351,0.72,0.51,0.64,0.71,0.58,0.63,0.53,0.82,0.62,0.48,0.64,0.63,0.48,0.46,0.57,0.72,0.54,0.52,0.58,0.57,0.61,0.86,0.64,0.7, 45compiled .so files ,0.35,0.65,1,1
158,45,Spacy,Stability,backwards,Stability aspect,66503956,"spaCy 3.0, installed files are different from the files in github repo","I installed spaCy 3.0 on my ubuntu. I use ctrl+B to find the definition of class ""sentencizer"" which is in sentencizer.py file:
Why there is no content in the functions defined in sentencizer.py. In the spaCy github repo, there is no sentencizer.py file and the class ""sentencizer"" is defined in sentencizer.pyx:
Why the installed files are different from the github repo? Thanks!","When developing a Python library, small changes are saved in Git as they're made, but they're only released to PyPI when the maintainer intentionally makes a release. So it's normal for the files on your computer to be a little different from the files in a git repo, even if you have a very recent release.

I was really confused about the sentencizer.py code you posted, since there doesn't seem to have ever been a file with that name in spaCy, but it looks like that is a magic PyCharm feature - it's not showing you the actual source code, it's doing some sort of decompilation.

You noticed spaCy has the sentencizer.pyx file. That's compiled into a binary .so file that Python runs when you use the code. PyCharm is presumably working backwards from the .so file.",normal git workflow,"Small changes are saved in Git as they're made, but they're only released to PyPI when the maintainer intentionally makes a release.",,1,,,,C20230706014957222120,0.63,0.66,0.85,0.54,0.51,0.25,0.4,0.89,0.41,0.46,0.5,0.41,0.5,0.52,0.62,0.55,0.46,0.33,0.62,0.52,0.52,0.76,0.48,0.25, 45normal git workflow ,0.03,0.97,1,1
159,45,Spacy,Stability,backwards,Stability aspect,66503956,"spaCy 3.0, installed files are different from the files in github repo","I installed spaCy 3.0 on my ubuntu. I use ctrl+B to find the definition of class ""sentencizer"" which is in sentencizer.py file:
Why there is no content in the functions defined in sentencizer.py. In the spaCy github repo, there is no sentencizer.py file and the class ""sentencizer"" is defined in sentencizer.pyx:
Why the installed files are different from the github repo? Thanks!","When developing a Python library, small changes are saved in Git as they're made, but they're only released to PyPI when the maintainer intentionally makes a release. So it's normal for the files on your computer to be a little different from the files in a git repo, even if you have a very recent release.

I was really confused about the sentencizer.py code you posted, since there doesn't seem to have ever been a file with that name in spaCy, but it looks like that is a magic PyCharm feature - it's not showing you the actual source code, it's doing some sort of decompilation.

You noticed spaCy has the sentencizer.pyx file. That's compiled into a binary .so file that Python runs when you use the code. PyCharm is presumably working backwards from the .so file.",pycharm decompilation feature,The code identified by the user as being in the file 'sentencizer.py' is not the actual source code - it seems to be a result of the PyCharm decompilation feature.,,1,,,,C20230706014957696750,0.62,0.77,0.77,0.66,0.79,0.69,0.54,0.7,0.46,0.72,0.72,0.55,0.47,0.61,0.57,0.54,0.46,0.61,0.68,0.59,0.47,0.69,0.63,0.67, 45pycharm decompilation feature ,0.29,0.71,1,1
160,46,Spacy,Stability,version,Stability aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",library maintenance,"The respondent is a maintainer of the spaCy library, indicating that the library has a dedicated team of developers actively working on it.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706014958946853,0.74,0.79,0.64,0.87,0.71,0.78,0.76,0.74,0.59,0.88,0.78,0.85,0.65,0.69,0.43,0.76,0.65,0.81,0.71,0.65,0.07,0.8,0.07,0.15, 46library maintenance ,0.03,0.97,1,0
161,46,Spacy,Stability,version,Stability aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",ongoing investigation,"The respondent mentions that they are investigating the installation issue, which suggests a commitment to maintaining and improving the library.",,1,,,,C20230706014959803445,0.62,0.41,0.67,0.71,0.87,0.73,0.81,0.44,0.76,0.57,0.89,0.65,0.34,0.53,0.48,0.66,0.51,0.56,0.27,0.55,0.15,0.49,0.09,-0.01, 46ongoing investigation ,0.05,0.95,1,1
162,47,Spacy,Stability,version,Stability aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",active maintenance,The maintainer of the library is actively engaging with users on how to improve their experience with the library.,,0,,,,C20230706015000328892,0.72,0.6,0.46,0.88,0.53,0.53,0.7,0.72,0.53,0.67,0.61,0.8,0.6,0.72,0.57,0.6,0.5,0.76,0.48,0.87,0.23,0.63,0.81,0.46, 47active maintenance ,0.71,0.29,0,1
163,47,Spacy,Stability,version,Stability aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",community involvement,"The existence of a community of Spacy users who are active in developing and improving its functionality indicates that it is a reliable, stable, and efficient library for natural language processing.",Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706015001398195,0.67,0.61,0.52,0.82,0.7,0.63,0.97,0.86,0.8,0.83,0.77,0.74,0.56,0.69,0.57,0.72,0.89,0.69,0.75,0.83,0.63,0.88,0.71,0.72, 47community involvement ,0.05,0.95,1,0
164,47,Spacy,Stability,version,Stability aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",prodigy tool,"The development of Prodigy, an annotation tool that integrates with Spacy and helps users train and evaluate models faster, is a testament to the stability and sophistication of the library.",Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015000849466,0.64,0.67,0.51,0.8,0.62,0.63,0.82,0.83,0.65,0.9,0.43,0.45,0.6,0.7,0.52,0.64,0.63,0.57,0.84,0.76,0.7,0.77,0.79,0.71, 47prodigy tool ,0.5,0.5,0,0
165,48,Spacy,Stability,version,Stability aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",active maintenance,The Spacy maintainer is active and working to fix issues with model downloads.,,1,,,,C20230706015001939199,0.7,0.55,0.6,0.47,0.51,0.53,0.45,0.59,0.64,0.42,0.48,0.72,0.4,0.41,0.57,0.54,0.49,0.55,0.53,0.56,0.59,0.76,0.81,0.6, 48active maintenance ,0.1,0.9,1,1
166,48,Spacy,Stability,version,Stability aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",manual model download,"In the meantime, models can be downloaded manually and placed in the appropriate folder.",,1,,,,C20230706015002416155,0.53,0.44,0.42,0.53,0.63,0.55,0.51,0.41,0.28,0.53,0.61,0.46,0.4,0.45,0.46,0.51,0.46,0.52,0.49,0.51,0.44,0.76,0.74,0.84, 48manual model download ,0.09,0.91,1,1
167,48,Spacy,Stability,version,Stability aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",multi-language support,The availability of models for multiple languages suggests that the library has been thoroughly tested.,Unclear context,1,Generic Error,Mixed,,C20230706015003472868,0.66,0.54,0.65,0.57,0.72,0.72,0.41,0.71,0.83,0.35,0.43,0.66,0.4,0.47,0.49,0.36,0.49,0.51,0.3,0.63,0.47,0.54,0.69,0.63, 48multi-language support ,0.7,0.3,0,0
168,48,Spacy,Stability,version,Stability aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",native python packages,"Models are now hosted on GitHub and wrapped as native Python packages, which makes managing model dependencies easier.",,1,,,,C20230706015002949755,0.82,0.48,0.54,0.76,0.79,0.83,0.28,0.7,0.63,0.27,0.31,0.78,0.64,0.35,0.55,0.64,0.73,0.81,0.56,0.77,0.84,0.74,0.69,0.8, 48native python packages ,0.12,0.88,1,1
169,49,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",active issue tracker,The presence of an issue tracker indicates an active approach to addressing reported issues and ensuring the library operates efficiently.,,0,,,,C20230706015005062066,0.73,0.5,0.65,0.67,0.44,0.53,0.81,0.81,0.47,0.94,0.64,0.65,0.53,0.37,0.76,0.45,0.64,0.32,0.78,0.69,0.33,0.75,0.35,0.4, 49active issue tracker ,0.6,0.4,0,1
170,49,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",ongoing issue investigation,"The spaCy maintainer indicated that they were investigating the issue to prevent it from affecting future users, demonstrating a focus on maintaining the library.",Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015004523526,0.61,0.73,0.62,0.56,0.64,0.41,0.59,0.83,0.38,0.7,0.58,0.45,0.5,0.7,0.57,0.59,0.52,0.29,0.38,0.81,0.33,0.46,0.45,0.38, 49ongoing issue investigation ,0.51,0.49,0,0
171,49,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",proactive approach to issue resolution,"The spaCy maintainer provided a specific command to address the user's issue, indicating a proactive and knowledgeable approach to resolving problems.",,1,,,,C20230706015005571661,0.76,0.69,0.71,0.77,0.84,0.74,0.79,0.79,0.82,0.74,0.82,0.73,0.58,0.76,0.66,0.61,0.57,0.44,0.85,0.77,0.54,0.78,0.63,0.55, 49proactive approach to issue resolution ,0.27,0.73,1,1
172,49,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",prompt response time,"The spaCy maintainer provided a prompt response to the user's issue, indicating an active and responsive maintenance process.",,1,,,,C20230706015003997792,0.79,0.59,0.48,0.75,0.85,0.81,0.76,0.82,0.78,0.63,0.79,0.69,0.64,0.61,0.65,0.68,0.73,0.64,0.83,0.82,0.81,0.79,0.76,0.78, 49prompt response time ,0.2,0.8,1,1
173,50,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",maintainer response,The answer was provided by one of the maintainers of the SpaCy library.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015006097079,0.46,0.77,0.54,0.78,0.86,0.8,0.64,0.66,0.67,0.58,0.72,0.78,0.25,0.58,0.26,0.66,0.61,0.48,0.36,0.5,0.38,0.63,0.81,0.61, 50maintainer response ,0.91,0.09,0,0
174,50,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",prodigy tool development,"The answer mentioned the development of a tool called Prodigy, which suggests that the SpaCy library is actively maintained and updated.",Diversified question and answers,1,Convoluted Challenge,Enquirer,,C20230706015006612277,0.74,0.58,0.71,0.9,0.84,0.94,0.85,0.87,0.76,0.88,0.68,0.69,0.52,0.51,0.46,0.79,0.78,0.94,0.8,0.71,0.82,0.82,0.87,0.77, 50prodigy tool development ,0.78,0.22,0,0
175,50,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",reference to brat tool,"The answer provided a recommendation for another tool called Brat, which suggests that the SpaCy maintainers are aware of and keeping up with other tools in the field.",Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015007205217,0.69,0.55,0.87,0.57,0.92,0.59,0.78,0.78,0.76,0.83,0.68,0.68,0.64,0.79,0.89,0.51,0.83,0.51,0.71,0.74,0.66,0.68,0.56,0.47, 50reference to brat tool ,0.01,0.99,1,1
176,51,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",active maintenance,Spacy is actively maintained by a dedicated team of developers who work to improve the library and make it more user-friendly.,,1,,,,C20230706015007862634,0.68,0.36,0.61,0.54,0.83,0.72,0.63,0.51,0.69,0.87,0.9,0.82,0.62,0.61,0.67,0.58,0.68,0.62,0.69,0.5,0.8,0.65,0.8,0.66, 51active maintenance ,0.01,0.99,1,1
177,51,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",awareness of potential issues,The responder is aware of potential issues that may arise during the upgrading process and provides information on training models on different versions of Spacy.,,1,,,,C20230706015009625687,0.59,0.47,0.67,0.54,0.89,0.59,0.64,0.48,0.53,0.77,0.87,0.81,0.44,0.52,0.46,0.62,0.94,0.65,0.39,0.4,0.44,0.76,0.77,0.76, 51awareness of potential issues ,0.02,0.98,1,1
178,51,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",recommendation for upgrade,The responder suggests upgrading to a more recent version of Spacy as models are more transparently hosted on GitHub and easier to manage.,,1,,,,C20230706015009016356,0.43,0.57,0.8,0.71,0.63,0.6,0.66,0.63,0.59,0.51,0.53,0.84,0.59,0.54,0.5,0.64,0.58,0.64,0.62,0.51,0.51,0.71,0.61,0.57, 51recommendation for upgrade ,0.05,0.95,1,1
179,51,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server ‚Äö√Ñ√∂‚àö√ë‚àö¬® this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release ‚Äö√Ñ√∂‚àö√ë‚àö¬® so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",solution provided,The responder provides a solution by offering a manual download of models while they work on fixing the download server issue.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015008376076,0.54,0.63,0.51,0.49,0.71,0.44,0.59,0.61,0.71,0.68,0.76,0.82,0.42,0.61,0.46,0.42,0.6,0.43,0.58,0.41,0.61,0.8,0.88,0.83, 51solution provided ,0.41,0.59,0,0
180,52,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,55852115,Token extension versus matcher versus phrase matcher vs entity ruler in spaCy,"I am trying to figure out the best way (fast) to extract entities, e.g. a month. I have come up with 5 different approaches using spaCy.

Initial setup

For each solution I start with an initial setup
Conclusion

The custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable. The EntityRuler seems to be the slowest which isn't surprising since it is changing the Doc.ents. It is however quite convenient that you have your matches in Doc.ents so you might want to consider this method still.

I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite:

If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall
Question

Am I missing something important here or can I trust this analysis on a larger scale?","I think ultimately, it all comes down to finding the optimal tradeoff between speed, maintainability of the code and the way this piece of logic fits into the larger picture of your application. Finding a few strings in a text is unlikely to be the end goal of what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® otherwise, you probably wouldn't be using spaCy and would stick to regular expressions. How your application needs to ""consume"" the result of the matching and what the matches mean in the larger context should motivate the approach you choose.

As you mention in the conclusion, if your matches are ""named entities"" by definition, adding them to the doc.ents makes a lot of sense and will even give you an easy way to combine your logic with statistical predictions. Even if it adds slightly more overhead, it'll likely still outperform any scaffolding you'd otherwise have to write around it yourself.",community engagement,"Users seeking guidance on best practices and sharing experiences show a level of community engagement, which can be an indicator of an active and supportive developer community.",Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706015010684020,0.54,0.67,0.43,0.6,0.32,0.28,0.86,0.64,0.24,0.76,0.3,0.45,0.34,0.43,0.6,0.45,0.5,0.27,0.18,0.71,0.23,0.41,0.77,0.49, 52community engagement ,0.06,0.94,1,0
181,52,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,55852115,Token extension versus matcher versus phrase matcher vs entity ruler in spaCy,"I am trying to figure out the best way (fast) to extract entities, e.g. a month. I have come up with 5 different approaches using spaCy.

Initial setup

For each solution I start with an initial setup
Conclusion

The custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable. The EntityRuler seems to be the slowest which isn't surprising since it is changing the Doc.ents. It is however quite convenient that you have your matches in Doc.ents so you might want to consider this method still.

I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite:

If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall
Question

Am I missing something important here or can I trust this analysis on a larger scale?","I think ultimately, it all comes down to finding the optimal tradeoff between speed, maintainability of the code and the way this piece of logic fits into the larger picture of your application. Finding a few strings in a text is unlikely to be the end goal of what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® otherwise, you probably wouldn't be using spaCy and would stick to regular expressions. How your application needs to ""consume"" the result of the matching and what the matches mean in the larger context should motivate the approach you choose.

As you mention in the conclusion, if your matches are ""named entities"" by definition, adding them to the doc.ents makes a lot of sense and will even give you an easy way to combine your logic with statistical predictions. Even if it adds slightly more overhead, it'll likely still outperform any scaffolding you'd otherwise have to write around it yourself.",continued development,The longevity and maintenance of the Spacy library will depend on continued engagement and support from its developers and users.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015011149537,0.81,0.62,0.71,0.82,0.55,0.54,0.71,0.6,0.69,0.82,0.72,0.64,0.55,0.69,0.83,0.6,0.57,0.46,0.53,0.59,0.57,0.72,0.35,0.38, 52continued development ,0.09,0.91,1,0
182,52,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,55852115,Token extension versus matcher versus phrase matcher vs entity ruler in spaCy,"I am trying to figure out the best way (fast) to extract entities, e.g. a month. I have come up with 5 different approaches using spaCy.

Initial setup

For each solution I start with an initial setup
Conclusion

The custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable. The EntityRuler seems to be the slowest which isn't surprising since it is changing the Doc.ents. It is however quite convenient that you have your matches in Doc.ents so you might want to consider this method still.

I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite:

If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall
Question

Am I missing something important here or can I trust this analysis on a larger scale?","I think ultimately, it all comes down to finding the optimal tradeoff between speed, maintainability of the code and the way this piece of logic fits into the larger picture of your application. Finding a few strings in a text is unlikely to be the end goal of what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® otherwise, you probably wouldn't be using spaCy and would stick to regular expressions. How your application needs to ""consume"" the result of the matching and what the matches mean in the larger context should motivate the approach you choose.

As you mention in the conclusion, if your matches are ""named entities"" by definition, adding them to the doc.ents makes a lot of sense and will even give you an easy way to combine your logic with statistical predictions. Even if it adds slightly more overhead, it'll likely still outperform any scaffolding you'd otherwise have to write around it yourself.",usage in conversation,The fact that Spacy is still being discussed suggests it is relevant and widely used in natural language processing.,,0,,,,C20230706015010038375,0.57,0.55,0.45,0.8,0.67,0.74,0.76,0.75,0.7,0.86,0.77,0.87,0.47,0.54,0.59,0.68,0.51,0.64,0.46,0.78,0.53,0.57,0.75,0.48, 52usage in conversation ,0.79,0.21,0,1
183,53,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44176829,Python Spacy error: RuntimeError: Language not supported,"I am going to add new entities to the own spacy data model ""mymodel"". Before I have installed ""mymodel"" using this tutorial and it was working good. When I want to use ""mymodel"" to add new entities I have a misunderstanding. Please, help me
","The problem here is that spacy.load() currently expects either a language ID (e.g. 'en'), or a shortcut link to a model that tells spaCy where to find the data. Because spaCy can't find a shortcut link, it assumes that 'my_model' is a language, which obviously doesn't exist.

You can set up a link for your model like this:

python -m spacy link my_model my_model # if it's installed via pip, or:
python -m spacy link /path/to/my_model/data my_model
This will create a symlink in the /spacy/data directory, so you should run it with admin permissions.

Alternatively, if you've created a model package that can be installed via pip, you can simply install and import it, and then call its load() method with no arguments:

import my_model
nlp = my_model.load()
In some cases, this way of loading models is actually more convenient, as it's cleaner and lets you debug your code more easily. For example, if a model doesn't exist, Python will raise an ImportError immediately. Similarly, if loading fails, you know there's likely a problem with the model's own loading and meta.

Btw: I'm one of the spaCy maintainers and I agree that the way spacy.load() currently works is definitely unideal and confusing. We're looking forward to finally changing this with the next major release. We're very close to releasing the first alpha of v2.0, which will solve this problem more elegantly, and will also include a lot of improvements to the training process and documentation.",acknowledgment of confusing method,The maintainer explicitly acknowledged that the method of loading models is currently unideal and confusing.,,1,,,,C20230706015012382461,0.58,0.79,0.65,0.79,0.54,0.67,0.51,0.82,0.54,0.47,0.35,0.58,0.47,0.39,0.68,0.41,0.43,0.49,0.58,0.65,0.65,0.73,0.56,0.79, 53acknowledgment of confusing method ,0.13,0.87,1,1
184,53,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44176829,Python Spacy error: RuntimeError: Language not supported,"I am going to add new entities to the own spacy data model ""mymodel"". Before I have installed ""mymodel"" using this tutorial and it was working good. When I want to use ""mymodel"" to add new entities I have a misunderstanding. Please, help me
","The problem here is that spacy.load() currently expects either a language ID (e.g. 'en'), or a shortcut link to a model that tells spaCy where to find the data. Because spaCy can't find a shortcut link, it assumes that 'my_model' is a language, which obviously doesn't exist.

You can set up a link for your model like this:

python -m spacy link my_model my_model # if it's installed via pip, or:
python -m spacy link /path/to/my_model/data my_model
This will create a symlink in the /spacy/data directory, so you should run it with admin permissions.

Alternatively, if you've created a model package that can be installed via pip, you can simply install and import it, and then call its load() method with no arguments:

import my_model
nlp = my_model.load()
In some cases, this way of loading models is actually more convenient, as it's cleaner and lets you debug your code more easily. For example, if a model doesn't exist, Python will raise an ImportError immediately. Similarly, if loading fails, you know there's likely a problem with the model's own loading and meta.

Btw: I'm one of the spaCy maintainers and I agree that the way spacy.load() currently works is definitely unideal and confusing. We're looking forward to finally changing this with the next major release. We're very close to releasing the first alpha of v2.0, which will solve this problem more elegantly, and will also include a lot of improvements to the training process and documentation.",improvements coming in next release,"The maintainer stated that the next major release will solve this problem more elegantly, and will also include improvements to the training process and documentation.",Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015012910461,0.51,0.58,0.56,0.63,0.45,0.6,0.6,0.71,0.76,0.85,0.63,0.67,0.53,0.58,0.41,0.62,0.48,0.46,0.55,0.66,0.4,0.72,0.76,0.72, 53improvements coming in next release ,0.95,0.05,0,0
185,53,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44176829,Python Spacy error: RuntimeError: Language not supported,"I am going to add new entities to the own spacy data model ""mymodel"". Before I have installed ""mymodel"" using this tutorial and it was working good. When I want to use ""mymodel"" to add new entities I have a misunderstanding. Please, help me
","The problem here is that spacy.load() currently expects either a language ID (e.g. 'en'), or a shortcut link to a model that tells spaCy where to find the data. Because spaCy can't find a shortcut link, it assumes that 'my_model' is a language, which obviously doesn't exist.

You can set up a link for your model like this:

python -m spacy link my_model my_model # if it's installed via pip, or:
python -m spacy link /path/to/my_model/data my_model
This will create a symlink in the /spacy/data directory, so you should run it with admin permissions.

Alternatively, if you've created a model package that can be installed via pip, you can simply install and import it, and then call its load() method with no arguments:

import my_model
nlp = my_model.load()
In some cases, this way of loading models is actually more convenient, as it's cleaner and lets you debug your code more easily. For example, if a model doesn't exist, Python will raise an ImportError immediately. Similarly, if loading fails, you know there's likely a problem with the model's own loading and meta.

Btw: I'm one of the spaCy maintainers and I agree that the way spacy.load() currently works is definitely unideal and confusing. We're looking forward to finally changing this with the next major release. We're very close to releasing the first alpha of v2.0, which will solve this problem more elegantly, and will also include a lot of improvements to the training process and documentation.",response from spacy maintainer,"The response was provided by one of the Spacy maintainers, indicating that the library is actively maintained.",,1,,,,C20230706015011758538,0.71,0.28,0.43,0.69,0.88,0.62,0.5,0.34,0.61,0.45,0.54,0.66,0.44,0.43,0.51,0.78,0.71,0.55,0.49,0.36,0.36,0.72,0.64,0.51, 53response from spacy maintainer ,0.19,0.81,1,1
186,54,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,72068918,"Python NLP Spacy : improve bi-gram extraction from a dataframe, and with named entities?","I am using Python and spaCy as my NLP library, working on a big dataframe that contains feedback about different cars, which looks like this:

enter image description here

'feedback' column contains natural language text to be processed,
'lemmatized' column contains lemmatized version of the feedback text,
'entities' column contains named entities extracted from the feedback text (I've trained the pipeline so that it will recognise car models and brands, labelling these as 'CAR_BRAND' and 'CAR_MODEL')
I then created the following function, which applies the Spacy nlp token to each row of my dataframe and extract any [noun + verb], [verb + noun], [adj + noun], [adj+ proper noun] combinations.
This is where I have a problem...

This is producing only one bigram per row maximum. How can I tweak the code so that more than one bigram can be extracted and put in a column? (Also are there more linguistic combinations I should try?)
Is there a possibility to find out what people are saying about 'CAR_BRAND' and 'CAR_MODEL' named entities extracted in the 'entities' column? For example 'Cool Porsche' - Some brands or models are made of more than two words so it's tricky to tackle.
I am very new to NLP.. If there is a more efficient way to tackle this, any advice will be super helpful! Many thanks for your help in advance.","spaCy has a built-in pattern matching engine that's perfect for your application ‚Äö√Ñ√∂‚àö√ë‚àö¬® it's documented here and in a more extensive usage guide. It allows you to define patterns in a readable and easy-to-maintain way, as lists of dictionaries that define the properties of the tokens to be matched.

Some ideas for improvement

Named entity recognition should be able to detect multi-word expressions, so brand and/or model names that consist of more than one token shouldn't be an issue if everything is set up correctly
Matching dependency patterns instead of linear patterns might slightly improve your results
That being said, what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® kind of sentiment analysis -is quite a difficult task that's normally engaged with machine learning approaches and heaps of training data. So don't expect too much from simple heuristics.
",acknowledgement of complexity,"The responder acknowledged the complex nature of sentiment analysis, demonstrating knowledge and understanding of the field.",,1,,,,C20230706015014556107,0.61,0.47,0.62,0.78,0.67,0.7,0.72,0.53,0.57,0.73,0.74,0.77,0.78,0.53,0.52,0.88,0.44,0.43,0.53,0.36,0.43,0.67,0.62,0.58, 54acknowledgement of complexity ,0.18,0.82,1,1
187,54,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,72068918,"Python NLP Spacy : improve bi-gram extraction from a dataframe, and with named entities?","I am using Python and spaCy as my NLP library, working on a big dataframe that contains feedback about different cars, which looks like this:

enter image description here

'feedback' column contains natural language text to be processed,
'lemmatized' column contains lemmatized version of the feedback text,
'entities' column contains named entities extracted from the feedback text (I've trained the pipeline so that it will recognise car models and brands, labelling these as 'CAR_BRAND' and 'CAR_MODEL')
I then created the following function, which applies the Spacy nlp token to each row of my dataframe and extract any [noun + verb], [verb + noun], [adj + noun], [adj+ proper noun] combinations.
This is where I have a problem...

This is producing only one bigram per row maximum. How can I tweak the code so that more than one bigram can be extracted and put in a column? (Also are there more linguistic combinations I should try?)
Is there a possibility to find out what people are saying about 'CAR_BRAND' and 'CAR_MODEL' named entities extracted in the 'entities' column? For example 'Cool Porsche' - Some brands or models are made of more than two words so it's tricky to tackle.
I am very new to NLP.. If there is a more efficient way to tackle this, any advice will be super helpful! Many thanks for your help in advance.","spaCy has a built-in pattern matching engine that's perfect for your application ‚Äö√Ñ√∂‚àö√ë‚àö¬® it's documented here and in a more extensive usage guide. It allows you to define patterns in a readable and easy-to-maintain way, as lists of dictionaries that define the properties of the tokens to be matched.

Some ideas for improvement

Named entity recognition should be able to detect multi-word expressions, so brand and/or model names that consist of more than one token shouldn't be an issue if everything is set up correctly
Matching dependency patterns instead of linear patterns might slightly improve your results
That being said, what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® kind of sentiment analysis -is quite a difficult task that's normally engaged with machine learning approaches and heaps of training data. So don't expect too much from simple heuristics.
",active development,"Overall, the conversation suggests that spaCy is well-maintained and has a knowledgeable and helpful community supporting it, indicating active development.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015015059649,0.62,0.49,0.5,0.73,0.79,0.74,0.73,0.72,0.64,0.91,0.78,0.82,0.67,0.57,0.53,0.64,0.63,0.57,0.65,0.56,0.45,0.68,0.76,0.78, 54active development ,0.73,0.27,0,0
188,54,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,72068918,"Python NLP Spacy : improve bi-gram extraction from a dataframe, and with named entities?","I am using Python and spaCy as my NLP library, working on a big dataframe that contains feedback about different cars, which looks like this:

enter image description here

'feedback' column contains natural language text to be processed,
'lemmatized' column contains lemmatized version of the feedback text,
'entities' column contains named entities extracted from the feedback text (I've trained the pipeline so that it will recognise car models and brands, labelling these as 'CAR_BRAND' and 'CAR_MODEL')
I then created the following function, which applies the Spacy nlp token to each row of my dataframe and extract any [noun + verb], [verb + noun], [adj + noun], [adj+ proper noun] combinations.
This is where I have a problem...

This is producing only one bigram per row maximum. How can I tweak the code so that more than one bigram can be extracted and put in a column? (Also are there more linguistic combinations I should try?)
Is there a possibility to find out what people are saying about 'CAR_BRAND' and 'CAR_MODEL' named entities extracted in the 'entities' column? For example 'Cool Porsche' - Some brands or models are made of more than two words so it's tricky to tackle.
I am very new to NLP.. If there is a more efficient way to tackle this, any advice will be super helpful! Many thanks for your help in advance.","spaCy has a built-in pattern matching engine that's perfect for your application ‚Äö√Ñ√∂‚àö√ë‚àö¬® it's documented here and in a more extensive usage guide. It allows you to define patterns in a readable and easy-to-maintain way, as lists of dictionaries that define the properties of the tokens to be matched.

Some ideas for improvement

Named entity recognition should be able to detect multi-word expressions, so brand and/or model names that consist of more than one token shouldn't be an issue if everything is set up correctly
Matching dependency patterns instead of linear patterns might slightly improve your results
That being said, what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® kind of sentiment analysis -is quite a difficult task that's normally engaged with machine learning approaches and heaps of training data. So don't expect too much from simple heuristics.
",resources offered,"The responder provided links to spaCy documentation and usage guides, indicating that the library is well-maintained and provides helpful resources for its users.",unknown,0,Generic Error,Mixed,,C20230706015014049568,0.75,0.65,0.47,0.71,0.62,0.55,0.79,0.9,0.72,0.7,0.73,0.77,0.56,0.73,0.52,0.57,0.66,0.55,0.5,0.71,0.59,0.8,0.76,0.67, 54resources offered ,0.65,0.35,0,1
189,54,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,72068918,"Python NLP Spacy : improve bi-gram extraction from a dataframe, and with named entities?","I am using Python and spaCy as my NLP library, working on a big dataframe that contains feedback about different cars, which looks like this:

enter image description here

'feedback' column contains natural language text to be processed,
'lemmatized' column contains lemmatized version of the feedback text,
'entities' column contains named entities extracted from the feedback text (I've trained the pipeline so that it will recognise car models and brands, labelling these as 'CAR_BRAND' and 'CAR_MODEL')
I then created the following function, which applies the Spacy nlp token to each row of my dataframe and extract any [noun + verb], [verb + noun], [adj + noun], [adj+ proper noun] combinations.
This is where I have a problem...

This is producing only one bigram per row maximum. How can I tweak the code so that more than one bigram can be extracted and put in a column? (Also are there more linguistic combinations I should try?)
Is there a possibility to find out what people are saying about 'CAR_BRAND' and 'CAR_MODEL' named entities extracted in the 'entities' column? For example 'Cool Porsche' - Some brands or models are made of more than two words so it's tricky to tackle.
I am very new to NLP.. If there is a more efficient way to tackle this, any advice will be super helpful! Many thanks for your help in advance.","spaCy has a built-in pattern matching engine that's perfect for your application ‚Äö√Ñ√∂‚àö√ë‚àö¬® it's documented here and in a more extensive usage guide. It allows you to define patterns in a readable and easy-to-maintain way, as lists of dictionaries that define the properties of the tokens to be matched.

Some ideas for improvement

Named entity recognition should be able to detect multi-word expressions, so brand and/or model names that consist of more than one token shouldn't be an issue if everything is set up correctly
Matching dependency patterns instead of linear patterns might slightly improve your results
That being said, what you're trying to do ‚Äö√Ñ√∂‚àö√ë‚àö¬® kind of sentiment analysis -is quite a difficult task that's normally engaged with machine learning approaches and heaps of training data. So don't expect too much from simple heuristics.
",solution provided,The responder provided a solution to the user's problem and addressed their concerns.,,1,,,,C20230706015013493903,0.64,0.32,0.36,0.32,0.35,0.92,0.54,0.29,0.25,0.39,0.35,0.94,0.67,0.47,0.48,0.19,0.43,0.69,0.47,0.22,0.23,0.55,0.59,0.94, 54solution provided ,0.01,0.99,1,1
190,55,Spacy,Active Maintenance,github,Active Maintenance aspect,58327580,How to add new lemma rule to existing language for spacy,"I want to add a new lemmatiser rule for an existing language, i.e. lemmatise all nouns ending with ""z"" to ending with """".

In the case of individual words, spaCy gives the opportunity to add a tokeniser exception to an existing language after loading using

nlp.tokenizer.add_special_case(""adidas"", [{ORTH: 'adidas', LEMMA: 'Adidas', POS: 'NOUN', TAG: 'NNP'}])
The above sets the lemma, pos and tag of the new word and this is not altered. The default English lemmatiser returned ""adida"" as the lemma.

Now, I am trying to ""lemmatise"" nouns ""wordz"" to ""word"", ""windowz"" to ""window"" etc without setting all cases as exceptions but rather add a new rule: Noun ending with ""z"" has lemma the noun without the trailing ""z"".

I understand that it will depend on the tagger output as the rules that exist in _lemma_rules.py are pos dependent.

Is there a way to add the rule without creating a new language as a copy of an existing with just one modified file?","Since my question was very specific, I had to communicate with the spaCy developer team and got a working answer.

Actually it is does not work for the fake example in English but it works in real case scenario while using the Greek models as Greek lemmatisation is mainly rule based.

The proposed solution is to use the Lookups Api, which is only available in versions 2.2 and later.

As they mention,

nlp.vocab.lookups.get_table(""lemma_rules"") 
returns a dict-like table that you can write to.

Full answer in spaCy GitHub",developer team available,The fact that a specific question about adding a new lemma rule to an existing language was answered by the Spacy developer team suggests that the library is still actively maintained and support is available for users.,wrong labeling,0,Generic Error,Mixed,,C20230706015015560924,0.71,0.42,0.67,0.46,0.51,0.29,0.62,0.9,0.85,0.64,0.73,0.79,0.54,0.44,0.48,0.29,0.56,0.38,0.72,0.72,0.6,0.66,0.55,0.5, 55developer team available ,0.43,0.57,0,1
191,55,Spacy,Active Maintenance,github,Active Maintenance aspect,58327580,How to add new lemma rule to existing language for spacy,"I want to add a new lemmatiser rule for an existing language, i.e. lemmatise all nouns ending with ""z"" to ending with """".

In the case of individual words, spaCy gives the opportunity to add a tokeniser exception to an existing language after loading using

nlp.tokenizer.add_special_case(""adidas"", [{ORTH: 'adidas', LEMMA: 'Adidas', POS: 'NOUN', TAG: 'NNP'}])
The above sets the lemma, pos and tag of the new word and this is not altered. The default English lemmatiser returned ""adida"" as the lemma.

Now, I am trying to ""lemmatise"" nouns ""wordz"" to ""word"", ""windowz"" to ""window"" etc without setting all cases as exceptions but rather add a new rule: Noun ending with ""z"" has lemma the noun without the trailing ""z"".

I understand that it will depend on the tagger output as the rules that exist in _lemma_rules.py are pos dependent.

Is there a way to add the rule without creating a new language as a copy of an existing with just one modified file?","Since my question was very specific, I had to communicate with the spaCy developer team and got a working answer.

Actually it is does not work for the fake example in English but it works in real case scenario while using the Greek models as Greek lemmatisation is mainly rule based.

The proposed solution is to use the Lookups Api, which is only available in versions 2.2 and later.

As they mention,

nlp.vocab.lookups.get_table(""lemma_rules"") 
returns a dict-like table that you can write to.

Full answer in spaCy GitHub",newer version with lookups api,"The suggestion of using the Lookups API, which is only available in newer versions of Spacy, suggests that the library is continuously updated and improved with new features.",,1,,,,C20230706015016118078,0.68,0.78,0.63,0.77,0.87,0.64,0.94,0.81,0.86,0.81,0.85,0.79,0.54,0.72,0.62,0.64,0.69,0.66,0.59,0.76,0.84,0.77,0.81,0.85, 55newer version with lookups api ,0.14,0.86,1,1
192,56,Spacy,Active Maintenance,github,Active Maintenance aspect,51855137,Rasa NLU: Tensorflow embedding with entity extraction,"Tensorflow embedding has a very good intent recogition.

In my experience it improved test results from about 45% confidence to 95% for simple inputs such as ""Hello"" compared to spacy.

But out of the box it does not have any entity extraction.

Is there any Pipeline configuration to solve this?","Config

The configuration below contains ner_crf for entity extraction.
Version

Rasa NLU: >=0.13.0

credit: github issue",config,The answer suggests a configuration solution for the entity extraction issue in Rasa NLU.,,1,,,,C20230706015016589020,0.64,0.54,0.48,0.26,0.93,0.15,0.7,0.21,0.47,0.05,0.63,0.24,0.66,0.31,0.61,0.34,0.91,0.38,0.76,0.59,0.57,0.72,0.8,0.71, 56config ,0.01,0.99,1,1
193,56,Spacy,Active Maintenance,github,Active Maintenance aspect,51855137,Rasa NLU: Tensorflow embedding with entity extraction,"Tensorflow embedding has a very good intent recogition.

In my experience it improved test results from about 45% confidence to 95% for simple inputs such as ""Hello"" compared to spacy.

But out of the box it does not have any entity extraction.

Is there any Pipeline configuration to solve this?","Config

The configuration below contains ner_crf for entity extraction.
Version

Rasa NLU: >=0.13.0

credit: github issue",version,The answer mentions that the solution is applicable for Rasa NLU versions equal or greater than 0.13.0.,,1,,,,C20230706015017294347,0.76,0.59,0.69,0.69,0.68,0.46,0.71,0.65,0.53,0.71,0.72,0.76,0.59,0.41,0.64,0.49,0.5,0.76,0.67,0.63,0.53,0.71,0.71,0.69, 56version ,0.04,0.96,1,1
194,57,Spacy,Active Maintenance,github,Active Maintenance aspect,46765198,Unable to load spacy English model - 'WindowsPath' object has no attribute 'read',"I installed spacy using pip and then downloaded the English model using  
$ python -m spacy download en which after downloading gave me the message

You can now load the model via spacy.load('en')
I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!",If anybody else receives this error : I opened this as an issue with spaCy's developers on Github. I was suggested using Python 3.6 instead of 2.7 for the moment as there is no alternate workaround to the problem. The next spaCy version should cover this bugfix (I'm told).,active community,The community is active and responsive as the issue was raised with the developers on Github and a solution was suggested.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706015017768465,0.64,0.72,0.5,0.38,0.58,0.5,0.61,0.68,0.83,0.38,0.58,0.66,0.57,0.77,0.74,0.4,0.53,0.47,0.58,0.55,0.45,0.23,0.41,0.42, 57active community ,0.24,0.76,1,1
195,57,Spacy,Active Maintenance,github,Active Maintenance aspect,46765198,Unable to load spacy English model - 'WindowsPath' object has no attribute 'read',"I installed spacy using pip and then downloaded the English model using  
$ python -m spacy download en which after downloading gave me the message

You can now load the model via spacy.load('en')
I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!",If anybody else receives this error : I opened this as an issue with spaCy's developers on Github. I was suggested using Python 3.6 instead of 2.7 for the moment as there is no alternate workaround to the problem. The next spaCy version should cover this bugfix (I'm told).,bug fixing,"The developers are currently working on a fix for the bug, indicating that the library is being actively maintained and updated.",,1,,,,C20230706015018716211,0.7,0.82,0.66,0.55,0.65,0.91,0.85,0.8,0.7,0.72,0.73,0.79,0.62,0.7,0.75,0.42,0.48,0.82,0.74,0.88,0.76,0.83,0.68,0.77, 57bug fixing ,0.02,0.98,1,1
196,57,Spacy,Active Maintenance,github,Active Maintenance aspect,46765198,Unable to load spacy English model - 'WindowsPath' object has no attribute 'read',"I installed spacy using pip and then downloaded the English model using  
$ python -m spacy download en which after downloading gave me the message

You can now load the model via spacy.load('en')
I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!",If anybody else receives this error : I opened this as an issue with spaCy's developers on Github. I was suggested using Python 3.6 instead of 2.7 for the moment as there is no alternate workaround to the problem. The next spaCy version should cover this bugfix (I'm told).,developer responsiveness,The fact that the developers responded on Github and suggested using a different version of Python for the moment highlights their responsiveness.,wrong labeling,0,Generic Error,Mixed,,C20230706015018261101,0.72,0.66,0.52,0.43,0.63,0.48,0.44,0.57,0.58,0.71,0.6,0.65,0.76,0.54,0.46,0.58,0.44,0.39,0.35,0.33,0.52,0.64,0.67,0.49, 57developer responsiveness ,0.39,0.61,0,1
197,57,Spacy,Active Maintenance,github,Active Maintenance aspect,46765198,Unable to load spacy English model - 'WindowsPath' object has no attribute 'read',"I installed spacy using pip and then downloaded the English model using  
$ python -m spacy download en which after downloading gave me the message

You can now load the model via spacy.load('en')
I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!",If anybody else receives this error : I opened this as an issue with spaCy's developers on Github. I was suggested using Python 3.6 instead of 2.7 for the moment as there is no alternate workaround to the problem. The next spaCy version should cover this bugfix (I'm told).,open source support,The ability for users to report issues and receive support from the community is an important aspect of maintaining a successful open source project.,Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706015019203032,0.6,0.65,0.55,0.36,0.53,0.37,0.75,0.7,0.62,0.75,0.66,0.65,0.52,0.43,0.59,0.35,0.32,0.21,0.65,0.55,0.43,0.83,0.62,0.6, 57open source support ,0.17,0.83,1,0
198,58,Spacy,Active Maintenance,github,Active Maintenance aspect,53728770,Cannot install es_core_news_sm from Spacy,"When I try to install es_core_news_sm with this commmand

python -m spacy download es_core_news_sm
with conda I get this error

No module name spacy._main_;'spacy'is a package and cannot be directly executed. 
Thank you so much!","This could happen if conda installed an older version of spaCy. See the following issue on spaCy's github.

The fix as stated in the issue is as follows:

Find the most recent version of spaCy on conda:conda search -f spacy
Then run: conda install spacy=NEWEST_VERSION
The newest version of spaCy on Conda is 2.0.18",compatibility issue,"The issue faced by the user is related to compatibility rather than maintenance, as the user is trying to install a model using an older version of spaCy which causes an error.",,1,,,,C20230706015019714783,0.6,0.7,0.64,0.51,0.45,0.24,0.55,0.73,0.69,0.57,0.51,0.62,0.47,0.69,0.64,0.4,0.5,0.44,0.58,0.4,0.65,0.27,0.56,0.26, 58compatibility issue ,0.08,0.92,1,1
199,58,Spacy,Active Maintenance,github,Active Maintenance aspect,53728770,Cannot install es_core_news_sm from Spacy,"When I try to install es_core_news_sm with this commmand

python -m spacy download es_core_news_sm
with conda I get this error

No module name spacy._main_;'spacy'is a package and cannot be directly executed. 
Thank you so much!","This could happen if conda installed an older version of spaCy. See the following issue on spaCy's github.

The fix as stated in the issue is as follows:

Find the most recent version of spaCy on conda:conda search -f spacy
Then run: conda install spacy=NEWEST_VERSION
The newest version of spaCy on Conda is 2.0.18",fix provided,The answer provides a solution to the compatibility issue by suggesting a fix to install the most recent version of spaCy on conda.,,1,,,,C20230706015020249953,0.62,0.8,0.63,0.66,0.62,0.42,0.87,0.71,0.78,0.64,0.72,0.76,0.69,0.68,0.16,0.48,0.47,0.2,0.91,0.77,0.73,0.79,0.7,0.68, 58fix provided ,0.07,0.93,1,1
200,59,Spacy,Active Maintenance,github,Active Maintenance aspect,61943409,Spacy's BERT model doesn't learn,"I've been trying to use spaCy's pretrained BERT model de_trf_bertbasecased_lg to increase accuracy in my classification project. I used to build a model from scratch using de_core_news_sm and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in this article as an example.

Function get_data() opens files with different categories, creates a tuple like this one (text, {'cats' : {'category1': 0, 'category2':1, ...}}), gathers all these tuples into one array, which is then being returned to the main function.

Function test(nlp) opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.

Again, everything worked just fine with de_core_news_sm, so I'm pretty sure that functions get_data() and test(nlp) are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.","Received an answer to my question on GitHub and it looks like there must be some optimizer parameters specified, just like in this example.",community support,The fact that the user received an answer to their question on GitHub suggests that there is a community of developers and users actively engaged with the library and providing support to each other.,,1,,,,C20230706015020776228,0.68,0.77,0.88,0.53,0.43,0.75,0.68,0.64,0.56,0.52,0.44,0.82,0.52,0.62,0.76,0.41,0.33,0.86,0.78,0.71,0.59,0.59,0.43,0.62, 59community support ,0.03,0.97,1,1
201,59,Spacy,Active Maintenance,github,Active Maintenance aspect,61943409,Spacy's BERT model doesn't learn,"I've been trying to use spaCy's pretrained BERT model de_trf_bertbasecased_lg to increase accuracy in my classification project. I used to build a model from scratch using de_core_news_sm and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in this article as an example.

Function get_data() opens files with different categories, creates a tuple like this one (text, {'cats' : {'category1': 0, 'category2':1, ...}}), gathers all these tuples into one array, which is then being returned to the main function.

Function test(nlp) opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.

Again, everything worked just fine with de_core_news_sm, so I'm pretty sure that functions get_data() and test(nlp) are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.","Received an answer to my question on GitHub and it looks like there must be some optimizer parameters specified, just like in this example.",multiple models available,"The existence of multiple models available for use in Spacy, including pre-trained ones like BERT, indicates that the development of the library is ongoing and continually improving.",Confidently wrong inference,0,ChatGPT Inferred Wrong Repeatedly,Mixed,,C20230706015021297922,0.66,0.49,0.47,0.75,0.55,0.59,0.94,0.88,0.81,0.84,0.83,0.79,0.52,0.51,0.76,0.54,0.45,0.52,0.73,0.21,0.36,0.34,0.48,0.54, 59multiple models available ,0.2,0.8,1,0
202,60,Spacy,Security,leak,Security aspect,55841087,Python: Spacy and memory consumption,"1 - THE PROBLEM

I'm using ""spacy"" on python for text documents lemmatization. There are 500,000 documents having size up to 20 Mb of clean text.

The problem is the following: spacy memory consuming is growing in time till the whole memory is used.

2 - BACKGROUND

My hardware configuration: CPU: Intel I7-8700K 3.7 GHz (12 cores) Memory: 16 Gb SSD: 1 Tb GPU is onboard but is not used for this task

I'm using ""multiprocessing"" to split the task among several processes (workers). Each worker receives a list of documents to process. The main process performs monitoring of child processes. I initiate ""spacy"" in each child process once and use this one spacy instance to handle the whole list of documents in the worker.

3 - EXPECTATIONS

I have seen a good recommendation to build a separated server-client solution [here]Is possible to keep spacy in memory to reduce the load time?

Is it possible to keep memory consumption under control using ""multiprocessing"" approach?
","Memory problems when processing large amounts of data seem to be a known issue, see some relevant github issues:

https://github.com/explosion/spaCy/issues/3623
https://github.com/explosion/spaCy/issues/3556
Unfortunately, it doesn't look like there's a good solution yet.",known memory issues,"Memory problems when processing large amounts of data are a known issue, as evidenced by relevant github issues.",,1,,,,C20230706015021820496,0.78,0.83,0.75,0.75,0.88,0.8,0.41,0.42,0.42,0.86,0.76,0.82,0.56,0.55,0.69,0.64,0.62,0.5,0.62,0.5,0.64,0.6,0.53,0.62, 60known memory issues ,0.02,0.98,1,1
203,60,Spacy,Security,leak,Security aspect,55841087,Python: Spacy and memory consumption,"1 - THE PROBLEM

I'm using ""spacy"" on python for text documents lemmatization. There are 500,000 documents having size up to 20 Mb of clean text.

The problem is the following: spacy memory consuming is growing in time till the whole memory is used.

2 - BACKGROUND

My hardware configuration: CPU: Intel I7-8700K 3.7 GHz (12 cores) Memory: 16 Gb SSD: 1 Tb GPU is onboard but is not used for this task

I'm using ""multiprocessing"" to split the task among several processes (workers). Each worker receives a list of documents to process. The main process performs monitoring of child processes. I initiate ""spacy"" in each child process once and use this one spacy instance to handle the whole list of documents in the worker.

3 - EXPECTATIONS

I have seen a good recommendation to build a separated server-client solution [here]Is possible to keep spacy in memory to reduce the load time?

Is it possible to keep memory consumption under control using ""multiprocessing"" approach?
","Memory problems when processing large amounts of data seem to be a known issue, see some relevant github issues:

https://github.com/explosion/spaCy/issues/3623
https://github.com/explosion/spaCy/issues/3556
Unfortunately, it doesn't look like there's a good solution yet.",no current solution,There may not be a good solution yet for managing memory consumption when using Spacy to process large amounts of data.,,1,,,,C20230706015022348112,0.86,0.78,0.74,0.75,0.77,0.86,0.78,0.6,0.77,0.46,0.53,0.82,0.6,0.63,0.76,0.7,0.82,0.7,0.76,0.68,0.71,0.55,0.56,0.64, 60no current solution ,0.05,0.95,1,1
204,61,Spacy,Security,leak,Security aspect,58956171,"Dataflow job failed after more than 6 hours with ""The worker lost contact with the service""?","I am using Dataflow to read data from BigQuery and then do NLP preprocessing using python. I am using Python 3 and SDK 2.16.0. I am using 100 workers (provite IP, private access and Cloud NAT) with workers in europe-west6 and endpoint in europe-west1. The BigQuery tables are in US. Test jobs were working without any issue but when trying to process the full table (32 GB), the job failed after 6h 40 min and it is hard to fully understand what is the underlying error.","After some test and after few plots for monitoring, it was clear that even if the length of the text was the sime the processing time started to augment rapidly (botton right plot)
Then it became clear that the issue was with SpaCy 2.1.8 (memory leak).

Using Spacy 2.2.3 fix the issue. Now the 32 Gb of data are processed in 4h30 without any issue.",bigquery security measures,"The BigQuery data tables being located in the US could mean that there are security measures in place to ensure confidentiality, integrity, and availability of the data.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015024731108,0.79,0.66,0.66,0.93,0.91,0.97,0.57,0.51,0.66,0.85,0.73,0.7,0.31,0.53,0.49,0.43,0.66,0.59,0.39,0.45,0.42,0.11,0.01,0.75, 61bigquery security measures ,0.03,0.97,1,0
205,61,Spacy,Security,leak,Security aspect,58956171,"Dataflow job failed after more than 6 hours with ""The worker lost contact with the service""?","I am using Dataflow to read data from BigQuery and then do NLP preprocessing using python. I am using Python 3 and SDK 2.16.0. I am using 100 workers (provite IP, private access and Cloud NAT) with workers in europe-west6 and endpoint in europe-west1. The BigQuery tables are in US. Test jobs were working without any issue but when trying to process the full table (32 GB), the job failed after 6h 40 min and it is hard to fully understand what is the underlying error.","After some test and after few plots for monitoring, it was clear that even if the length of the text was the sime the processing time started to augment rapidly (botton right plot)
Then it became clear that the issue was with SpaCy 2.1.8 (memory leak).

Using Spacy 2.2.3 fix the issue. Now the 32 Gb of data are processed in 4h30 without any issue.",cloud nat,Using Cloud NAT further enhances the security by providing a single gateway for internet access from private instances.,,1,,,,C20230706015024313870,0.85,0.74,0.78,0.96,0.99,0.97,0.91,0.81,0.86,0.85,0.91,0.83,0.33,0.28,0.33,0.96,0.97,0.96,0.31,0.29,0.3,0.86,0.9,0.9, 61cloud nat ,0.01,0.99,1,1
206,61,Spacy,Security,leak,Security aspect,58956171,"Dataflow job failed after more than 6 hours with ""The worker lost contact with the service""?","I am using Dataflow to read data from BigQuery and then do NLP preprocessing using python. I am using Python 3 and SDK 2.16.0. I am using 100 workers (provite IP, private access and Cloud NAT) with workers in europe-west6 and endpoint in europe-west1. The BigQuery tables are in US. Test jobs were working without any issue but when trying to process the full table (32 GB), the job failed after 6h 40 min and it is hard to fully understand what is the underlying error.","After some test and after few plots for monitoring, it was clear that even if the length of the text was the sime the processing time started to augment rapidly (botton right plot)
Then it became clear that the issue was with SpaCy 2.1.8 (memory leak).

Using Spacy 2.2.3 fix the issue. Now the 32 Gb of data are processed in 4h30 without any issue.",private ip and access,Using private IP and access provides additional security measures to protect the data from unauthorized access.,Unknown,1,Generic Error,Mixed,,C20230706015022871961,0.72,0.74,0.66,0.97,0.98,0.96,0.82,0.83,0.79,0.94,0.9,0.94,0.65,0.53,0.44,0.68,0.72,0.97,0.68,0.39,0.16,0.53,0.16,0.46, 61private ip and access ,0.15,0.85,1,1
207,62,GSON,Security,Security,Security aspect,13267258,Gson SecurityException: Can not make Class Constructor accessible,"I am using Gson to serialize/deserialize a class that contains a static nested class. The class looks like below.

public class ClassA {
private NestedClass nestedClass;

public NestedClass getNestedClass() {
return nestedClass;
}

public void setNestedClass(NestedClass nestedClass) {
this.nestedClass = nestedClass;
}

public static class NestedClass implements Serializable {

public NestedClass() {
}
}
}
The serialization works fine, but when i am trying to deserialize the json string back into an object i get the following SecurityException.

The exception seems to be coming from Java while trying to reflect the inner class. Has anyone been in that situation before?

Any help appreciated.",It appears you're running Gson on a JVM with a SecurityManager enabled. You'll need to disable the security manager to use Gson.,disabling security manager,The answer provided suggests that disabling the security manager may be necessary to use GSON.,,1,,,,C20230706015027101697,0.76,0.64,0.77,0.84,0.79,0.74,0.54,0.67,0.51,0.48,0.82,0.51,0.63,0.62,0.71,0.62,0.68,0.62,0.65,0.66,0.68,0.55,0.68,0.72, 62disabling security manager ,0,1,1,1
208,62,GSON,Security,Security,Security aspect,13267258,Gson SecurityException: Can not make Class Constructor accessible,"I am using Gson to serialize/deserialize a class that contains a static nested class. The class looks like below.

public class ClassA {
private NestedClass nestedClass;

public NestedClass getNestedClass() {
return nestedClass;
}

public void setNestedClass(NestedClass nestedClass) {
this.nestedClass = nestedClass;
}

public static class NestedClass implements Serializable {

public NestedClass() {
}
}
}
The serialization works fine, but when i am trying to deserialize the json string back into an object i get the following SecurityException.

The exception seems to be coming from Java while trying to reflect the inner class. Has anyone been in that situation before?

Any help appreciated.",It appears you're running Gson on a JVM with a SecurityManager enabled. You'll need to disable the security manager to use Gson.,gson reliance on reflection,"The conversation suggests that GSON may rely on reflective mechanisms to deserialize JSON strings, which could present security risks in certain contexts.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015026208106,0.75,0.74,0.75,0.95,0.6,0.6,0.73,0.78,0.41,0.71,0.44,0.47,0.6,0.68,0.48,0.78,0.49,0.45,0.62,0.85,0.55,0.6,0.59,0.68, 62gson reliance on reflection ,0.01,0.99,1,0
209,62,GSON,Security,Security,Security aspect,13267258,Gson SecurityException: Can not make Class Constructor accessible,"I am using Gson to serialize/deserialize a class that contains a static nested class. The class looks like below.

public class ClassA {
private NestedClass nestedClass;

public NestedClass getNestedClass() {
return nestedClass;
}

public void setNestedClass(NestedClass nestedClass) {
this.nestedClass = nestedClass;
}

public static class NestedClass implements Serializable {

public NestedClass() {
}
}
}
The serialization works fine, but when i am trying to deserialize the json string back into an object i get the following SecurityException.

The exception seems to be coming from Java while trying to reflect the inner class. Has anyone been in that situation before?

Any help appreciated.",It appears you're running Gson on a JVM with a SecurityManager enabled. You'll need to disable the security manager to use Gson.,securityexception reflection,The SecurityException being experienced by the user seems to be related to Java's reflection feature attempting to access the inner class.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015025687365,0.67,0.77,0.59,0.73,0.85,0.69,0.83,0.8,0.71,0.89,0.75,0.7,0.71,0.71,0.5,0.64,0.5,0.66,0.66,0.81,0.57,0.77,0.83,0.79, 62securityexception reflection ,0.13,0.87,1,0
210,62,GSON,Security,Security,Security aspect,13267258,Gson SecurityException: Can not make Class Constructor accessible,"I am using Gson to serialize/deserialize a class that contains a static nested class. The class looks like below.

public class ClassA {
private NestedClass nestedClass;

public NestedClass getNestedClass() {
return nestedClass;
}

public void setNestedClass(NestedClass nestedClass) {
this.nestedClass = nestedClass;
}

public static class NestedClass implements Serializable {

public NestedClass() {
}
}
}
The serialization works fine, but when i am trying to deserialize the json string back into an object i get the following SecurityException.

The exception seems to be coming from Java while trying to reflect the inner class. Has anyone been in that situation before?

Any help appreciated.",It appears you're running Gson on a JVM with a SecurityManager enabled. You'll need to disable the security manager to use Gson.,securitymanager enabled,The conversation indicates that running Gson on a JVM with a SecurityManager enabled limits the usage of Gson.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015025235112,0.73,0.73,0.62,0.87,0.43,0.39,0.8,0.74,0.62,0.72,0.51,0.61,0.61,0.71,0.52,0.75,0.38,0.37,0.69,0.79,0.35,0.7,0.29,0.35, 62securitymanager enabled ,0.52,0.48,0,0
211,63,GSON,Security,Security,Security aspect,45793451,Kotlin not nullable value can be null?,"I have backend that return me some json.

I parse it to my class:

class SomeData(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String
)
Use gson converter factory:

Retrofit retrofit = new Retrofit.Builder()
  .baseUrl(ENDPOINT)
  .client(okHttpClient)
  .addConverterFactory(GsonConverterFactory.create(gson))
  .addCallAdapterFactory(RxJava2CallAdapterFactory.create())
  .build();
And in my interface:

interface MyAPI {
    @GET(""get_data"")
    Observable<List<SomeData>> getSomeData();
}
Then I retrieve data from the server (with rxJava) without any error. But I expected an error because I thought I should do something like this (to prevent GSON converter error, because notNullableValue is not present in my JSON response):

class SomeData @JvmOverloads constructor(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String = """"
)
After the data is received from backend and parsed to my SomeData class with constructor without def value, the value of the notNullableValue == null.

As I understand not nullable value can be null in Kotlin?","Yes, that is because you're giving it a default value. Ofcourse it will never be null. That's the whole point of a default value.

Remove ="""" from constructor and you will get an error.

Edit: Found the issue. GSON uses the magic sun.misc.Unsafe class which has an allocateInstance method which is obviously considered very unsafe because what it does is skip initialization (constructors/field initializers and the like) and security checks. So there is your answer why a Kotlin non-nullable field can be null. Offending code is in com/google/gson/internal/ConstructorConstructor.java:223

Some interesting details about the Unsafe class: http://mishadoff.com/blog/java-magic-part-4-sun-dot-misc-dot-unsafe/",default value,Default value in Kotlin makes non-nullable values null when parsed by GSON.,,1,,,,C20230706015027646369,0.76,0.75,0.63,0.83,0.87,0.89,0.7,0.72,0.62,0.73,0.64,0.79,0.66,0.78,0.64,0.68,0.71,0.79,0.73,0.65,0.33,0.63,0.23,0.55, 63default value ,0.09,0.91,1,1
212,63,GSON,Security,Security,Security aspect,45793451,Kotlin not nullable value can be null?,"I have backend that return me some json.

I parse it to my class:

class SomeData(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String
)
Use gson converter factory:

Retrofit retrofit = new Retrofit.Builder()
  .baseUrl(ENDPOINT)
  .client(okHttpClient)
  .addConverterFactory(GsonConverterFactory.create(gson))
  .addCallAdapterFactory(RxJava2CallAdapterFactory.create())
  .build();
And in my interface:

interface MyAPI {
    @GET(""get_data"")
    Observable<List<SomeData>> getSomeData();
}
Then I retrieve data from the server (with rxJava) without any error. But I expected an error because I thought I should do something like this (to prevent GSON converter error, because notNullableValue is not present in my JSON response):

class SomeData @JvmOverloads constructor(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String = """"
)
After the data is received from backend and parsed to my SomeData class with constructor without def value, the value of the notNullableValue == null.

As I understand not nullable value can be null in Kotlin?","Yes, that is because you're giving it a default value. Ofcourse it will never be null. That's the whole point of a default value.

Remove ="""" from constructor and you will get an error.

Edit: Found the issue. GSON uses the magic sun.misc.Unsafe class which has an allocateInstance method which is obviously considered very unsafe because what it does is skip initialization (constructors/field initializers and the like) and security checks. So there is your answer why a Kotlin non-nullable field can be null. Offending code is in com/google/gson/internal/ConstructorConstructor.java:223

Some interesting details about the Unsafe class: http://mishadoff.com/blog/java-magic-part-4-sun-dot-misc-dot-unsafe/",unsafe class,GSON uses sun.misc.Unsafe class which skips initialization and security checks. This can potentially lead to security issues.,,1,,,,C20230706015028509518,0.62,0.72,0.68,0.81,0.89,0.89,0.56,0.74,0.71,0.75,0.73,0.96,0.58,0.6,0.78,0.81,0.94,0.8,0.75,0.71,0.68,0.79,0.76,0.77, 63unsafe class ,0,1,1,1
213,64,GSON,Security,Security,Security aspect,8403277,GSON vulnerabilities or exploits on deserialization to avoid,"I am planning to use gson's fromJson() method to parse a string coming from the browser. Are there any potential vulnerabilities associated with doing that? The data type I am converting to is relatively simple, a List and a boolean. But since gson uses reflection, is there something for me to watch out for?

For example, with older jvm (pre 6.24), a DOS attack could have been used against integers, where the integer parser would hang.

Can some clever json cause gson to start loading classes it should be?","The thing to watch with Gson is what type of builder you are using (see custom deserializer / serializer)

Gson has another weakness, when you are deserializing, (let's say using a custom one) you better check for the type of object you are passing (use instanceof).

Other main point: Gson will automatically convert the variable based on the type being passed.

ie. { ""var1"":1 , ""var2"":""1""} The first one will be converted into an integer, the second based to a string, therefore I would watch out on your objects transformation.",automatic variable conversion,"Gson will automatically convert variables based on their type, which could lead to unintended transformations and potential vulnerabilities.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015030259230,0.78,0.76,0.71,0.89,0.9,0.93,0.78,0.69,0.87,0.82,0.78,0.75,0.63,0.62,0.63,0.91,0.91,0.93,0.74,0.69,0.72,0.85,0.91,0.79, 64automatic variable conversion ,0.01,0.99,1,1
214,64,GSON,Security,Security,Security aspect,8403277,GSON vulnerabilities or exploits on deserialization to avoid,"I am planning to use gson's fromJson() method to parse a string coming from the browser. Are there any potential vulnerabilities associated with doing that? The data type I am converting to is relatively simple, a List and a boolean. But since gson uses reflection, is there something for me to watch out for?

For example, with older jvm (pre 6.24), a DOS attack could have been used against integers, where the integer parser would hang.

Can some clever json cause gson to start loading classes it should be?","The thing to watch with Gson is what type of builder you are using (see custom deserializer / serializer)

Gson has another weakness, when you are deserializing, (let's say using a custom one) you better check for the type of object you are passing (use instanceof).

Other main point: Gson will automatically convert the variable based on the type being passed.

ie. { ""var1"":1 , ""var2"":""1""} The first one will be converted into an integer, the second based to a string, therefore I would watch out on your objects transformation.",custom builder weakness,Custom deserializer/serializer builders can pose security weaknesses.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015028930329,0.72,0.58,0.61,0.64,0.86,0.71,0.69,0.62,0.59,0.82,0.76,0.73,0.48,0.59,0.72,0.58,0.61,0.87,0.49,0.53,0.57,0.79,0.66,0.77, 64custom builder weakness ,0.01,0.99,1,1
215,64,GSON,Security,Security,Security aspect,8403277,GSON vulnerabilities or exploits on deserialization to avoid,"I am planning to use gson's fromJson() method to parse a string coming from the browser. Are there any potential vulnerabilities associated with doing that? The data type I am converting to is relatively simple, a List and a boolean. But since gson uses reflection, is there something for me to watch out for?

For example, with older jvm (pre 6.24), a DOS attack could have been used against integers, where the integer parser would hang.

Can some clever json cause gson to start loading classes it should be?","The thing to watch with Gson is what type of builder you are using (see custom deserializer / serializer)

Gson has another weakness, when you are deserializing, (let's say using a custom one) you better check for the type of object you are passing (use instanceof).

Other main point: Gson will automatically convert the variable based on the type being passed.

ie. { ""var1"":1 , ""var2"":""1""} The first one will be converted into an integer, the second based to a string, therefore I would watch out on your objects transformation.",object type checking,"It's important to check for the type of object being passed, using 'instanceof'.",,1,,,,C20230706015029455158,0.77,0.58,0.71,0.73,0.89,0.7,0.69,0.64,0.55,0.81,0.8,0.9,0.63,0.41,0.64,0.45,0.68,0.53,0.58,0.71,0.68,0.77,0.85,0.82, 64object type checking ,0.08,0.92,1,1
216,65,GSON,Security,Security,Security aspect,4981101,Gson Security concerns,I plan to use Gson (http://code.google.com/p/google-gson/) on my server and transfer JavaScript objects from the browser to Java objects on the server. I will use Ajax to POST the request. I am a little concerned that everybody will be able to post Java objects to my server VM. A Java object that runs in the server process will have the potential to do whatever it wants. How does Gson address that?,"gson/json is data only, not code. So noone can post code to your server, just data.

When you're deserializing json, you specify which class (or you just extract primitives or collections of primitives directly) to populate with data - noone can send you json that aribtarly instantiate any of your classes living on the server.",data-oriented approach,"Gson strictly deals with data, not code. This eliminates the possibility of posting Java objects that could potentially run arbitrary code on the server.",,1,,,,C20230706015030804084,0.63,0.76,0.52,0.69,0.82,0.62,0.68,0.76,0.7,0.73,0.7,0.89,0.68,0.8,0.58,0.85,0.66,0.69,0.81,0.82,0.82,0.82,0.88,0.86, 65data-oriented approach ,0.04,0.96,1,1
217,65,GSON,Security,Security,Security aspect,4981101,Gson Security concerns,I plan to use Gson (http://code.google.com/p/google-gson/) on my server and transfer JavaScript objects from the browser to Java objects on the server. I will use Ajax to POST the request. I am a little concerned that everybody will be able to post Java objects to my server VM. A Java object that runs in the server process will have the potential to do whatever it wants. How does Gson address that?,"gson/json is data only, not code. So noone can post code to your server, just data.

When you're deserializing json, you specify which class (or you just extract primitives or collections of primitives directly) to populate with data - noone can send you json that aribtarly instantiate any of your classes living on the server.",deserialization process,"The Gson deserialization process requires the specification of classes or extraction of primitives or collections of primitives. As such, it is not possible for anyone to send arbitrary data that could instantiate any of the classes living in the server.",,1,,,,C20230706015031256736,0.71,0.68,0.61,0.81,0.58,0.61,0.86,0.86,0.78,0.94,0.84,0.9,0.73,0.71,0.67,0.95,0.86,0.84,0.63,0.73,0.74,0.78,0.89,0.77, 65deserialization process ,0.05,0.95,1,1
218,65,GSON,Security,Security,Security aspect,4981101,Gson Security concerns,I plan to use Gson (http://code.google.com/p/google-gson/) on my server and transfer JavaScript objects from the browser to Java objects on the server. I will use Ajax to POST the request. I am a little concerned that everybody will be able to post Java objects to my server VM. A Java object that runs in the server process will have the potential to do whatever it wants. How does Gson address that?,"gson/json is data only, not code. So noone can post code to your server, just data.

When you're deserializing json, you specify which class (or you just extract primitives or collections of primitives directly) to populate with data - noone can send you json that aribtarly instantiate any of your classes living on the server.",security enhancements,"While Gson's data processing methods ensure security, added security measures such as input validation can further enhance overall security and prevent potential security breaches on the server.",,1,,,,C20230706015031681465,0.83,0.81,0.62,0.68,0.89,0.74,0.9,0.68,0.79,0.77,0.68,0.52,0.72,0.68,0.65,0.51,0.89,0.48,0.91,0.64,0.84,0.66,0.81,0.59, 65security enhancements ,0.02,0.98,1,1
219,66,GSON,Security,Security,Security aspect,39176229,Deserializing to a templated class with Gson,"I have a class with a nested subclass
Currently I would like to be able to have a function that takes as a parameter the held class, and returns an appropriate instance of GenericClassHolder, i.e. something like my current code:
But this does not work in my current implementation, with the error

Unable to invoke no-args constructor for GenericClassHolder<T>. Register an InstanceCreator with Gson for this type may fix this problem.
How can I succinctly achieve my aforementioned goal of having a function that takes as input the specification for a class that may change and parses to this object accordingly? If it is not possible to achieve an elegant solution through a function, how can I achieve this goal without one?","I deployed this code in a Google Appengine instance, and due to some security reasons, I could not deserialize the way I was deserializing and replicate the same way on my local machine. The solution was to make the deserialized classes static; they were nested inside of an outer encapsulating class as well",adapting to environment,The solution to the technical issue involved adapting the code to work in a specific environment (Google Appengine).,,1,,,,C20230706015032607177,0.56,0.52,0.56,0.55,0.47,0.62,0.52,0.53,0.45,0.45,0.45,0.45,0.57,0.64,0.38,0.4,0.46,0.24,0.4,0.63,0.65,0.46,0.35,0.76, 66adapting to environment ,0.06,0.94,1,1
220,66,GSON,Security,Security,Security aspect,39176229,Deserializing to a templated class with Gson,"I have a class with a nested subclass
Currently I would like to be able to have a function that takes as a parameter the held class, and returns an appropriate instance of GenericClassHolder, i.e. something like my current code:
But this does not work in my current implementation, with the error

Unable to invoke no-args constructor for GenericClassHolder<T>. Register an InstanceCreator with Gson for this type may fix this problem.
How can I succinctly achieve my aforementioned goal of having a function that takes as input the specification for a class that may change and parses to this object accordingly? If it is not possible to achieve an elegant solution through a function, how can I achieve this goal without one?","I deployed this code in a Google Appengine instance, and due to some security reasons, I could not deserialize the way I was deserializing and replicate the same way on my local machine. The solution was to make the deserialized classes static; they were nested inside of an outer encapsulating class as well",focus on technical issue,The conversation is focused on resolving a technical issue with deserialization of a nested class using GSON.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015032151593,0.59,0.65,0.52,0.52,0.28,0.59,0.45,0.72,0.7,0.71,0.45,0.64,0.5,0.78,0.36,0.52,0.48,0.38,0.31,0.41,0.59,0.21,0.41,0.63, 66focus on technical issue ,0.58,0.42,0,0
221,66,GSON,Security,Security,Security aspect,39176229,Deserializing to a templated class with Gson,"I have a class with a nested subclass
Currently I would like to be able to have a function that takes as a parameter the held class, and returns an appropriate instance of GenericClassHolder, i.e. something like my current code:
But this does not work in my current implementation, with the error

Unable to invoke no-args constructor for GenericClassHolder<T>. Register an InstanceCreator with Gson for this type may fix this problem.
How can I succinctly achieve my aforementioned goal of having a function that takes as input the specification for a class that may change and parses to this object accordingly? If it is not possible to achieve an elegant solution through a function, how can I achieve this goal without one?","I deployed this code in a Google Appengine instance, and due to some security reasons, I could not deserialize the way I was deserializing and replicate the same way on my local machine. The solution was to make the deserialized classes static; they were nested inside of an outer encapsulating class as well",importance of secure handling,"The conversation highlights the importance of handling data and code securely in different environments, especially in context of web applications and web services.",,1,,,,C20230706015033617762,0.74,0.77,0.64,0.9,0.59,0.72,0.75,0.72,0.51,0.71,0.69,0.55,0.69,0.7,0.64,0.94,0.41,0.39,0.65,0.57,0.63,0.83,0.55,0.56, 66importance of secure handling ,0.01,0.99,1,1
222,66,GSON,Security,Security,Security aspect,39176229,Deserializing to a templated class with Gson,"I have a class with a nested subclass
Currently I would like to be able to have a function that takes as a parameter the held class, and returns an appropriate instance of GenericClassHolder, i.e. something like my current code:
But this does not work in my current implementation, with the error

Unable to invoke no-args constructor for GenericClassHolder<T>. Register an InstanceCreator with Gson for this type may fix this problem.
How can I succinctly achieve my aforementioned goal of having a function that takes as input the specification for a class that may change and parses to this object accordingly? If it is not possible to achieve an elegant solution through a function, how can I achieve this goal without one?","I deployed this code in a Google Appengine instance, and due to some security reasons, I could not deserialize the way I was deserializing and replicate the same way on my local machine. The solution was to make the deserialized classes static; they were nested inside of an outer encapsulating class as well",possible security implications,The issue with deserialization could have possible security implications related to preventing malicious code execution or preventing unauthorized access to sensitive data.,No answer available against the challenge,0,Out of Context Challenge,Challenger,,C20230706015033125862,0.69,0.58,0.41,0.7,0.57,0.47,0.63,0.55,0.34,0.62,0.48,0.68,0.3,0.58,0.69,0.39,0.42,0.23,0.26,0.4,0.56,0.57,0.59,0.47, 66possible security implications ,0.74,0.26,0,1
223,67,GSON,Security,attack,Security aspect,53661408,Retrofit2 strange combination of different braces in front of request Body,"I am using Retrofit2 and I got stuck on the problem. I wrote simple entity for body:
also I wrote custom serializer for it:
DateTimeUtil:
and in request body somehow appears this:
How this "")]}'"" could be attached at front of my ""date"" json in request?
Only that I found is after JsonWriter do some magic in buffer.readByteString() it stores broken body.","I found where was the problem comes from:

Here my custom GsonBuilder:

private fun getGsonFactory(): Gson {
        return GsonBuilder()
            .setLenient()
            .serializeSpecialFloatingPointValues()
            .setLongSerializationPolicy(LongSerializationPolicy.DEFAULT)
            .generateNonExecutableJson()
            .enableComplexMapKeySerialization()
            .serializeNulls()
            .setPrettyPrinting()
            .registerTypeAdapter(CleaningProgress::class.java, CleaningProgressDeserializer())
            .create()
    }
The line of code below, add that "")]}'"" at front of request. Why am I add it to builder? I trust official documentation about this method:

Makes the output JSON non-executable in Javascript by prefixing the generated JSON with some special text. This prevents attacks from third-party sites through script sourcing.",trust in documentation,The developer trust that the official documentation is correct in their recommendation to use the generateNonExecutableJson() method to improve security.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015034552180,0.67,0.7,0.49,0.79,0.51,0.57,0.77,0.78,0.77,0.83,0.71,0.67,0.62,0.65,0.41,0.67,0.63,0.62,0.7,0.65,0.6,0.65,0.56,0.44, 67trust in documentation ,0.92,0.08,0,0
224,67,GSON,Security,attack,Security aspect,53661408,Retrofit2 strange combination of different braces in front of request Body,"I am using Retrofit2 and I got stuck on the problem. I wrote simple entity for body:
also I wrote custom serializer for it:
DateTimeUtil:
and in request body somehow appears this:
How this "")]}'"" could be attached at front of my ""date"" json in request?
Only that I found is after JsonWriter do some magic in buffer.readByteString() it stores broken body.","I found where was the problem comes from:

Here my custom GsonBuilder:

private fun getGsonFactory(): Gson {
        return GsonBuilder()
            .setLenient()
            .serializeSpecialFloatingPointValues()
            .setLongSerializationPolicy(LongSerializationPolicy.DEFAULT)
            .generateNonExecutableJson()
            .enableComplexMapKeySerialization()
            .serializeNulls()
            .setPrettyPrinting()
            .registerTypeAdapter(CleaningProgress::class.java, CleaningProgressDeserializer())
            .create()
    }
The line of code below, add that "")]}'"" at front of request. Why am I add it to builder? I trust official documentation about this method:

Makes the output JSON non-executable in Javascript by prefixing the generated JSON with some special text. This prevents attacks from third-party sites through script sourcing.",use of method,generateNonExecutableJson() adds a special prefix to the JSON output to make it non-executable in JavaScript and prevent potential attacks from third-party sites through script sourcing.,,1,,,,C20230706015034064138,0.84,0.86,0.46,0.88,0.36,0.45,0.9,0.84,0.78,0.8,0.74,0.79,0.68,0.82,0.5,0.68,0.31,0.39,0.91,0.81,0.86,0.78,0.85,0.78, 67use of method ,0.01,0.99,1,1
225,68,Spacy,Feature,tokenization,tokenization feature,59500498,spacy tokenizer: is there a way to use regex as a key in custom exceptions for update_exc,"It is possible to add custom exceptions to spacy tokenizer. And these exceptions work fine. However, as far as I know, it's possible to use only strings as keys to match for those exceptions. It's done this way:
The only clue I found is: https://github.com/explosion/spaCy/issues/840

In that revision of tokenizer_exceptions.py there was some way to use regexps as keys for tokenizer exceptions(however, I haven't found any examples to do so)

But in current revisions, at least initial analysis hasn't shown any ways to do s

So is there a way to solve this task?

(input: regex as a key for exception, output - phone numbers with spaces inside)","No, there's no way to have regular expressions as tokenizer exceptions. The tokenizer only looks for exceptions as exact string matches, mainly for reasons of speed. The other difficulty for this kind of example is that tokenizer exceptions currently can't contain spaces. (Support for spaces is planned for a future version of spacy, but not regexes, which would still be too slow.)

I think the best way to do this would be to add a custom pipeline component at the beginning of the pipeline that retokenizes the document with the retokenizer: https://spacy.io/api/doc#retokenize. You can provide any required attributes like lemmas while retokenizing.",custom retokenizer component,A custom pipeline component with the retokenizer can be added at the beginning to tokenize the document with desired attributes like lemmas.,,1,,,,C20230706015037365641,0.66,0.52,0.68,0.49,0.65,0.48,0.56,0.3,0.46,0.46,0.57,0.69,0.74,0.56,0.57,0.57,0.6,0.57,0.49,0.51,0.46,0.7,0.76,0.73, 68custom retokenizer component ,0.01,0.99,1,1
226,68,Spacy,Feature,tokenization,tokenization feature,59500498,spacy tokenizer: is there a way to use regex as a key in custom exceptions for update_exc,"It is possible to add custom exceptions to spacy tokenizer. And these exceptions work fine. However, as far as I know, it's possible to use only strings as keys to match for those exceptions. It's done this way:
The only clue I found is: https://github.com/explosion/spaCy/issues/840

In that revision of tokenizer_exceptions.py there was some way to use regexps as keys for tokenizer exceptions(however, I haven't found any examples to do so)

But in current revisions, at least initial analysis hasn't shown any ways to do s

So is there a way to solve this task?

(input: regex as a key for exception, output - phone numbers with spaces inside)","No, there's no way to have regular expressions as tokenizer exceptions. The tokenizer only looks for exceptions as exact string matches, mainly for reasons of speed. The other difficulty for this kind of example is that tokenizer exceptions currently can't contain spaces. (Support for spaces is planned for a future version of spacy, but not regexes, which would still be too slow.)

I think the best way to do this would be to add a custom pipeline component at the beginning of the pipeline that retokenizes the document with the retokenizer: https://spacy.io/api/doc#retokenize. You can provide any required attributes like lemmas while retokenizing.",exact string matches,The tokenizer only matches exceptions with exact string matches for reasons of speed.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015036106424,0.7,0.74,0.75,0.72,0.75,0.71,0.83,0.77,0.66,0.78,0.72,0.76,0.57,0.65,0.6,0.61,0.63,0.72,0.64,0.59,0.49,0.81,0.78,0.6, 68exact string matches ,0.49,0.51,0,0
227,68,Spacy,Feature,tokenization,tokenization feature,59500498,spacy tokenizer: is there a way to use regex as a key in custom exceptions for update_exc,"It is possible to add custom exceptions to spacy tokenizer. And these exceptions work fine. However, as far as I know, it's possible to use only strings as keys to match for those exceptions. It's done this way:
The only clue I found is: https://github.com/explosion/spaCy/issues/840

In that revision of tokenizer_exceptions.py there was some way to use regexps as keys for tokenizer exceptions(however, I haven't found any examples to do so)

But in current revisions, at least initial analysis hasn't shown any ways to do s

So is there a way to solve this task?

(input: regex as a key for exception, output - phone numbers with spaces inside)","No, there's no way to have regular expressions as tokenizer exceptions. The tokenizer only looks for exceptions as exact string matches, mainly for reasons of speed. The other difficulty for this kind of example is that tokenizer exceptions currently can't contain spaces. (Support for spaces is planned for a future version of spacy, but not regexes, which would still be too slow.)

I think the best way to do this would be to add a custom pipeline component at the beginning of the pipeline that retokenizes the document with the retokenizer: https://spacy.io/api/doc#retokenize. You can provide any required attributes like lemmas while retokenizing.",no regex support,Spacy does not support regular expressions as tokenizer exceptions.,,1,,,,C20230706015035650962,0.64,0.59,0.68,0.84,0.69,0.69,0.52,0.44,0.49,0.73,0.82,0.78,0.59,0.67,0.75,0.92,0.65,0.61,0.43,0.5,0.57,0.87,0.71,0.78, 68no regex support ,0.01,0.99,1,1
228,68,Spacy,Feature,tokenization,tokenization feature,59500498,spacy tokenizer: is there a way to use regex as a key in custom exceptions for update_exc,"It is possible to add custom exceptions to spacy tokenizer. And these exceptions work fine. However, as far as I know, it's possible to use only strings as keys to match for those exceptions. It's done this way:
The only clue I found is: https://github.com/explosion/spaCy/issues/840

In that revision of tokenizer_exceptions.py there was some way to use regexps as keys for tokenizer exceptions(however, I haven't found any examples to do so)

But in current revisions, at least initial analysis hasn't shown any ways to do s

So is there a way to solve this task?

(input: regex as a key for exception, output - phone numbers with spaces inside)","No, there's no way to have regular expressions as tokenizer exceptions. The tokenizer only looks for exceptions as exact string matches, mainly for reasons of speed. The other difficulty for this kind of example is that tokenizer exceptions currently can't contain spaces. (Support for spaces is planned for a future version of spacy, but not regexes, which would still be too slow.)

I think the best way to do this would be to add a custom pipeline component at the beginning of the pipeline that retokenizes the document with the retokenizer: https://spacy.io/api/doc#retokenize. You can provide any required attributes like lemmas while retokenizing.",no spaces,Tokenizer exceptions currently do not support spaces.,,1,,,,C20230706015036902513,0.73,0.42,0.56,0.83,0.76,0.82,0.54,0.35,0.49,0.62,0.68,0.9,0.65,0.37,0.52,0.64,0.61,0.78,0.38,0.45,0.56,0.79,0.69,0.73, 68no spaces ,0.08,0.92,1,1
229,69,Spacy,Feature,tokenization,tokenization feature,50719732,Keeping all white spaces as tokens,"I have a question about whether there is a way to keep single white space as an independent token in spaCy tokenization.

For example if I ran:

import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is easy."")
toks = [w.text for w in doc]
toks
The result is

['This', 'is', 'easy', '.']
Instead, I would like to have something like

['This', ' ', 'is', ' ', 'easy', '.']
Is there are a simple way to do that?","spaCy exposes the token's whitespace as the whitespace_ attribute. So if you only need a list of strings, you could do:

token_texts = []
for token in doc:
   token_texts.append(token.text)
   if token.whitespace_:  # filter out empty strings
       token_texts.append(token.whitespace_)
If you want to create an actual Doc object out of those tokens, that's possible, too. Doc objects can be constructed with a words keyword argument (a list of strings to add as tokens). However, I'm not sure how useful that would be.",construction of doc objects,"Spacy allows the creation of an actual Doc object out of the tokens, but it may not be very useful in certain cases.",,1,,,,C20230706015038633753,0.71,0.8,0.68,0.8,0.87,0.8,0.7,0.56,0.55,0.68,0.58,0.81,0.65,0.75,0.56,0.74,0.94,0.73,0.58,0.72,0.69,0.62,0.69,0.69, 69construction of doc objects ,0.01,0.99,1,1
230,69,Spacy,Feature,tokenization,tokenization feature,50719732,Keeping all white spaces as tokens,"I have a question about whether there is a way to keep single white space as an independent token in spaCy tokenization.

For example if I ran:

import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is easy."")
toks = [w.text for w in doc]
toks
The result is

['This', 'is', 'easy', '.']
Instead, I would like to have something like

['This', ' ', 'is', ' ', 'easy', '.']
Is there are a simple way to do that?","spaCy exposes the token's whitespace as the whitespace_ attribute. So if you only need a list of strings, you could do:

token_texts = []
for token in doc:
   token_texts.append(token.text)
   if token.whitespace_:  # filter out empty strings
       token_texts.append(token.whitespace_)
If you want to create an actual Doc object out of those tokens, that's possible, too. Doc objects can be constructed with a words keyword argument (a list of strings to add as tokens). However, I'm not sure how useful that would be.",use of whitespace_ attribute,The whitespace_ attribute can be used to expose the token's whitespace and create a list of strings that includes white spaces.,wrong labeling,1,Generic Error,Mixed,,C20230706015038195214,0.7,0.76,0.59,0.71,0.41,0.33,0.87,0.85,0.73,0.82,0.76,0.75,0.73,0.67,0.43,0.57,0.51,0.33,0.63,0.85,0.78,0.7,0.67,0.81, 69use of whitespace_ attribute ,0.01,0.99,1,1
231,69,Spacy,Feature,tokenization,tokenization feature,50719732,Keeping all white spaces as tokens,"I have a question about whether there is a way to keep single white space as an independent token in spaCy tokenization.

For example if I ran:

import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is easy."")
toks = [w.text for w in doc]
toks
The result is

['This', 'is', 'easy', '.']
Instead, I would like to have something like

['This', ' ', 'is', ' ', 'easy', '.']
Is there are a simple way to do that?","spaCy exposes the token's whitespace as the whitespace_ attribute. So if you only need a list of strings, you could do:

token_texts = []
for token in doc:
   token_texts.append(token.text)
   if token.whitespace_:  # filter out empty strings
       token_texts.append(token.whitespace_)
If you want to create an actual Doc object out of those tokens, that's possible, too. Doc objects can be constructed with a words keyword argument (a list of strings to add as tokens). However, I'm not sure how useful that would be.",whitespace tokenization available,Spacy provides support for tokenizing whitespace as individual tokens.,Out of Scope Challenge,0,Out of Scope Challenge,Challenger,,C20230706015037794630,0.76,0.63,0.31,0.73,0.17,0.13,0.89,0.6,0.51,0.72,0.58,0.55,0.62,0.65,0.57,0.66,0.31,0.32,0.62,0.64,0.5,0.86,0.67,0.61, 69whitespace tokenization available ,0.01,0.99,1,0
232,70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",adding language subclass,A Language subclass can be added for specific dialects to improve tokenization accuracy.,,1,,,,C20230706015040071473,0.76,0.74,0.59,0.74,0.7,0.69,0.77,0.9,0.73,0.82,0.88,0.8,0.61,0.66,0.56,0.53,0.57,0.58,0.79,0.72,0.8,0.83,0.88,0.83, 70adding language subclass ,0.01,0.99,1,1
233,70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",context-dependent tokenization,Spacy's tokenization feature relies on language-specific data to define rules based on surrounding characters.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015039128403,0.74,0.6,0.61,0.75,0.68,0.73,0.65,0.68,0.7,0.67,0.73,0.8,0.64,0.71,0.53,0.56,0.56,0.62,0.56,0.66,0.72,0.66,0.76,0.79, 70context-dependent tokenization ,0.78,0.22,0,0
234,70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",customizing tokenization rules,"For specific cases, tokenization rules can be customized to provide accurate results.",,1,,,,C20230706015039640525,0.77,0.67,0.55,0.65,0.69,0.91,0.77,0.78,0.65,0.84,0.68,0.76,0.57,0.68,0.67,0.41,0.44,0.93,0.7,0.71,0.69,0.82,0.86,0.87, 70customizing tokenization rules ,0.03,0.97,1,1
235,70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",merging split tokens,Adding a component to merge split tokens back together can improve tokenization accuracy.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015040518241,0.79,0.67,0.51,0.78,0.62,0.66,0.71,0.73,0.53,0.82,0.73,0.81,0.76,0.62,0.61,0.61,0.62,0.51,0.76,0.77,0.59,0.81,0.78,0.72, 70merging split tokens ,0.55,0.45,0,0
236,70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",using multilanguage class,The language-independent MultiLanguage class allows basic tokenization rules but not English-specific rules.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015040931352,0.72,0.74,0.56,0.71,0.72,0.74,0.64,0.75,0.68,0.8,0.73,0.72,0.54,0.73,0.6,0.55,0.48,0.75,0.59,0.64,0.63,0.66,0.65,0.68, 70using multilanguage class ,0.89,0.11,0,0
237,71,Spacy,Feature,tokenization,tokenization feature,50330455,How to detokenize spacy text without doc context?,"I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.

The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.

Example:

Input to Seq2Seq: Some text

Output from Seq2Seq: This does n't work .

Is there any API in spacy to reverse tokenization done by rules in its tokenizer?","TL;DR I've written a code that attempts to do it, the snippet is below.

Another approach, with a computational complexity of O(n^2) * would be to use a function I just wrote. The main thought was ""What spaCy splits, shall be rejoined once more!""
Downsides:

In this approach you may easily merge ""do"" and ""nt"", as well as strip space between the dot ""."" and preceding word. This method is not perfect, as there are multiple possible combinations of sentences that lead to specific spaCy tokenization.

I am not sure if there is a method to fully detokenize a sentence when all you have is spaCy separated text, but this is the best I've got.

After having searched for hours on Google, only a few answers came along, with this very stack question being opened on 3 of my tabs on chrome ;), and all it wrote was basically ""don't use spaCy, use revtok"". As I couldn't change the tokenization other researchers chose, I had to develop my own solution. Hope it helps someone ;)",attempted solution,The user presenting the answer attempted to detokenize the text by rejoining the words that Spacy splits.,wrong labeling,1,Generic Error,Mixed,,C20230706015041864245,0.74,0.75,0.69,0.79,0.85,0.87,0.66,0.71,0.73,0.67,0.84,0.72,0.56,0.67,0.64,0.65,0.63,0.63,0.56,0.72,0.61,0.84,0.76,0.81, 71attempted solution ,0.03,0.97,1,1
238,71,Spacy,Feature,tokenization,tokenization feature,50330455,How to detokenize spacy text without doc context?,"I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.

The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.

Example:

Input to Seq2Seq: Some text

Output from Seq2Seq: This does n't work .

Is there any API in spacy to reverse tokenization done by rules in its tokenizer?","TL;DR I've written a code that attempts to do it, the snippet is below.

Another approach, with a computational complexity of O(n^2) * would be to use a function I just wrote. The main thought was ""What spaCy splits, shall be rejoined once more!""
Downsides:

In this approach you may easily merge ""do"" and ""nt"", as well as strip space between the dot ""."" and preceding word. This method is not perfect, as there are multiple possible combinations of sentences that lead to specific spaCy tokenization.

I am not sure if there is a method to fully detokenize a sentence when all you have is spaCy separated text, but this is the best I've got.

After having searched for hours on Google, only a few answers came along, with this very stack question being opened on 3 of my tabs on chrome ;), and all it wrote was basically ""don't use spaCy, use revtok"". As I couldn't change the tokenization other researchers chose, I had to develop my own solution. Hope it helps someone ;)",imperfect solution,"The attempted solution may result in incorrectly merged words or stripped spaces between words and punctuation, as there are multiple possible combinations of sentences that lead to specific Spacy tokenization.",,1,,,,C20230706015042302919,0.5,0.66,0.73,0.52,0.54,0.89,0.9,0.75,0.86,0.77,0.85,0.69,0.83,0.79,0.7,0.57,0.54,0.94,0.8,0.74,0.81,0.73,0.73,0.82, 71imperfect solution ,0,1,1,1
239,71,Spacy,Feature,tokenization,tokenization feature,50330455,How to detokenize spacy text without doc context?,"I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.

The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.

Example:

Input to Seq2Seq: Some text

Output from Seq2Seq: This does n't work .

Is there any API in spacy to reverse tokenization done by rules in its tokenizer?","TL;DR I've written a code that attempts to do it, the snippet is below.

Another approach, with a computational complexity of O(n^2) * would be to use a function I just wrote. The main thought was ""What spaCy splits, shall be rejoined once more!""
Downsides:

In this approach you may easily merge ""do"" and ""nt"", as well as strip space between the dot ""."" and preceding word. This method is not perfect, as there are multiple possible combinations of sentences that lead to specific spaCy tokenization.

I am not sure if there is a method to fully detokenize a sentence when all you have is spaCy separated text, but this is the best I've got.

After having searched for hours on Google, only a few answers came along, with this very stack question being opened on 3 of my tabs on chrome ;), and all it wrote was basically ""don't use spaCy, use revtok"". As I couldn't change the tokenization other researchers chose, I had to develop my own solution. Hope it helps someone ;)",no built-in function,Spacy does not have a built-in function to reverse tokenization done by its tokenizer.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015041337049,0.64,0.92,0.55,0.46,0.46,0.9,0.5,0.85,0.55,0.45,0.74,0.67,0.54,0.56,0.51,0.47,0.29,0.56,0.48,0.58,0.64,0.74,0.71,0.81, 71no built-in function ,0.25,0.75,1,0
240,71,Spacy,Feature,tokenization,tokenization feature,50330455,How to detokenize spacy text without doc context?,"I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.

The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.

Example:

Input to Seq2Seq: Some text

Output from Seq2Seq: This does n't work .

Is there any API in spacy to reverse tokenization done by rules in its tokenizer?","TL;DR I've written a code that attempts to do it, the snippet is below.

Another approach, with a computational complexity of O(n^2) * would be to use a function I just wrote. The main thought was ""What spaCy splits, shall be rejoined once more!""
Downsides:

In this approach you may easily merge ""do"" and ""nt"", as well as strip space between the dot ""."" and preceding word. This method is not perfect, as there are multiple possible combinations of sentences that lead to specific spaCy tokenization.

I am not sure if there is a method to fully detokenize a sentence when all you have is spaCy separated text, but this is the best I've got.

After having searched for hours on Google, only a few answers came along, with this very stack question being opened on 3 of my tabs on chrome ;), and all it wrote was basically ""don't use spaCy, use revtok"". As I couldn't change the tokenization other researchers chose, I had to develop my own solution. Hope it helps someone ;)",no satisfactory solution found,"After hours of searching on Google, the user was not able to find a satisfactory solution and had to develop their own method to accomplish the detokenization.",,1,,,,C20230706015042805088,0.59,0.77,0.46,0.8,0.35,0.42,0.55,0.7,0.67,0.56,0.42,0.63,0.64,0.77,0.5,0.74,0.59,0.47,0.63,0.8,0.63,0.69,0.84,0.76, 71no satisfactory solution found ,0.03,0.97,1,1
241,72,Spacy,Feature,POS,POS tagging feature,42830248,how to write spacy matcher of POS regex,"Spacy has two features I'd like to combine - part-of-speech (POS) and rule-based matching.

How can I combine them in a neat way?

For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example

Rule-based matching can have POS rules?

If not - here is my current plan - gather everything in one string and apply regex","Sure, simply use the POS attribute.

import spacy
nlp = spacy.load('en')
from spacy.matcher import Matcher
from spacy.attrs import POS
matcher = Matcher(nlp.vocab)
matcher.add_pattern(""Adjective and noun"", [{POS: 'ADJ'}, {POS: 'NOUN'}])

doc = nlp(u'what are the main issues')
matches = matcher(doc)",customizability,"Spacy supports regex as well as rule-based matching, giving users the flexibility to choose an approach based on their needs.",,1,,,,C20230706015044887437,0.7,0.67,0.58,0.58,0.54,0.76,0.32,0.62,0.47,0.74,0.58,0.65,0.83,0.76,0.75,0.44,0.55,0.63,0.57,0.56,0.53,0.61,0.63,0.83, 72customizability ,0.27,0.73,1,1
242,72,Spacy,Feature,POS,POS tagging feature,42830248,how to write spacy matcher of POS regex,"Spacy has two features I'd like to combine - part-of-speech (POS) and rule-based matching.

How can I combine them in a neat way?

For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example

Rule-based matching can have POS rules?

If not - here is my current plan - gather everything in one string and apply regex","Sure, simply use the POS attribute.

import spacy
nlp = spacy.load('en')
from spacy.matcher import Matcher
from spacy.attrs import POS
matcher = Matcher(nlp.vocab)
matcher.add_pattern(""Adjective and noun"", [{POS: 'ADJ'}, {POS: 'NOUN'}])

doc = nlp(u'what are the main issues')
matches = matcher(doc)",ease of use,The POS attribute can be used seamlessly with Spacy matcher.,,1,,,,C20230706015044036755,0.61,0.53,0.55,0.63,0.55,0.56,0.42,0.57,0.55,0.47,0.37,0.7,0.64,0.47,0.61,0.55,0.54,0.39,0.75,0.71,0.56,0.64,0.71,0.47, 72ease of use ,0.05,0.95,1,1
243,72,Spacy,Feature,POS,POS tagging feature,42830248,how to write spacy matcher of POS regex,"Spacy has two features I'd like to combine - part-of-speech (POS) and rule-based matching.

How can I combine them in a neat way?

For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example

Rule-based matching can have POS rules?

If not - here is my current plan - gather everything in one string and apply regex","Sure, simply use the POS attribute.

import spacy
nlp = spacy.load('en')
from spacy.matcher import Matcher
from spacy.attrs import POS
matcher = Matcher(nlp.vocab)
matcher.add_pattern(""Adjective and noun"", [{POS: 'ADJ'}, {POS: 'NOUN'}])

doc = nlp(u'what are the main issues')
matches = matcher(doc)",efficiency,"Spacy provides efficient NLP pipelines, making it easier to process large amounts of text.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015045602098,0.78,0.55,0.71,0.67,0.8,0.54,0.51,0.5,0.49,0.7,0.65,0.55,0.61,0.6,0.62,0.6,0.61,0.47,0.69,0.52,0.61,0.5,0.58,0.63, 72efficiency ,0.01,0.99,1,0
244,72,Spacy,Feature,POS,POS tagging feature,42830248,how to write spacy matcher of POS regex,"Spacy has two features I'd like to combine - part-of-speech (POS) and rule-based matching.

How can I combine them in a neat way?

For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example

Rule-based matching can have POS rules?

If not - here is my current plan - gather everything in one string and apply regex","Sure, simply use the POS attribute.

import spacy
nlp = spacy.load('en')
from spacy.matcher import Matcher
from spacy.attrs import POS
matcher = Matcher(nlp.vocab)
matcher.add_pattern(""Adjective and noun"", [{POS: 'ADJ'}, {POS: 'NOUN'}])

doc = nlp(u'what are the main issues')
matches = matcher(doc)",flexibility,Spacy matcher can easily incorporate POS regex to match patterns.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015043323359,0.79,0.68,0.47,0.63,0.62,0.68,0.35,0.52,0.5,0.65,0.47,0.53,0.58,0.63,0.84,0.47,0.54,0.44,0.8,0.51,0.55,0.56,0.56,0.55, 72flexibility ,0.54,0.46,0,0
245,73,Spacy,Feature,POS,POS tagging feature,47856247,Extract verb phrases using Spacy,I have been using Spacy for noun chunks extraction using Doc.noun_chunks property provided by Spacy. How could I extract verb phrases from input text using Spacy library (of the form 'VERB ? ADV * VERB +' )?,"This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
On how to highlight the verb phrases do check the link below.

Highlight verb phrases using spacy and html

Another Approach:

Recently observed Textacy has made some changes to regex matches. Based on that approach i tried this way.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The cat sat on the mat. He dog jumped into the water. The author is writing a book.'
pattern = [{'POS': 'VERB', 'OP': '?'},
           {'POS': 'ADV', 'OP': '*'},
           {'POS': 'VERB', 'OP': '+'}]
doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

sat
jumped
writing
I checked the POS matches in this links seems the result is not the intended one.

[https://explosion.ai/demos/matcher][1]

Did anybody try framing POS tags instead of Regexp pattern for finding Verb phrases?",alternative approach using textacy,"The answer provides an alternative approach to using regex patterns, using the Textacy library's make_spacy_doc and extract functions to extract verb phrases.",,1,,,,C20230706015047516109,0.6,0.67,0.73,0.71,0.61,0.7,0.79,0.77,0.63,0.84,0.81,0.73,0.66,0.59,0.71,0.61,0.52,0.74,0.68,0.7,0.73,0.7,0.68,0.8, 73alternative approach using textacy ,0,1,1,1
246,73,Spacy,Feature,POS,POS tagging feature,47856247,Extract verb phrases using Spacy,I have been using Spacy for noun chunks extraction using Doc.noun_chunks property provided by Spacy. How could I extract verb phrases from input text using Spacy library (of the form 'VERB ? ADV * VERB +' )?,"This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
On how to highlight the verb phrases do check the link below.

Highlight verb phrases using spacy and html

Another Approach:

Recently observed Textacy has made some changes to regex matches. Based on that approach i tried this way.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The cat sat on the mat. He dog jumped into the water. The author is writing a book.'
pattern = [{'POS': 'VERB', 'OP': '?'},
           {'POS': 'ADV', 'OP': '*'},
           {'POS': 'VERB', 'OP': '+'}]
doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

sat
jumped
writing
I checked the POS matches in this links seems the result is not the intended one.

[https://explosion.ai/demos/matcher][1]

Did anybody try framing POS tags instead of Regexp pattern for finding Verb phrases?",pos tags may be more reliable,"As noted in the answer, using POS tags instead of regex patterns for finding verb phrases may be more reliable and accurate.",,1,,,,C20230706015048042149,0.57,0.78,0.71,0.73,0.68,0.83,0.74,0.81,0.74,0.72,0.81,0.74,0.67,0.66,0.66,0.67,0.63,0.63,0.75,0.78,0.7,0.8,0.78,0.86, 73pos tags may be more reliable ,0.01,0.99,1,1
247,73,Spacy,Feature,POS,POS tagging feature,47856247,Extract verb phrases using Spacy,I have been using Spacy for noun chunks extraction using Doc.noun_chunks property provided by Spacy. How could I extract verb phrases from input text using Spacy library (of the form 'VERB ? ADV * VERB +' )?,"This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
On how to highlight the verb phrases do check the link below.

Highlight verb phrases using spacy and html

Another Approach:

Recently observed Textacy has made some changes to regex matches. Based on that approach i tried this way.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The cat sat on the mat. He dog jumped into the water. The author is writing a book.'
pattern = [{'POS': 'VERB', 'OP': '?'},
           {'POS': 'ADV', 'OP': '*'},
           {'POS': 'VERB', 'OP': '+'}]
doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

sat
jumped
writing
I checked the POS matches in this links seems the result is not the intended one.

[https://explosion.ai/demos/matcher][1]

Did anybody try framing POS tags instead of Regexp pattern for finding Verb phrases?",regex pattern may not be accurate,"While the regex pattern approach described in the answer can be used to extract verb phrases, it may not always produce accurate results.",No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706015046508182,0.85,0.78,0.71,0.91,0.63,0.5,0.86,0.83,0.62,0.83,0.63,0.64,0.65,0.56,0.62,0.71,0.48,0.42,0.84,0.81,0.67,0.88,0.72,0.78, 73regex pattern may not be accurate ,0.02,0.98,1,1
248,73,Spacy,Feature,POS,POS tagging feature,47856247,Extract verb phrases using Spacy,I have been using Spacy for noun chunks extraction using Doc.noun_chunks property provided by Spacy. How could I extract verb phrases from input text using Spacy library (of the form 'VERB ? ADV * VERB +' )?,"This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
On how to highlight the verb phrases do check the link below.

Highlight verb phrases using spacy and html

Another Approach:

Recently observed Textacy has made some changes to regex matches. Based on that approach i tried this way.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The cat sat on the mat. He dog jumped into the water. The author is writing a book.'
pattern = [{'POS': 'VERB', 'OP': '?'},
           {'POS': 'ADV', 'OP': '*'},
           {'POS': 'VERB', 'OP': '+'}]
doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

sat
jumped
writing
I checked the POS matches in this links seems the result is not the intended one.

[https://explosion.ai/demos/matcher][1]

Did anybody try framing POS tags instead of Regexp pattern for finding Verb phrases?",spacy supports pos tagging,"The Spacy library has a variety of functions that support POS tagging, including the Doc and extract functions that were used to extract verb phrases from the input text.",,1,,,,C20230706015046058134,0.55,0.53,0.62,0.55,0.62,0.53,0.87,0.74,0.45,0.75,0.46,0.51,0.42,0.61,0.48,0.6,0.35,0.47,0.82,0.66,0.69,0.66,0.7,0.61, 73spacy supports pos tagging ,0.05,0.95,1,1
249,74,Spacy,Feature,POS,POS tagging feature,58136715,Is there a way to determine part of speech patterns in a dataset of sentences?,I have a pandas dataframe with sentences and I'd like to determine part of speech patterns in each sentence. What's a good way to do this?,"The two most popular POS taggers for Python are NLTK and spaCy. spaCy is both faster and easier to use, and as you need to do more, spaCy also provides a nice API for interacting with wordnet, word Vectors, and other more advanced tools.

This is how you do POS reading with spaCy:

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)",spacy has a pre-built model for english language,"Spacy provides a pre-trained model for English language, which can be used directly for tagging POS in English text.",,1,,,,C20230706015050136022,0.7,0.65,0.74,0.67,0.69,0.75,0.53,0.58,0.45,0.6,0.57,0.69,0.76,0.43,0.58,0.39,0.64,0.33,0.65,0.67,0.57,0.82,0.64,0.59, 74spacy has a pre-built model for english language ,0.08,0.92,1,1
250,74,Spacy,Feature,POS,POS tagging feature,58136715,Is there a way to determine part of speech patterns in a dataset of sentences?,I have a pandas dataframe with sentences and I'd like to determine part of speech patterns in each sentence. What's a good way to do this?,"The two most popular POS taggers for Python are NLTK and spaCy. spaCy is both faster and easier to use, and as you need to do more, spaCy also provides a nice API for interacting with wordnet, word Vectors, and other more advanced tools.

This is how you do POS reading with spaCy:

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)",spacy is faster and easier,"As compared to NLTK, Spacy is faster and easier to use. It is a great tool for POS tagging and can handle even complex and lengthy datasets.",,1,,,,C20230706015049087216,0.65,0.71,0.59,0.81,0.64,0.69,0.58,0.78,0.72,0.79,0.69,0.84,0.78,0.85,0.72,0.73,0.49,0.57,0.73,0.72,0.7,0.67,0.51,0.71, 74spacy is faster and easier ,0.02,0.98,1,1
251,74,Spacy,Feature,POS,POS tagging feature,58136715,Is there a way to determine part of speech patterns in a dataset of sentences?,I have a pandas dataframe with sentences and I'd like to determine part of speech patterns in each sentence. What's a good way to do this?,"The two most popular POS taggers for Python are NLTK and spaCy. spaCy is both faster and easier to use, and as you need to do more, spaCy also provides a nice API for interacting with wordnet, word Vectors, and other more advanced tools.

This is how you do POS reading with spaCy:

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)",spacy provides advanced tools,"Spacy's API extends beyond basic tagging, and provides additional tools for interacting with wordnet, word vectors, and other advanced features.",,1,,,,C20230706015049608399,0.71,0.51,0.58,0.64,0.65,0.68,0.81,0.73,0.61,0.77,0.77,0.7,0.66,0.54,0.63,0.47,0.84,0.57,0.67,0.67,0.72,0.56,0.73,0.83, 74spacy provides advanced tools ,0.15,0.85,1,1
252,74,Spacy,Feature,POS,POS tagging feature,58136715,Is there a way to determine part of speech patterns in a dataset of sentences?,I have a pandas dataframe with sentences and I'd like to determine part of speech patterns in each sentence. What's a good way to do this?,"The two most popular POS taggers for Python are NLTK and spaCy. spaCy is both faster and easier to use, and as you need to do more, spaCy also provides a nice API for interacting with wordnet, word Vectors, and other more advanced tools.

This is how you do POS reading with spaCy:

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)",spacy supports pos tagging,Spacy is mentioned as one of the two most popular POS taggers for Python in the conversation. Spacy provides a simple and easy to use API for POS tagging.,,1,,,,C20230706015048564484,0.54,0.58,0.37,0.69,0.45,0.56,0.71,0.65,0.79,0.85,0.81,0.72,0.46,0.51,0.64,0.56,0.52,0.53,0.68,0.63,0.74,0.89,0.88,0.83, 74spacy supports pos tagging ,0.09,0.91,1,1
253,75,Spacy,Feature,POS,POS tagging feature,34102420,POS tagging using spaCy,"I am trying to do POS tagging using the spaCy module in Python.
Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like

a=""we had crispy fries""
It recognizes that crispy is an adjective. Here is the output:

[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]
I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case.

Is there any way I can get crispy to be tagged as an adjective in the second case too?","TL;DR: You should accept the occasional error.

Details:

Spacy's tagger is statistical, meaning that the tags you get are its best estimate based on the data it was shown during training. I would guess those data did not contain the word dosa. The tagger had to guess, and guessed wrong. There isn't an easy way to correct its output, because it is not using rules or anything you can modify easily. The model has been trained on a standard corpus of English, which may be quite different to the kind of language you are using it for (domain). If error rate is too high for your purposes, you can re-train the model using domain-specific data. This will be very laborious though. Ask yourself what you are trying to achieve and whether 3% error rate in PoS tagging is the worst of your problems.

In general, you shouldn't judge the performance of a statistical system on a case-by-case basis. The accuracy of modern English PoS taggers is around 97%, which is roughly the same as the average human. You will inevitably get some errors. However, the errors of the model will not be the same as the human errors, as the two have ""learnt"" how to solve the problem in a different way. Sometimes the model will get confused by things you and I consider obvious, e.g. your example. This doesn't mean it is bad overall, or that PoS tagging is your real problem.",97% accuracy,"The accuracy of modern English PoS taggers, including Spacy, is around 97%, which is comparable to the average human performance.",,1,,,,C20230706015051783880,0.87,0.69,0.44,0.85,0.32,0.34,0.65,0.65,0.89,0.59,0.61,0.75,0.59,0.59,0.54,0.56,0.37,0.44,0.68,0.61,0.71,0.78,0.93,0.76, 7597% accuracy ,0.02,0.98,1,1
254,75,Spacy,Feature,POS,POS tagging feature,34102420,POS tagging using spaCy,"I am trying to do POS tagging using the spaCy module in Python.
Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like

a=""we had crispy fries""
It recognizes that crispy is an adjective. Here is the output:

[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]
I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case.

Is there any way I can get crispy to be tagged as an adjective in the second case too?","TL;DR: You should accept the occasional error.

Details:

Spacy's tagger is statistical, meaning that the tags you get are its best estimate based on the data it was shown during training. I would guess those data did not contain the word dosa. The tagger had to guess, and guessed wrong. There isn't an easy way to correct its output, because it is not using rules or anything you can modify easily. The model has been trained on a standard corpus of English, which may be quite different to the kind of language you are using it for (domain). If error rate is too high for your purposes, you can re-train the model using domain-specific data. This will be very laborious though. Ask yourself what you are trying to achieve and whether 3% error rate in PoS tagging is the worst of your problems.

In general, you shouldn't judge the performance of a statistical system on a case-by-case basis. The accuracy of modern English PoS taggers is around 97%, which is roughly the same as the average human. You will inevitably get some errors. However, the errors of the model will not be the same as the human errors, as the two have ""learnt"" how to solve the problem in a different way. Sometimes the model will get confused by things you and I consider obvious, e.g. your example. This doesn't mean it is bad overall, or that PoS tagging is your real problem.",occasional errors,"Due to the statistical nature of Spacy's tagger, there can be occasional errors, particularly for new or uncommon words.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015051198148,0.64,0.73,0.41,0.69,0.68,0.64,0.66,0.78,0.5,0.77,0.87,0.66,0.58,0.63,0.6,0.56,0.39,0.51,0.52,0.62,0.53,0.83,0.63,0.67, 75occasional errors ,0.8,0.2,0,0
255,75,Spacy,Feature,POS,POS tagging feature,34102420,POS tagging using spaCy,"I am trying to do POS tagging using the spaCy module in Python.
Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like

a=""we had crispy fries""
It recognizes that crispy is an adjective. Here is the output:

[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]
I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case.

Is there any way I can get crispy to be tagged as an adjective in the second case too?","TL;DR: You should accept the occasional error.

Details:

Spacy's tagger is statistical, meaning that the tags you get are its best estimate based on the data it was shown during training. I would guess those data did not contain the word dosa. The tagger had to guess, and guessed wrong. There isn't an easy way to correct its output, because it is not using rules or anything you can modify easily. The model has been trained on a standard corpus of English, which may be quite different to the kind of language you are using it for (domain). If error rate is too high for your purposes, you can re-train the model using domain-specific data. This will be very laborious though. Ask yourself what you are trying to achieve and whether 3% error rate in PoS tagging is the worst of your problems.

In general, you shouldn't judge the performance of a statistical system on a case-by-case basis. The accuracy of modern English PoS taggers is around 97%, which is roughly the same as the average human. You will inevitably get some errors. However, the errors of the model will not be the same as the human errors, as the two have ""learnt"" how to solve the problem in a different way. Sometimes the model will get confused by things you and I consider obvious, e.g. your example. This doesn't mean it is bad overall, or that PoS tagging is your real problem.",retraining with domain data,"While there is no easy way to correct occasional errors, you can retrain the Spacy model with domain-specific data if higher accuracy is desired.",,1,,,,C20230706015052235583,0.82,0.8,0.58,0.81,0.42,0.44,0.74,0.79,0.52,0.66,0.33,0.51,0.51,0.61,0.64,0.58,0.52,0.63,0.61,0.63,0.56,0.92,0.88,0.89, 75retraining with domain data ,0.02,0.98,1,1
256,75,Spacy,Feature,POS,POS tagging feature,34102420,POS tagging using spaCy,"I am trying to do POS tagging using the spaCy module in Python.
Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like

a=""we had crispy fries""
It recognizes that crispy is an adjective. Here is the output:

[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]
I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case.

Is there any way I can get crispy to be tagged as an adjective in the second case too?","TL;DR: You should accept the occasional error.

Details:

Spacy's tagger is statistical, meaning that the tags you get are its best estimate based on the data it was shown during training. I would guess those data did not contain the word dosa. The tagger had to guess, and guessed wrong. There isn't an easy way to correct its output, because it is not using rules or anything you can modify easily. The model has been trained on a standard corpus of English, which may be quite different to the kind of language you are using it for (domain). If error rate is too high for your purposes, you can re-train the model using domain-specific data. This will be very laborious though. Ask yourself what you are trying to achieve and whether 3% error rate in PoS tagging is the worst of your problems.

In general, you shouldn't judge the performance of a statistical system on a case-by-case basis. The accuracy of modern English PoS taggers is around 97%, which is roughly the same as the average human. You will inevitably get some errors. However, the errors of the model will not be the same as the human errors, as the two have ""learnt"" how to solve the problem in a different way. Sometimes the model will get confused by things you and I consider obvious, e.g. your example. This doesn't mean it is bad overall, or that PoS tagging is your real problem.",spacy's statistical tagger,"Spacy's tagger is a statistical model, meaning that its tags are based on its best estimate from the data it was trained on.",,1,,,,C20230706015050662611,0.72,0.64,0.58,0.8,0.65,0.65,0.66,0.61,0.51,0.56,0.84,0.6,0.69,0.61,0.4,0.47,0.87,0.37,0.48,0.48,0.5,0.72,0.75,0.92, 75spacy's statistical tagger ,0.07,0.93,1,1
257,76,Spacy,Feature,dependency parsing,dependency parsing feature,55521858,"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed. 

I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".

Is this the correct behaviour, or am I using spacy incorrectly? If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.","There are a few things to consider here

Lemmatisation is token based
POS tagging and dependency parsing is predictive
You probably will get the big dog if you take the lemma_ attribute for each token. It does not update the token pos of you use the attribute.

Also, since dependency parsing and POS tagging is trained in a predictive model, it is not guaranteed to always be ""right"" from a human linguistic perspective.

Other than the lemma issue, it seems you are using spacy correct",lemmatisation is token based,The lemma attribute for each token does not update the token pos. It is token based.,,1,,,,C20230706015052945867,0.72,0.78,0.7,0.69,0.66,0.86,0.48,0.74,0.75,0.67,0.64,0.81,0.86,0.53,0.82,0.74,0.62,0.66,0.66,0.1,0.52,0.3,0.63,0.52, 76lemmatisation is token based ,0.01,0.99,1,1
258,76,Spacy,Feature,dependency parsing,dependency parsing feature,55521858,"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed. 

I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".

Is this the correct behaviour, or am I using spacy incorrectly? If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.","There are a few things to consider here

Lemmatisation is token based
POS tagging and dependency parsing is predictive
You probably will get the big dog if you take the lemma_ attribute for each token. It does not update the token pos of you use the attribute.

Also, since dependency parsing and POS tagging is trained in a predictive model, it is not guaranteed to always be ""right"" from a human linguistic perspective.

Other than the lemma issue, it seems you are using spacy correct",no major issues with dependency parsing in spacy,"Overall, there are no major issues with Spacy's dependency parsing features and the user's specific issues with noun chunking are likely due to misunderstandings rather than limitations of the library.",,1,,,,C20230706015054856693,0.76,0.71,0.44,0.47,0.79,0.46,0.69,0.86,0.76,0.67,0.77,0.67,0.7,0.6,0.64,0.39,0.66,0.49,0.54,0.69,0.69,0.85,0.78,0.84, 76no major issues with dependency parsing in spacy ,0.14,0.86,1,1
259,76,Spacy,Feature,dependency parsing,dependency parsing feature,55521858,"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed. 

I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".

Is this the correct behaviour, or am I using spacy incorrectly? If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.","There are a few things to consider here

Lemmatisation is token based
POS tagging and dependency parsing is predictive
You probably will get the big dog if you take the lemma_ attribute for each token. It does not update the token pos of you use the attribute.

Also, since dependency parsing and POS tagging is trained in a predictive model, it is not guaranteed to always be ""right"" from a human linguistic perspective.

Other than the lemma issue, it seems you are using spacy correct",pos tagging and dependency parsing is predictive,Dependency parsing and POS tagging in Spacy is based on a predictive model.,Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015053804277,0.66,0.5,0.54,0.87,0.81,0.78,0.6,0.57,0.51,0.89,0.8,0.74,0.65,0.57,-0.03,0.73,0.47,0.58,0.63,0.55,0.19,0.85,0.01,0.03, 76pos tagging and dependency parsing is predictive ,0.08,0.92,1,1
260,76,Spacy,Feature,dependency parsing,dependency parsing feature,55521858,"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed. 

I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".

Is this the correct behaviour, or am I using spacy incorrectly? If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.","There are a few things to consider here

Lemmatisation is token based
POS tagging and dependency parsing is predictive
You probably will get the big dog if you take the lemma_ attribute for each token. It does not update the token pos of you use the attribute.

Also, since dependency parsing and POS tagging is trained in a predictive model, it is not guaranteed to always be ""right"" from a human linguistic perspective.

Other than the lemma issue, it seems you are using spacy correct",predictive model not always linguistically accurate,The predictive nature of Spacy's parsing model means that it may not always align perfectly with human linguistic perspectives.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706015054333723,0.83,0.73,0.61,0.91,0.65,0.62,0.77,0.8,0.76,0.62,0.64,0.73,0.63,0.51,0.55,0.61,0.62,0.54,0.8,0.79,0.75,0.72,0.79,0.69, 76predictive model not always linguistically accurate ,0.35,0.65,1,1
261,77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",adding tokens with the same label,The user can add more than one token with the same label or create a list of tokens for the same label.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015057731889,0.76,0.72,0.68,0.8,0.65,0.7,0.58,0.53,0.37,0.74,0.63,0.53,0.71,0.61,0.69,0.86,0.83,0.87,0.62,0.4,0.34,0.7,0.61,0.46, 77adding tokens with the same label ,0,1,1,0
262,77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",attribute for relationships,The dep and dep_ attributes can be used to find relationships between tokens.,,1,,,,C20230706015056426706,0.73,0.73,0.59,0.93,0.82,0.83,0.76,0.75,0.65,0.93,0.88,0.86,0.42,0.55,0.71,0.92,0.62,0.56,0.17,0.15,0.63,0.73,0.31,0.31, 77attribute for relationships ,0.08,0.92,1,1
263,77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",dependency parsing for relations,Spacy supports dependency parsing to find relations between tokens.,,1,,,,C20230706015055389209,0.67,0.64,0.74,0.77,0.69,0.55,0.71,0.61,0.63,0.82,0.56,0.58,0.52,0.71,0.69,0.55,0.55,0.52,0.64,0.75,0.61,0.66,0.63,0.71, 77dependency parsing for relations ,0.12,0.88,1,1
264,77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",no explicit mention of regex,It is not explicitly mentioned whether regex can be used to find patterns.,,1,,,,C20230706015056954946,0.68,0.57,0.66,0.71,0.76,0.7,0.69,0.51,0.62,0.44,0.65,0.63,0.59,0.87,0.7,0.89,0.9,0.9,0.48,0.74,0.58,0.59,0.66,0.75, 77no explicit mention of regex ,0.05,0.95,1,1
265,77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",traversing the dependency tree,The user can traverse the tree to find relations between tokens.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015055909051,0.68,0.71,0.85,0.78,0.82,0.77,0.59,0.55,0.54,0.86,0.58,0.57,0.62,0.48,0.56,0.62,0.61,0.67,0.55,0.61,0.36,0.63,0.62,0.48, 77traversing the dependency tree ,0.47,0.53,0,0
266,78,Spacy,Feature,dependency parsing,dependency parsing feature,39258476,Collocations with spaCy,"I've been using NLTK for finding collocations, or n-grams, and have recently discovered the spaCy module for NLP. I've only just begun familiarizing myself with it and have, thus far, seen little mention for supported collocation functions.

Can spaCy be used to find collocations directly?

I have read through the documentation, but haven't seen mention.","Collocations detection also can be based on dependency parsing, but spaCy do not have support to do it. You can use spaCy as part of an approach, but not directly.

may you also consider gensim: https://radimrehurek.com/gensim/models/phrases.html

I hope it can help you",consider gensim library,The answer suggests considering the alternative library gensim for supported collocation functions.,,1,,,,C20230706015058794289,0.63,0.64,0.82,0.67,0.63,0.64,0.53,0.54,0.62,0.67,0.73,0.58,0.59,0.61,0.75,0.92,0.5,0.52,0.64,0.48,0.6,0.73,0.62,0.56, 78consider gensim library ,0.03,0.97,1,1
267,78,Spacy,Feature,dependency parsing,dependency parsing feature,39258476,Collocations with spaCy,"I've been using NLTK for finding collocations, or n-grams, and have recently discovered the spaCy module for NLP. I've only just begun familiarizing myself with it and have, thus far, seen little mention for supported collocation functions.

Can spaCy be used to find collocations directly?

I have read through the documentation, but haven't seen mention.","Collocations detection also can be based on dependency parsing, but spaCy do not have support to do it. You can use spaCy as part of an approach, but not directly.

may you also consider gensim: https://radimrehurek.com/gensim/models/phrases.html

I hope it can help you",no direct support,Spacy does not have direct support for collocation detection based on dependency parsing.,,1,,,,C20230706015058221783,0.51,0.45,0.6,0.87,0.56,0.6,0.64,0.28,0.76,0.48,0.67,0.24,0.65,0.44,0.55,0.53,0.49,0.54,0.76,0.74,0.75,0.9,0.88,0.86, 78no direct support ,0.02,0.98,1,1
268,78,Spacy,Feature,dependency parsing,dependency parsing feature,39258476,Collocations with spaCy,"I've been using NLTK for finding collocations, or n-grams, and have recently discovered the spaCy module for NLP. I've only just begun familiarizing myself with it and have, thus far, seen little mention for supported collocation functions.

Can spaCy be used to find collocations directly?

I have read through the documentation, but haven't seen mention.","Collocations detection also can be based on dependency parsing, but spaCy do not have support to do it. You can use spaCy as part of an approach, but not directly.

may you also consider gensim: https://radimrehurek.com/gensim/models/phrases.html

I hope it can help you",spacy as part of approach,"Although not directly supporting collocation detection based on dependency parsing, Spacy can be used as part of an approach in detecting collocations.",Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015059267312,0.62,0.55,0.71,0.83,0.76,0.77,0.71,0.58,0.86,0.83,0.69,0.61,0.49,0.54,0.44,0.9,0.77,0.73,0.8,0.79,0.75,0.91,0.93,0.88, 78spacy as part of approach ,0.32,0.68,1,0
269,79,Spacy,Feature,dependency parsing,dependency parsing feature,55498881,Vectorized form of cleaning function for NLP,"I made the following function to clean the text notes of my dataset :
Problem is when I want to clean all my dataset text, it take hour and hour. (my dataset is 70k row and between 100 to 5000 words per row)

I tried to use swifter to run the apply method on multiplethread like that : data.note_line_comment.swifter.apply(clean)

But it doesn't made really better as it took almost one hour.

I was wondering if there is any way to make a vectorized form of my function or maybe and other way to speed up the process. Any idea ?","Short answer

This type of problem inherently takes time.

Long answer

Use regular expressions
Change the spacy pipeline
The more information about the strings you need to make a decision, the longer it will take.

Good news is, if your cleaning of the text is relatively simplified, a few regular expressions might do the trick.

Otherwise you are using the spacy pipeline to help remove bits of text which is costly since it does many things by default:

Tokenisation
Lemmatisation
Dependency parsing
NER
Chunking
Alternatively, you can try your task again and turn off the aspects of the spacy pipeline you don't want which may speed it up quite a bit.

For example, maybe turn off named entity recognition, tagging and dependency parsing...

nlp = spacy.load(""en"", disable=[""parser"", ""tagger"", ""ner""])
Then try again, it will speed up.",dependency parsing bottleneck,"Dependency parsing is one of the default features of Spacy pipeline, which can slow down the processing time for certain tasks.",,1,,,,C20230706015100617801,0.55,0.4,0.54,0.79,0.67,0.73,0.82,0.72,0.66,0.88,0.79,0.67,0.57,0.48,0.75,0.62,0.65,0.56,0.73,0.7,0.69,0.85,0.83,0.76, 79dependency parsing bottleneck ,0.02,0.98,1,1
270,79,Spacy,Feature,dependency parsing,dependency parsing feature,55498881,Vectorized form of cleaning function for NLP,"I made the following function to clean the text notes of my dataset :
Problem is when I want to clean all my dataset text, it take hour and hour. (my dataset is 70k row and between 100 to 5000 words per row)

I tried to use swifter to run the apply method on multiplethread like that : data.note_line_comment.swifter.apply(clean)

But it doesn't made really better as it took almost one hour.

I was wondering if there is any way to make a vectorized form of my function or maybe and other way to speed up the process. Any idea ?","Short answer

This type of problem inherently takes time.

Long answer

Use regular expressions
Change the spacy pipeline
The more information about the strings you need to make a decision, the longer it will take.

Good news is, if your cleaning of the text is relatively simplified, a few regular expressions might do the trick.

Otherwise you are using the spacy pipeline to help remove bits of text which is costly since it does many things by default:

Tokenisation
Lemmatisation
Dependency parsing
NER
Chunking
Alternatively, you can try your task again and turn off the aspects of the spacy pipeline you don't want which may speed it up quite a bit.

For example, maybe turn off named entity recognition, tagging and dependency parsing...

nlp = spacy.load(""en"", disable=[""parser"", ""tagger"", ""ner""])
Then try again, it will speed up.",pipeline optimization,Spacy's pipeline features can be turned off to optimize performance for specific tasks. Dependency parsing along with other unnecessary features can be disabled to speed up the process.,,1,,,,C20230706015101017912,0.6,0.74,0.53,0.6,0.75,0.65,0.8,0.74,0.56,0.79,0.61,0.61,0.54,0.54,0.66,0.35,0.58,0.65,0.66,0.8,0.82,0.79,0.8,0.87, 79pipeline optimization ,0.02,0.98,1,1
271,79,Spacy,Feature,dependency parsing,dependency parsing feature,55498881,Vectorized form of cleaning function for NLP,"I made the following function to clean the text notes of my dataset :
Problem is when I want to clean all my dataset text, it take hour and hour. (my dataset is 70k row and between 100 to 5000 words per row)

I tried to use swifter to run the apply method on multiplethread like that : data.note_line_comment.swifter.apply(clean)

But it doesn't made really better as it took almost one hour.

I was wondering if there is any way to make a vectorized form of my function or maybe and other way to speed up the process. Any idea ?","Short answer

This type of problem inherently takes time.

Long answer

Use regular expressions
Change the spacy pipeline
The more information about the strings you need to make a decision, the longer it will take.

Good news is, if your cleaning of the text is relatively simplified, a few regular expressions might do the trick.

Otherwise you are using the spacy pipeline to help remove bits of text which is costly since it does many things by default:

Tokenisation
Lemmatisation
Dependency parsing
NER
Chunking
Alternatively, you can try your task again and turn off the aspects of the spacy pipeline you don't want which may speed it up quite a bit.

For example, maybe turn off named entity recognition, tagging and dependency parsing...

nlp = spacy.load(""en"", disable=[""parser"", ""tagger"", ""ner""])
Then try again, it will speed up.",spacy support,Spacy has good support for dependency parsing feature.,Similarity Technique Not Enough,0,Similarity Technique Not Enough,Decider,,C20230706015100095221,0.63,0.55,0.49,0.94,0.93,0.94,0.58,0.6,0.66,0.65,0.68,0.88,0.72,0.56,0.54,0.57,0.71,0.5,0.56,0.57,0.35,0.85,0.65,0.65, 79spacy support ,0.12,0.88,1,0
272,80,Spacy,Feature,Lemmatization,Lemmatization feature,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.

class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas",getting lemmas for every upos,The lemmatizer object is used to get the lemmas for every upos in the SpacyLemmatizer class.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015134444194,0.76,0.67,0.61,0.74,0.83,0.78,0.75,0.79,0.69,0.84,0.75,0.74,0.4,0.71,0.54,0.52,0.62,0.7,0.63,0.78,0.57,0.7,0.86,0.68, 80getting lemmas for every upos ,0.98,0.02,0,0
273,80,Spacy,Feature,Lemmatization,Lemmatization feature,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.

class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas",loading model with spacy.load,The SpacyLemmatizer object uses the spacy.load method to load the model and get the lemmatizer object.,,1,,,,C20230706015102389529,0.83,0.62,0.64,0.93,0.7,0.67,0.9,0.76,0.34,0.72,0.33,0.32,0.57,0.57,0.49,0.67,0.76,0.57,0.75,0.62,0.41,0.75,0.38,0.33, 80loading model with spacy.load ,0.18,0.82,1,1
274,80,Spacy,Feature,Lemmatization,Lemmatization feature,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.

class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas",modifying code to get correct upos,The code can be modified to pass in the correct upos for a specific word and receive the correct lemma.,,1,,,,C20230706015135009886,0.45,0.73,0.72,0.66,0.85,0.76,0.69,0.85,0.79,0.75,0.56,0.8,0.52,0.73,0.66,0.44,0.59,0.58,0.72,0.81,0.73,0.76,0.71,0.68, 80modifying code to get correct upos ,0.03,0.97,1,1
275,80,Spacy,Feature,Lemmatization,Lemmatization feature,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.

class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas",spacy supports lemmatization,SpaCy supports lemmatization as a stand-alone component.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706015101506603,0.77,0.57,0.74,0.78,0.8,0.77,0.65,0.51,0.74,0.53,0.82,0.61,0.52,0.55,0.56,0.87,0.7,0.68,0.58,0.5,0.53,0.68,0.83,0.61, 80spacy supports lemmatization ,0.15,0.85,1,1
276,81,Spacy,Feature,Lemmatization,Lemmatization feature,59636002,Spacy lemmatization of a single word,"I am trying to get the lemmatized version of a single word. Is there a way using ""spacy"" (fantastic python NLP library) to do this.
Below is the code I have tried but this does not work):

from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
lookups = Lookups()
lemmatizer = Lemmatizer(lookups)
word = ""ducks""
lemmas = lemmatizer.lookup(word)
print(lemmas)
The result I was hoping for was that the word ""ducks"" (plural) would result in ""duck"" (singular). Unfortunately, ""ducks"" (plural) is returned.

Is there a way of doing this?

NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.","Otherwise you can keep using spaCy, but after disabling parser and NER pipeline components:

Start by downloading a 12M small model (English multi-task CNN trained on OntoNotes)
$ python -m spacy download en_core_web_sm
Python code
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # just keep tagger for lemmatization
"" "".join([token.lemma_ for token in nlp('ducks')])
Output

> duck",disabling components for lemmatization,The parser and NER pipeline components of Spacy can be disabled when using the library for lemmatization.,,1,,,,C20230706015136936957,0.72,0.62,0.57,0.81,0.83,0.93,0.66,0.57,0.61,0.74,0.72,0.81,0.71,0.61,0.62,0.73,0.97,0.74,0.62,0.69,0.51,0.68,0.71,0.69, 81disabling components for lemmatization ,0.11,0.89,1,1
277,81,Spacy,Feature,Lemmatization,Lemmatization feature,59636002,Spacy lemmatization of a single word,"I am trying to get the lemmatized version of a single word. Is there a way using ""spacy"" (fantastic python NLP library) to do this.
Below is the code I have tried but this does not work):

from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
lookups = Lookups()
lemmatizer = Lemmatizer(lookups)
word = ""ducks""
lemmas = lemmatizer.lookup(word)
print(lemmas)
The result I was hoping for was that the word ""ducks"" (plural) would result in ""duck"" (singular). Unfortunately, ""ducks"" (plural) is returned.

Is there a way of doing this?

NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.","Otherwise you can keep using spaCy, but after disabling parser and NER pipeline components:

Start by downloading a 12M small model (English multi-task CNN trained on OntoNotes)
$ python -m spacy download en_core_web_sm
Python code
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # just keep tagger for lemmatization
"" "".join([token.lemma_ for token in nlp('ducks')])
Output

> duck",downloading pre-trained model,Spacy has a pre-trained model (English multi-task CNN trained on OntoNotes) that can be downloaded and used for performing lemmatization.,,1,,,,C20230706015137410511,0.83,0.69,0.81,0.77,0.8,0.78,0.5,0.62,0.58,0.47,0.67,0.62,0.77,0.57,0.67,0.63,0.6,0.61,0.62,0.56,0.73,0.75,0.73,0.53, 81downloading pre-trained model ,0.01,0.99,1,1
278,81,Spacy,Feature,Lemmatization,Lemmatization feature,59636002,Spacy lemmatization of a single word,"I am trying to get the lemmatized version of a single word. Is there a way using ""spacy"" (fantastic python NLP library) to do this.
Below is the code I have tried but this does not work):

from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
lookups = Lookups()
lemmatizer = Lemmatizer(lookups)
word = ""ducks""
lemmas = lemmatizer.lookup(word)
print(lemmas)
The result I was hoping for was that the word ""ducks"" (plural) would result in ""duck"" (singular). Unfortunately, ""ducks"" (plural) is returned.

Is there a way of doing this?

NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.","Otherwise you can keep using spaCy, but after disabling parser and NER pipeline components:

Start by downloading a 12M small model (English multi-task CNN trained on OntoNotes)
$ python -m spacy download en_core_web_sm
Python code
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # just keep tagger for lemmatization
"" "".join([token.lemma_ for token in nlp('ducks')])
Output

> duck",initial code did not work,The initial code did not provide the expected result when it was run.,,1,,,,C20230706015136464612,0.63,0.43,0.21,0.45,0.42,0.66,0.32,0.5,0.28,0.55,0.75,0.64,0.69,0.42,0.29,0.48,0.4,0.55,0.26,0.55,0.33,0.57,0.74,0.75, 81initial code did not work ,0.36,0.64,1,1
279,81,Spacy,Feature,Lemmatization,Lemmatization feature,59636002,Spacy lemmatization of a single word,"I am trying to get the lemmatized version of a single word. Is there a way using ""spacy"" (fantastic python NLP library) to do this.
Below is the code I have tried but this does not work):

from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
lookups = Lookups()
lemmatizer = Lemmatizer(lookups)
word = ""ducks""
lemmas = lemmatizer.lookup(word)
print(lemmas)
The result I was hoping for was that the word ""ducks"" (plural) would result in ""duck"" (singular). Unfortunately, ""ducks"" (plural) is returned.

Is there a way of doing this?

NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.","Otherwise you can keep using spaCy, but after disabling parser and NER pipeline components:

Start by downloading a 12M small model (English multi-task CNN trained on OntoNotes)
$ python -m spacy download en_core_web_sm
Python code
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # just keep tagger for lemmatization
"" "".join([token.lemma_ for token in nlp('ducks')])
Output

> duck",spacy supports lemmatization,Spacy has a lemmatizer that can be used to obtain the base form of a word.,,1,,,,C20230706015135624843,0.82,0.6,0.51,0.66,0.85,0.68,0.68,0.61,0.66,0.48,0.53,0.67,0.67,0.65,0.55,0.59,0.79,0.58,0.78,0.59,0.66,0.63,0.69,0.62, 81spacy supports lemmatization ,0.24,0.76,1,1
280,82,Spacy,Feature,Lemmatization,Lemmatization feature,74175424,"Is Spacy lemmatization not working properly or does it not lemmatize all words ending with ""-ing""?","When I run the spacy lemmatizer, it does not lemmatize the word ""consulting"" and therefore I suspect it is failing.

Here is my code:

nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
And my output:

['consulting']","The spaCy lemmatizer is not failing, it's performing as expected. Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words). For example, parts-of-speech.info which is based on the Stanford PoS tagger, does not allow you to enter single words.

In your case, the single word ""consulting"" is being tagged as a noun, and the spaCy model you are using deems ""consulting"" to be the appropriate lemma for this case. You'll see if you change your string instead to ""consulting tomorrow"", spaCy will lemmatize ""consulting"" to ""consult"" as it is tagged as a verb (see output from the code below). In short, I recommend not trying to perform lemmatization on single tokens, instead, use the model on sentences/documents as it was intended.",contextual dependence,"Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words).",,1,,,,C20230706015138036079,0.67,0.69,0.64,0.54,0.33,0.57,0.65,0.82,0.71,0.73,0.66,0.77,0.58,0.7,0.47,0.67,0.45,0.62,0.78,0.81,0.72,0.85,0.64,0.7, 82contextual dependence ,0.04,0.96,1,1
281,82,Spacy,Feature,Lemmatization,Lemmatization feature,74175424,"Is Spacy lemmatization not working properly or does it not lemmatize all words ending with ""-ing""?","When I run the spacy lemmatizer, it does not lemmatize the word ""consulting"" and therefore I suspect it is failing.

Here is my code:

nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
And my output:

['consulting']","The spaCy lemmatizer is not failing, it's performing as expected. Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words). For example, parts-of-speech.info which is based on the Stanford PoS tagger, does not allow you to enter single words.

In your case, the single word ""consulting"" is being tagged as a noun, and the spaCy model you are using deems ""consulting"" to be the appropriate lemma for this case. You'll see if you change your string instead to ""consulting tomorrow"", spaCy will lemmatize ""consulting"" to ""consult"" as it is tagged as a verb (see output from the code below). In short, I recommend not trying to perform lemmatization on single tokens, instead, use the model on sentences/documents as it was intended.",proper lemmatization,"In the given code snippet, the lemmatizer correctly identifies 'consulting' as a noun and therefore lemmatizes it to 'consulting'.",Diversified question and answers,1,Convoluted Challenge,Enquirer,,C20230706015138586906,0.7,0.63,0.67,0.54,0.6,0.8,0.5,0.84,0.94,0.49,0.53,0.87,0.43,0.71,0.53,0.42,0.39,0.78,0.59,0.75,0.66,0.82,0.68,0.72, 82proper lemmatization ,0.49,0.51,0,0
282,82,Spacy,Feature,Lemmatization,Lemmatization feature,74175424,"Is Spacy lemmatization not working properly or does it not lemmatize all words ending with ""-ing""?","When I run the spacy lemmatizer, it does not lemmatize the word ""consulting"" and therefore I suspect it is failing.

Here is my code:

nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
And my output:

['consulting']","The spaCy lemmatizer is not failing, it's performing as expected. Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words). For example, parts-of-speech.info which is based on the Stanford PoS tagger, does not allow you to enter single words.

In your case, the single word ""consulting"" is being tagged as a noun, and the spaCy model you are using deems ""consulting"" to be the appropriate lemma for this case. You'll see if you change your string instead to ""consulting tomorrow"", spaCy will lemmatize ""consulting"" to ""consult"" as it is tagged as a verb (see output from the code below). In short, I recommend not trying to perform lemmatization on single tokens, instead, use the model on sentences/documents as it was intended.",recommendations,It is recommended to use the model on sentences/documents as it was intended to improve the accuracy of the lemmatization feature.,,1,,,,C20230706015139949583,0.6,0.68,0.79,0.74,0.66,0.82,0.77,0.81,0.72,0.74,0.71,0.83,0.77,0.68,0.81,0.69,0.83,0.74,0.66,0.65,0.7,0.58,0.66,0.73, 82recommendations ,0.01,0.99,1,1
283,82,Spacy,Feature,Lemmatization,Lemmatization feature,74175424,"Is Spacy lemmatization not working properly or does it not lemmatize all words ending with ""-ing""?","When I run the spacy lemmatizer, it does not lemmatize the word ""consulting"" and therefore I suspect it is failing.

Here is my code:

nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
And my output:

['consulting']","The spaCy lemmatizer is not failing, it's performing as expected. Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words). For example, parts-of-speech.info which is based on the Stanford PoS tagger, does not allow you to enter single words.

In your case, the single word ""consulting"" is being tagged as a noun, and the spaCy model you are using deems ""consulting"" to be the appropriate lemma for this case. You'll see if you change your string instead to ""consulting tomorrow"", spaCy will lemmatize ""consulting"" to ""consult"" as it is tagged as a verb (see output from the code below). In short, I recommend not trying to perform lemmatization on single tokens, instead, use the model on sentences/documents as it was intended.",single token issues,Performing lemmatization on single tokens (words) without considering the context can be unreliable and lead to incorrect lemmatization results.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015139424764,0.82,0.59,0.65,0.72,0.83,0.8,0.92,0.78,0.69,0.8,0.75,0.91,0.67,0.58,0.53,0.86,0.63,0.61,0.67,0.61,0.75,0.94,0.89,0.82, 82single token issues ,0.52,0.48,0,0
284,83,Spacy,Feature,Lemmatization,Lemmatization feature,63693463,SpaCy lemmatizer removes capitalization,"I would like to lemmatize some textual data in Hungarian language and encountered a strange feature in spaCy. The token.lemma_ function works well in terms of lemmatization, however, it returns some of the sentences without first letter capitalization. This is quite annoying, as my next function, unnest_stences (R) requires first capital letters in order to identify and break the text down into individual sentences. 

When I pass my original data to the function, it returns some sentences with upercase first letters, others with lowercase letters. For some strange reason I couldn't reproduce that pattern above, but I guess the main point is visible. The function does not work as expected.

Any ideas how I could fix this?

I'm using Jupyter Notebook, Python 2.7, Win 7 and a Toshiba laptop (Port‚Äö√†√∂¬¨¬©g‚Äö√†√∂¬¨¬© Z830-10R i3-2367M).","Lowercasing is the expected behavior of spaCy's lemmatizer for non-proper-noun tokens.

One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).

import spacy

nlp = spacy.load('en_core_web_sm')

text = 'This is a test sentence.'
doc = nlp(text)
newtext = ' '.join([tok.lemma_.title() if tok.is_title else tok.lemma_ for tok in doc])
print(newtext)
# This be a test sentence .",lowercasing behavior,Spacy's lemmatizer produces lowercased output by default for non-proper-noun tokens.,,1,,,,C20230706015140468990,0.88,0.77,0.72,0.76,0.77,0.84,0.61,0.75,0.6,0.64,0.54,0.79,0.75,0.65,0.61,0.65,0.75,0.59,0.72,0.74,0.61,0.88,0.73,0.77, 83lowercasing behavior ,0.05,0.95,1,1
285,83,Spacy,Feature,Lemmatization,Lemmatization feature,63693463,SpaCy lemmatizer removes capitalization,"I would like to lemmatize some textual data in Hungarian language and encountered a strange feature in spaCy. The token.lemma_ function works well in terms of lemmatization, however, it returns some of the sentences without first letter capitalization. This is quite annoying, as my next function, unnest_stences (R) requires first capital letters in order to identify and break the text down into individual sentences. 

When I pass my original data to the function, it returns some sentences with upercase first letters, others with lowercase letters. For some strange reason I couldn't reproduce that pattern above, but I guess the main point is visible. The function does not work as expected.

Any ideas how I could fix this?

I'm using Jupyter Notebook, Python 2.7, Win 7 and a Toshiba laptop (Port‚Äö√†√∂¬¨¬©g‚Äö√†√∂¬¨¬© Z830-10R i3-2367M).","Lowercasing is the expected behavior of spaCy's lemmatizer for non-proper-noun tokens.

One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).

import spacy

nlp = spacy.load('en_core_web_sm')

text = 'This is a test sentence.'
doc = nlp(text)
newtext = ' '.join([tok.lemma_.title() if tok.is_title else tok.lemma_ for tok in doc])
print(newtext)
# This be a test sentence .",workaround for capitalization,"One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).",,1,,,,C20230706015140995767,0.46,0.73,0.69,0.71,0.86,0.75,0.37,0.87,0.69,0.47,0.59,0.81,0.7,0.71,0.81,0.72,0.92,0.7,0.54,0.78,0.9,0.66,0.63,0.76, 83workaround for capitalization ,0.04,0.96,1,1
286,84,Spacy,Feature,NER,NER feature,57536896,Custom entity ruler with SpaCy did not return a match,"This link shows how to create custom entity ruler.

I basically copied and modified the code for another custom entity ruler and used it to find a match in a doc as follows:
However, when I run the code (in Jupyter notebook), nothing returned.

Could you please tell me:

If the code returned nothing, did it mean no match was found?
Why couldn't my code find a match although it's almost identical to the original (except for the patterns added to the ruler)? What did I do wrong?","The problem is an interaction between the NER component provided in the English model and your EntityRuler component. The NER component finds 2 as a number (CARDINAL) and there's a restriction that entities aren't allowed to overlap, so the EntityRuler component doesn't find any matches.

You can either add your EntityRuler before the NER component:

nlp.add_pipe(ruler, before='ner')
Or tell the EntityRuler that it's allowed to overwrite existing entities:

ruler = EntityRuler(nlp, overwrite_ents=True)",issue with overlapping entities,"The NER component found an entity that overlapped with the entities defined in the custom entity ruler, resulting in no matches being found.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015141525231,0.73,0.56,0.6,0.58,0.62,0.7,0.67,0.79,0.74,0.76,0.85,0.79,0.64,0.38,0.66,0.46,0.52,0.49,0.68,0.61,0.7,0.69,0.85,0.63, 84issue with overlapping entities ,0.61,0.39,0,0
287,84,Spacy,Feature,NER,NER feature,57536896,Custom entity ruler with SpaCy did not return a match,"This link shows how to create custom entity ruler.

I basically copied and modified the code for another custom entity ruler and used it to find a match in a doc as follows:
However, when I run the code (in Jupyter notebook), nothing returned.

Could you please tell me:

If the code returned nothing, did it mean no match was found?
Why couldn't my code find a match although it's almost identical to the original (except for the patterns added to the ruler)? What did I do wrong?","The problem is an interaction between the NER component provided in the English model and your EntityRuler component. The NER component finds 2 as a number (CARDINAL) and there's a restriction that entities aren't allowed to overlap, so the EntityRuler component doesn't find any matches.

You can either add your EntityRuler before the NER component:

nlp.add_pipe(ruler, before='ner')
Or tell the EntityRuler that it's allowed to overwrite existing entities:

ruler = EntityRuler(nlp, overwrite_ents=True)",solution: change pipeline order,Adjusting the pipeline order by adding the EntityRuler before the NER component can prevent overlapping entities from causing issues.,,1,,,,C20230706015142743653,0.69,0.65,0.63,0.96,0.74,0.72,0.75,0.68,0.88,0.87,0.78,0.74,0.69,0.72,0.63,0.76,0.61,0.83,0.76,0.81,0.85,0.85,0.84,0.87, 84solution: change pipeline order ,0.14,0.86,1,1
288,84,Spacy,Feature,NER,NER feature,57536896,Custom entity ruler with SpaCy did not return a match,"This link shows how to create custom entity ruler.

I basically copied and modified the code for another custom entity ruler and used it to find a match in a doc as follows:
However, when I run the code (in Jupyter notebook), nothing returned.

Could you please tell me:

If the code returned nothing, did it mean no match was found?
Why couldn't my code find a match although it's almost identical to the original (except for the patterns added to the ruler)? What did I do wrong?","The problem is an interaction between the NER component provided in the English model and your EntityRuler component. The NER component finds 2 as a number (CARDINAL) and there's a restriction that entities aren't allowed to overlap, so the EntityRuler component doesn't find any matches.

You can either add your EntityRuler before the NER component:

nlp.add_pipe(ruler, before='ner')
Or tell the EntityRuler that it's allowed to overwrite existing entities:

ruler = EntityRuler(nlp, overwrite_ents=True)",solution: overwrite entities,Allowing the EntityRuler to overwrite existing entities provides another solution to the issue of overlapping entities.,,1,,,,C20230706015143429047,0.69,0.71,0.63,0.94,0.97,0.94,0.8,0.79,0.73,0.88,0.75,0.84,0.73,0.75,0.73,0.68,0.69,0.66,0.8,0.89,0.8,0.78,0.85,0.83, 84solution: overwrite entities ,0.01,0.99,1,1
289,85,Spacy,Feature,NER,NER feature,65220447,Add new named entity to Spacy's en_core_web_sm model?,"I'm following the example here on training a new entity type:

https://spacy.io/usage/training#example-new-entity-type

It works fine when I don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.

 python.exe train-new-entity-type.py
It also works fine when I pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.

 python.exe train-new-entity-type.py -m dir/my_model
However, I want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so I tried:

 python.exe train-new-entity-type.py -m en_core_web_sm
However, this didn't seem to work. Spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.

Am I doing something wrong? Is this possible (adding named entities to en_core_web_sm)?","Read about the ""catastrophic forgetting"" problem when updating an existing model: https://spacy.io/usage/training#ner

It can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the NER component to the en_core_web_sm pipeline with a custom name. The main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:

import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")
Where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",catastrophic forgetting problem,Updating an existing model can be difficult due to the 'catastrophic forgetting' problem.,ChatGPT giving out of context response,0,Generic Error,Mixed,,C20230706015143914143,0.44,0.71,0.35,0.74,0.64,0.78,0.35,0.8,0.53,0.57,0.84,0.67,0.33,0.56,0.52,0.46,0.44,0.57,0.38,0.58,0.47,0.82,0.9,0.8, 85catastrophic forgetting problem ,0.66,0.34,0,1
290,85,Spacy,Feature,NER,NER feature,65220447,Add new named entity to Spacy's en_core_web_sm model?,"I'm following the example here on training a new entity type:

https://spacy.io/usage/training#example-new-entity-type

It works fine when I don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.

 python.exe train-new-entity-type.py
It also works fine when I pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.

 python.exe train-new-entity-type.py -m dir/my_model
However, I want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so I tried:

 python.exe train-new-entity-type.py -m en_core_web_sm
However, this didn't seem to work. Spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.

Am I doing something wrong? Is this possible (adding named entities to en_core_web_sm)?","Read about the ""catastrophic forgetting"" problem when updating an existing model: https://spacy.io/usage/training#ner

It can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the NER component to the en_core_web_sm pipeline with a custom name. The main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:

import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")
Where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",ner customization possible,"Spacy's NER feature is flexible and can be customized to recognize new named entities, as long as the models are compatible and loaded properly.",,1,,,,C20230706015216643389,0.78,0.71,0.71,0.67,0.64,0.9,0.72,0.81,0.71,0.79,0.79,0.93,0.62,0.69,0.61,0.84,0.64,0.68,0.65,0.75,0.74,0.89,0.71,0.83, 85ner customization possible ,0.04,0.96,1,1
291,85,Spacy,Feature,NER,NER feature,65220447,Add new named entity to Spacy's en_core_web_sm model?,"I'm following the example here on training a new entity type:

https://spacy.io/usage/training#example-new-entity-type

It works fine when I don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.

 python.exe train-new-entity-type.py
It also works fine when I pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.

 python.exe train-new-entity-type.py -m dir/my_model
However, I want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so I tried:

 python.exe train-new-entity-type.py -m en_core_web_sm
However, this didn't seem to work. Spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.

Am I doing something wrong? Is this possible (adding named entities to en_core_web_sm)?","Read about the ""catastrophic forgetting"" problem when updating an existing model: https://spacy.io/usage/training#ner

It can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the NER component to the en_core_web_sm pipeline with a custom name. The main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:

import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")
Where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",same vocab required,Ensure that the models are loaded with the same vocab to avoid any problems with the string store.,,1,,,,C20230706015216152367,0.67,0.55,0.79,0.54,0.57,0.62,0.69,0.77,0.8,0.76,0.77,0.68,0.66,0.56,0.72,0.59,0.46,0.42,0.52,0.78,0.66,0.69,0.73,0.74, 85same vocab required ,0.03,0.97,1,1
292,85,Spacy,Feature,NER,NER feature,65220447,Add new named entity to Spacy's en_core_web_sm model?,"I'm following the example here on training a new entity type:

https://spacy.io/usage/training#example-new-entity-type

It works fine when I don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.

 python.exe train-new-entity-type.py
It also works fine when I pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.

 python.exe train-new-entity-type.py -m dir/my_model
However, I want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so I tried:

 python.exe train-new-entity-type.py -m en_core_web_sm
However, this didn't seem to work. Spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.

Am I doing something wrong? Is this possible (adding named entities to en_core_web_sm)?","Read about the ""catastrophic forgetting"" problem when updating an existing model: https://spacy.io/usage/training#ner

It can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the NER component to the en_core_web_sm pipeline with a custom name. The main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:

import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")
Where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",separate model recommended,It might be easier to train a separate model for a new entity type and add the NER component to the en_core_web_sm pipeline.,,1,,,,C20230706015144437869,0.77,0.66,0.68,0.65,0.69,0.82,0.8,0.75,0.6,0.87,0.7,0.76,0.73,0.59,0.58,0.56,0.63,0.7,0.75,0.7,0.68,0.88,0.83,0.86, 85separate model recommended ,0.23,0.77,1,1
293,86,Spacy,Feature,NER,NER feature,68682465,pattern match issue with SpaCy,"I have the following script using a pattern to match the postcode. However, depending on how I supply the postcode string seems to affect how it is matched.

It would appear that for some reason the first postcode is never matched. In the last example both ""RH5 0XE"" & ""PO01 0IO"" should match, as you can see ""RH5 0XE"" matched in the previous run.

Any ideas on how I can improve the match rate?","Put the entity_ruler before ner in the pipeline so that its matches have priority over the CARDINAL spans from ner, or alternatively you can set it to overwrite overlapping entities with the overwrite_ents setting.",entity ruler feature,"SpaCy provides an entity ruler that can be used before NER in the pipeline, giving matches priority over CARDINAL spans from ner.",,1,,,,C20230706015217173223,0.7,0.67,0.59,0.46,0.62,0.58,0.75,0.55,0.63,0.79,0.78,0.76,0.62,0.71,0.63,0.57,0.63,0.83,0.74,0.4,0.57,0.55,0.72,0.59, 86entity ruler feature ,0.05,0.95,1,1
294,86,Spacy,Feature,NER,NER feature,68682465,pattern match issue with SpaCy,"I have the following script using a pattern to match the postcode. However, depending on how I supply the postcode string seems to affect how it is matched.

It would appear that for some reason the first postcode is never matched. In the last example both ""RH5 0XE"" & ""PO01 0IO"" should match, as you can see ""RH5 0XE"" matched in the previous run.

Any ideas on how I can improve the match rate?","Put the entity_ruler before ner in the pipeline so that its matches have priority over the CARDINAL spans from ner, or alternatively you can set it to overwrite overlapping entities with the overwrite_ents setting.",higher match rate,"These features of Spacy improve the match rate of NER pattern matching, making it an effective tool for text analysis and entity recognition.",Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706015218223542,0.66,0.7,0.56,0.75,0.65,0.46,0.71,0.81,0.52,0.8,0.5,0.53,0.68,0.58,0.71,0.63,0.51,0.55,0.61,0.57,0.33,0.52,0.31,0.48, 86higher match rate ,0.11,0.89,1,0
295,86,Spacy,Feature,NER,NER feature,68682465,pattern match issue with SpaCy,"I have the following script using a pattern to match the postcode. However, depending on how I supply the postcode string seems to affect how it is matched.

It would appear that for some reason the first postcode is never matched. In the last example both ""RH5 0XE"" & ""PO01 0IO"" should match, as you can see ""RH5 0XE"" matched in the previous run.

Any ideas on how I can improve the match rate?","Put the entity_ruler before ner in the pipeline so that its matches have priority over the CARDINAL spans from ner, or alternatively you can set it to overwrite overlapping entities with the overwrite_ents setting.",overwrite overlapping entities,Spacy also provides an option to overwrite overlapping entities with the overwrite_ents setting.,,1,,,,C20230706015217693697,0.69,0.62,0.59,0.62,0.64,0.65,0.64,0.4,0.63,0.59,0.66,0.71,0.81,0.53,0.29,0.68,0.43,0.47,0.64,0.64,0.52,0.64,0.57,0.64, 86overwrite overlapping entities ,0.38,0.62,1,1
296,87,Spacy,Feature,NER,NER feature,46934523,How to get spaCy NER probability,"I want to combine spaCy's NER engine with a separate NER engine (a BoW model). I'm currently comparing outputs from the two engines, trying to figure out what the optimal combination of the two would be. Both perform decently, but quite often spaCy finds entities that the BoW engine misses, and vice versa. What I would like is to access a probability score (or something similar) from spaCy whenever it finds an entity that is not found by the BoW engine. Can I get spaCy to print out its own probability score for a given entity it has found? As in, ""Hi, I'm spaCy. I've found this token (or combination of tokens) that I'm X% certain is an entity of type BLAH."" I want to know that number X every time spaCy finds an entity. I imagine there must be such a number somewhere internally in spaCy's NER engine, plus a threshold value below which the possible entity is not flagged as an entity, and I'd like to know how to get my hands on that number. Thanks in advance.","Actually, there is an issue for that.

The author of the library, suggests there (among others) the following solution:

Beam search with global objective. This is the standard solution: use a global objective, so that the parser model is trained to prefer parses that are better overall. Keep N different candidates, and output the best one. This can be used to support confidence by looking at the alternate analyses in the beam. If an entity occurs in every analysis, the NER is more confident it's correct.

mportant note: The outputs you will get here are probably different from the outputs you would get using the Standard NER and not the beam search alternative. However, the beam search alternative provides you a metric of confidence that as I understand from your question is useful for your case.
",beam search with global objective supported,"The author suggests using beam search with global objective to support confidence, which can provide useful metrics. This shows that Spacy has provisions for providing confidence scores for identifying entities and is a capable library for NER tasks.",,1,,,,C20230706015218739626,0.7,0.71,0.77,0.84,0.75,0.83,0.6,0.69,0.72,0.85,0.6,0.67,0.71,0.72,0.62,0.88,0.48,0.54,0.7,0.7,0.65,0.93,0.86,0.88, 87beam search with global objective supported ,0.1,0.9,1,1
297,88,Spacy,Feature,Entity linking,Entity linking feature,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?

import spacy
nlp = spacy.load(r""D:\el model\nlp"")
doc = nlp(""Amir Khan is a great boxer"")
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) ","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",file for entity descriptions,Spacy generates an intermediary result during processing which includes a file entity_descriptions.csv. This file maps the WikiData ID to its corresponding description in a simple tabular format.,,1,,,,C20230706015220316530,0.67,0.61,0.71,0.64,0.59,0.69,0.47,0.79,0.76,0.67,0.62,0.73,0.62,0.59,0.63,0.6,0.53,0.54,0.61,0.87,0.73,0.65,0.46,0.73, 88file for entity descriptions ,0.26,0.74,1,1
298,88,Spacy,Feature,Entity linking,Entity linking feature,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?

import spacy
nlp = spacy.load(r""D:\el model\nlp"")
doc = nlp(""Amir Khan is a great boxer"")
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) ","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",kb doesn't store descriptions,The descriptions are not stored in the KB itself because of performance reasons.,,1,,,,C20230706015219786661,0.81,0.5,0.64,0.53,0.57,0.65,0.59,0.69,0.75,0.64,0.55,0.56,0.74,0.64,0.5,0.53,0.52,0.61,0.64,0.65,0.66,0.62,0.57,0.88, 88kb doesn't store descriptions ,0.31,0.69,1,1
299,88,Spacy,Feature,Entity linking,Entity linking feature,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?

import spacy
nlp = spacy.load(r""D:\el model\nlp"")
doc = nlp(""Amir Khan is a great boxer"")
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) ","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",solution for entity descriptions,The user can make use of the entity_descriptions.csv to display entity descriptions from the KB.,,1,,,,C20230706015220864942,0.68,0.79,0.7,0.77,0.79,0.87,0.7,0.7,0.61,0.85,0.69,0.82,0.72,0.7,0.69,0.66,0.79,0.76,0.62,0.73,0.71,0.7,0.85,0.86, 88solution for entity descriptions ,0,1,1,1
300,88,Spacy,Feature,Entity linking,Entity linking feature,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?

import spacy
nlp = spacy.load(r""D:\el model\nlp"")
doc = nlp(""Amir Khan is a great boxer"")
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) ","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",spacy supports entity linking,The user has successfully trained a Spacy entity linking model and is now looking for ways to display the description of an entity from the knowledge base (KB) as output.,,1,,,,C20230706015219267186,0.61,0.67,0.74,0.26,0.7,0.31,0.6,0.85,0.57,0.61,0.65,0.57,0.71,0.64,0.36,0.37,0.56,0.31,0.56,0.71,0.67,0.48,0.44,0.66, 88spacy supports entity linking ,0.02,0.98,1,1
301,89,Spacy,Feature,Entity linking,Entity linking feature,59050554,Error running Spacy entity linking example,"I was trying the entity linking example in spacy.
Using this example to train the entity linker and generating the knowledge base for the same with this example.

I can create a knowledge base with the available en_core_web_md, this is the output for the same.
When I try to train the entity linker with the knowledge base from above, I get this error.
File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
I did follow the instructions specified here. I used the en_core_web_md to create the knowledge base since I do not have a pre-trained model.

I did not write any custom code just trying to run this example, Can someone point me to the right direction.
","This was asked and answered in the following issue on spaCy's GitHub.

It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline.

The new script adds such an EntityRuler to the pipeline as an example. I.e.,

# Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, this can be replaced with your own statistical NER model.",customization may be needed,"To use entity linking in Spacy, users may need to customize the pipeline according to their specific needs.",,1,,,,C20230706015223129651,0.7,0.73,0.59,0.82,0.43,0.46,0.72,0.7,0.71,0.84,0.74,0.8,0.75,0.74,0.55,0.95,0.48,0.47,0.63,0.64,0.55,0.81,0.61,0.64, 89customization may be needed ,0.13,0.87,1,1
302,89,Spacy,Feature,Entity linking,Entity linking feature,59050554,Error running Spacy entity linking example,"I was trying the entity linking example in spacy.
Using this example to train the entity linker and generating the knowledge base for the same with this example.

I can create a knowledge base with the available en_core_web_md, this is the output for the same.
When I try to train the entity linker with the knowledge base from above, I get this error.
File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
I did follow the instructions specified here. I used the en_core_web_md to create the knowledge base since I do not have a pre-trained model.

I did not write any custom code just trying to run this example, Can someone point me to the right direction.
","This was asked and answered in the following issue on spaCy's GitHub.

It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline.

The new script adds such an EntityRuler to the pipeline as an example. I.e.,

# Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, this can be replaced with your own statistical NER model.",error caused by pipeline refactor,The error encountered by the user was due to a recent refactor of the entity linking pipeline in Spacy. The new pipeline expects a statistical or rule-based NER component.,,1,,,,C20230706015221955271,0.62,0.71,0.47,0.67,0.46,0.64,0.58,0.82,0.8,0.62,0.54,0.82,0.62,0.52,0.5,0.54,0.41,0.45,0.67,0.81,0.67,0.67,0.65,0.88, 89error caused by pipeline refactor ,0.23,0.77,1,1
303,89,Spacy,Feature,Entity linking,Entity linking feature,59050554,Error running Spacy entity linking example,"I was trying the entity linking example in spacy.
Using this example to train the entity linker and generating the knowledge base for the same with this example.

I can create a knowledge base with the available en_core_web_md, this is the output for the same.
When I try to train the entity linker with the knowledge base from above, I get this error.
File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
I did follow the instructions specified here. I used the en_core_web_md to create the knowledge base since I do not have a pre-trained model.

I did not write any custom code just trying to run this example, Can someone point me to the right direction.
","This was asked and answered in the following issue on spaCy's GitHub.

It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline.

The new script adds such an EntityRuler to the pipeline as an example. I.e.,

# Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, this can be replaced with your own statistical NER model.",example demonstrates adding ner component,The example provided in the answer demonstrates how to add a custom NER component (EntityRuler) to the pipeline. This can be replaced with a user's own statistical NER model for a more realistic application.,No answer available against the challenge,1,Out of Context Challenge,Challenger,,C20230706015222412999,0.71,0.8,0.67,0.5,0.47,0.56,0.72,0.83,0.83,0.8,0.68,0.81,0.62,0.59,0.71,0.56,0.47,0.45,0.74,0.82,0.73,0.83,0.69,0.69, 89example demonstrates adding ner component ,0.01,0.99,1,1
304,89,Spacy,Feature,Entity linking,Entity linking feature,59050554,Error running Spacy entity linking example,"I was trying the entity linking example in spacy.
Using this example to train the entity linker and generating the knowledge base for the same with this example.

I can create a knowledge base with the available en_core_web_md, this is the output for the same.
When I try to train the entity linker with the knowledge base from above, I get this error.
File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
I did follow the instructions specified here. I used the en_core_web_md to create the knowledge base since I do not have a pre-trained model.

I did not write any custom code just trying to run this example, Can someone point me to the right direction.
","This was asked and answered in the following issue on spaCy's GitHub.

It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline.

The new script adds such an EntityRuler to the pipeline as an example. I.e.,

# Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, this can be replaced with your own statistical NER model.",spacy supports entity linking feature,The conversation indicates that Spacy supports entity linking feature. The user encountered an issue with an example in the documentation.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015221368593,0.58,0.53,0.34,0.79,0.44,0.39,0.7,0.77,0.74,0.8,0.79,0.78,0.5,0.31,0.52,0.55,0.29,0.39,0.47,0.51,0.38,0.48,0.78,0.45, 89spacy supports entity linking feature ,0.96,0.04,0,0
305,90,Spacy,Feature,Entity linking,Entity linking feature,60922171,Spacy - entity linker - why is the predict score a combination of prob and cosine sim?,"I was going through the predict method for the entity linker pipe under spacy, and for some reason the score is defined as the following :

scores = prior_probs + sims - (prior_probs*sims)

Link here

Anybody has experience with this / knows where this formula comes from?

Thanks!","It is taken from Entity Linking via Joint Encoding of Types, Descriptions, and Context section 4 equation 2.

I don't feel confident enough though in explaining the formula in detail, on overall the purpose is to combine probability scores for entitiy candidates derived from external knowledge based resources (KB in the paper), which are the prior probabilities, and scores estimated with a sentence encoder, used to encode the mention to link along with its context, sims in the formula because they compute cosine similarity between the encoded mention vector and all entity candidates (which is why this formula is used only if ""incl_context"" is true).",context score,"Spacy includes context scores for entity linking by computing cosine similarity between the mention vector and entity candidates, which is why the formula is used only if 'incl_context' is true.",,1,,,,C20230706015225592152,0.8,0.69,0.78,0.51,0.8,0.66,0.83,0.83,0.75,0.77,0.75,0.77,0.7,0.61,0.83,0.59,0.61,0.51,0.73,0.81,0.71,0.74,0.62,0.71, 90context score ,0.04,0.96,1,1
306,90,Spacy,Feature,Entity linking,Entity linking feature,60922171,Spacy - entity linker - why is the predict score a combination of prob and cosine sim?,"I was going through the predict method for the entity linker pipe under spacy, and for some reason the score is defined as the following :

scores = prior_probs + sims - (prior_probs*sims)

Link here

Anybody has experience with this / knows where this formula comes from?

Thanks!","It is taken from Entity Linking via Joint Encoding of Types, Descriptions, and Context section 4 equation 2.

I don't feel confident enough though in explaining the formula in detail, on overall the purpose is to combine probability scores for entitiy candidates derived from external knowledge based resources (KB in the paper), which are the prior probabilities, and scores estimated with a sentence encoder, used to encode the mention to link along with its context, sims in the formula because they compute cosine similarity between the encoded mention vector and all entity candidates (which is why this formula is used only if ""incl_context"" is true).",formula source,"The entity linker's formula is taken from the paper 'Entity Linking via Joint Encoding of Types, Descriptions, and Context,' section 4 equation 2.",,1,,,,C20230706015224020667,0.72,0.72,0.78,0.26,0.86,0.24,0.33,0.69,0.36,0.55,0.77,0.46,0.66,0.41,0.52,0.45,0.6,0.4,0.5,0.3,0.81,0.59,0.68,0.46, 90formula source ,0.16,0.84,1,1
307,90,Spacy,Feature,Entity linking,Entity linking feature,60922171,Spacy - entity linker - why is the predict score a combination of prob and cosine sim?,"I was going through the predict method for the entity linker pipe under spacy, and for some reason the score is defined as the following :

scores = prior_probs + sims - (prior_probs*sims)

Link here

Anybody has experience with this / knows where this formula comes from?

Thanks!","It is taken from Entity Linking via Joint Encoding of Types, Descriptions, and Context section 4 equation 2.

I don't feel confident enough though in explaining the formula in detail, on overall the purpose is to combine probability scores for entitiy candidates derived from external knowledge based resources (KB in the paper), which are the prior probabilities, and scores estimated with a sentence encoder, used to encode the mention to link along with its context, sims in the formula because they compute cosine similarity between the encoded mention vector and all entity candidates (which is why this formula is used only if ""incl_context"" is true).",probability score,"The formula combines scores derived from external knowledge based resources, which are the prior probabilities.",,1,,,,C20230706015225137093,0.66,0.65,0.74,0.73,0.52,0.48,0.71,0.73,0.55,0.86,0.78,0.8,0.56,0.53,0.89,0.6,0.61,0.54,0.74,0.68,0.61,0.67,0.76,0.71, 90probability score ,0,1,1,1
308,91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",entity recognizer adjustment,The exact entities stored in the KB and used for training depend on the adjustments made to the entity recognizer component.,Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015227657025,0.81,0.8,0.86,0.45,0.54,0.87,0.62,0.79,0.85,0.82,0.77,0.87,0.56,0.7,0.66,0.32,0.4,0.64,0.44,0.78,0.66,0.58,0.68,0.69, 91entity recognizer adjustment ,0.46,0.54,0,0
309,91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",kb and entity linker,Creating a Knowledge Base and Entity Linker from Wikipedia/Wikidata can potentially link other entities as well.,Out of Scope Challenge,0,Out of Scope Challenge,Challenger,,C20230706015227142689,0.81,0.8,0.61,0.82,0.72,0.78,0.76,0.82,0.74,0.87,0.67,0.74,0.73,0.56,0.46,0.55,0.56,0.49,0.73,0.72,0.51,0.87,0.7,0.8, 91kb and entity linker ,0.15,0.85,1,0
310,91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",named entity support,spaCy supports entity linking feature for named entities.,,1,,,,C20230706015226084336,0.56,0.53,0.46,0.69,0.69,0.85,0.76,0.53,0.53,0.55,0.66,0.66,0.47,0.5,0.42,0.38,0.49,0.55,0.5,0.43,0.45,0.89,0.85,0.81, 91named entity support ,0.15,0.85,1,1
311,91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",suitability of spacy,"spaCy can be a suitable tool for the task if the entity recognizer component is properly trained, although the performance of the NER model on non-named entities may not be perfect.",,1,,,,C20230706015228182220,0.73,0.63,0.84,0.63,0.59,0.45,0.81,0.64,0.5,0.74,0.43,0.3,0.67,0.63,0.54,0.68,0.54,0.53,0.79,0.65,0.7,0.73,0.75,0.7, 91suitability of spacy ,0.04,0.96,1,1
312,91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",training needed,An NER model needs to be trained to identify other important entities.,,0,,,,C20230706015226609389,0.75,0.74,0.63,0.92,0.81,0.85,0.77,0.81,0.69,0.85,0.79,0.78,0.59,0.39,0.6,0.35,0.66,0.37,0.54,0.61,0.51,0.74,0.76,0.69, 91training needed ,0.44,0.56,0,1
313,92,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,56416641,separate texts into sentences NLTK vs spaCy,"I want to separate texts into sentences.

looking in stack overflow I found:

WITH NLTK

from nltk.tokenize import sent_tokenize
text=""""""Hello Mr. Smith, how are you doing today? The weathe is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard""""""
tokenized_text=sent_tokenize(text)
print(tokenized_text)
WITH SPACY

from spacy.lang.en import English # updated

raw_text = 'Hello, world. Here are two sentences.'
nlp = English()
nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated
doc = nlp(raw_text)
sentences = [sent.string.strip() for sent in doc.sents]
The question is what in the background for spacy having to do it differently with a so called create_pipe. Sentences are important for training your own word embedings for NLP. There should be a reason why spaCy does not include directly out of the box a sentence tokenizer.

Thanks.

NOTE: Be aware that a simply .split(.) does not work, there are several decimal numbers in the text and other kind of tokens containing '.'","By default, spaCy uses its dependency parser to do sentence segmentation, which requires loading a statistical model. The sentencizer is a rule-based sentence segmenter that you can use to define your own sentence segmentation rules without loading a model.

If you don't mind leaving the parser activated, you can use the following code:

import spacy
nlp = spacy.load('en_core_web_sm') # or whatever model you have installed
raw_text = 'Hello, world. Here are two sentences.'
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]",advantages of parser,The dependency parser is more accurate and can handle complex sentences.,No answer available against the challenge,0,Out of Context Challenge,Challenger,,C20230706015229224024,0.55,0.79,0.53,0.7,0.85,0.68,0.54,0.78,0.67,0.73,0.71,0.74,0.47,0.56,0.52,0.67,0.66,0.57,0.39,0.47,0.4,0.6,0.56,0.75, 92advantages of parser ,0.85,0.15,0,1
314,92,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,56416641,separate texts into sentences NLTK vs spaCy,"I want to separate texts into sentences.

looking in stack overflow I found:

WITH NLTK

from nltk.tokenize import sent_tokenize
text=""""""Hello Mr. Smith, how are you doing today? The weathe is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard""""""
tokenized_text=sent_tokenize(text)
print(tokenized_text)
WITH SPACY

from spacy.lang.en import English # updated

raw_text = 'Hello, world. Here are two sentences.'
nlp = English()
nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated
doc = nlp(raw_text)
sentences = [sent.string.strip() for sent in doc.sents]
The question is what in the background for spacy having to do it differently with a so called create_pipe. Sentences are important for training your own word embedings for NLP. There should be a reason why spaCy does not include directly out of the box a sentence tokenizer.

Thanks.

NOTE: Be aware that a simply .split(.) does not work, there are several decimal numbers in the text and other kind of tokens containing '.'","By default, spaCy uses its dependency parser to do sentence segmentation, which requires loading a statistical model. The sentencizer is a rule-based sentence segmenter that you can use to define your own sentence segmentation rules without loading a model.

If you don't mind leaving the parser activated, you can use the following code:

import spacy
nlp = spacy.load('en_core_web_sm') # or whatever model you have installed
raw_text = 'Hello, world. Here are two sentences.'
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]",reasons for lack of direct tokenizer,Spacy is designed for more advanced NLP tasks and assumed users could create a tokenizer using sentencizer or external tools like NLTK.,Limitation of similarity calculation (large text),0,Similary Calculation Issue ,Decider,,C20230706015229757819,0.54,0.66,0.45,0.64,0.66,0.57,0.58,0.76,0.63,0.75,0.77,0.74,0.6,0.71,0.54,0.8,0.63,0.45,0.56,0.6,0.56,0.74,0.76,0.85, 92reasons for lack of direct tokenizer ,0.17,0.83,1,0
315,92,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,56416641,separate texts into sentences NLTK vs spaCy,"I want to separate texts into sentences.

looking in stack overflow I found:

WITH NLTK

from nltk.tokenize import sent_tokenize
text=""""""Hello Mr. Smith, how are you doing today? The weathe is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard""""""
tokenized_text=sent_tokenize(text)
print(tokenized_text)
WITH SPACY

from spacy.lang.en import English # updated

raw_text = 'Hello, world. Here are two sentences.'
nlp = English()
nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated
doc = nlp(raw_text)
sentences = [sent.string.strip() for sent in doc.sents]
The question is what in the background for spacy having to do it differently with a so called create_pipe. Sentences are important for training your own word embedings for NLP. There should be a reason why spaCy does not include directly out of the box a sentence tokenizer.

Thanks.

NOTE: Be aware that a simply .split(.) does not work, there are several decimal numbers in the text and other kind of tokens containing '.'","By default, spaCy uses its dependency parser to do sentence segmentation, which requires loading a statistical model. The sentencizer is a rule-based sentence segmenter that you can use to define your own sentence segmentation rules without loading a model.

If you don't mind leaving the parser activated, you can use the following code:

import spacy
nlp = spacy.load('en_core_web_sm') # or whatever model you have installed
raw_text = 'Hello, world. Here are two sentences.'
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]",spacy's segmentation options,Spacy offers a rule-based sentence segmenter or a dependency parser for sentence segmentation.,,1,,,,C20230706015228707347,0.58,0.6,0.47,0.71,0.51,0.57,0.44,0.63,0.56,0.43,0.59,0.71,0.44,0.67,0.44,0.47,0.48,0.46,0.72,0.72,0.76,0.82,0.73,0.76, 92spacy's segmentation options ,0.06,0.94,1,1
316,93,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,51725599,training sentence tokenizer in spaCy,"I'm trying to tokenize sentences using spacy.

The text includes lots of abbreviations and comments which ends with a period. Also, the text was obtained with OCR and sometimes there are line breaks in the middle of sentences. Spacy doesn't seem to be performing so well in these situations.

I have extracted some examples of how I want these sentences to be split. Is there any way to train spacy's sentence tokenizer?","Spacy is a little unusual in that the default sentence segmentation comes from the dependency parser, so you can't train a sentence boundary detector directly as such, but you can add your own custom component to the pipeline or pre-insert some boundaries that the parser will respect. See their documentation with examples: Spacy Sentence Segmentation

For the cases you're describing it would potentially be useful also be able to specify that a particular position is NOT a sentence boundary, but as far as I can tell that's not currently possible.",customization possible,It is possible to add a custom component to the pipeline or pre-insert boundaries to improve sentence segmentation.,,1,,,,C20230706015230801167,0.72,0.74,0.55,0.89,0.65,0.6,0.79,0.83,0.79,0.87,0.93,0.84,0.86,0.67,0.42,0.61,0.41,0.33,0.69,0.71,0.66,0.65,0.61,0.89, 93customization possible ,0.09,0.91,1,1
317,93,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,51725599,training sentence tokenizer in spaCy,"I'm trying to tokenize sentences using spacy.

The text includes lots of abbreviations and comments which ends with a period. Also, the text was obtained with OCR and sometimes there are line breaks in the middle of sentences. Spacy doesn't seem to be performing so well in these situations.

I have extracted some examples of how I want these sentences to be split. Is there any way to train spacy's sentence tokenizer?","Spacy is a little unusual in that the default sentence segmentation comes from the dependency parser, so you can't train a sentence boundary detector directly as such, but you can add your own custom component to the pipeline or pre-insert some boundaries that the parser will respect. See their documentation with examples: Spacy Sentence Segmentation

For the cases you're describing it would potentially be useful also be able to specify that a particular position is NOT a sentence boundary, but as far as I can tell that's not currently possible.",limitations of default segmentation,"Spacy's default sentence segmentation may not work well with text containing abbreviations, comments that end with a period, and OCR-induced line breaks.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015230277318,0.6,0.73,0.59,0.62,0.64,0.62,0.59,0.73,0.92,0.85,0.66,0.77,0.66,0.43,0.59,0.56,0.59,0.5,0.62,0.57,0.69,0.84,0.53,0.5, 93limitations of default segmentation ,0.6,0.4,0,0
318,93,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,51725599,training sentence tokenizer in spaCy,"I'm trying to tokenize sentences using spacy.

The text includes lots of abbreviations and comments which ends with a period. Also, the text was obtained with OCR and sometimes there are line breaks in the middle of sentences. Spacy doesn't seem to be performing so well in these situations.

I have extracted some examples of how I want these sentences to be split. Is there any way to train spacy's sentence tokenizer?","Spacy is a little unusual in that the default sentence segmentation comes from the dependency parser, so you can't train a sentence boundary detector directly as such, but you can add your own custom component to the pipeline or pre-insert some boundaries that the parser will respect. See their documentation with examples: Spacy Sentence Segmentation

For the cases you're describing it would potentially be useful also be able to specify that a particular position is NOT a sentence boundary, but as far as I can tell that's not currently possible.",no specific non-boundary option,There is no current support for specifying a position as NOT a sentence boundary.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015231327911,0.78,0.73,0.75,0.78,0.82,0.9,0.52,0.74,0.6,0.6,0.59,0.79,0.74,0.69,0.49,0.48,0.66,0.62,0.48,0.5,0.51,0.62,0.63,0.81, 93no specific non-boundary option ,0.65,0.35,0,0
319,94,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,50742516,How to get the index of a token in a sentence in spaCy?,"Is there an elegant way to get the index of a word/token in its sentence? I am aware of the attributes for tokens https://spacy.io/api/token#attributes The i attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.

Example:

""This is an example. This is another example.""
What I need is both ""This"" to be returned as index 0, both ""is"" to be returned as index 1 etc...","A spaCy Doc object also lets you iterate over the doc.sents, which are Span objects of the individual sentence. To get a span's start and end index in the parent document you can look at the start and end attribute. So if you iterate over the sentences and subtract the sentence start index from the token.i, you get the token's relative index within the sentence:

for sent in doc.sents:
    for token in sent:
        print(token.text, token.i - sent.start)
The default sentence segmentation uses the dependency parse, which is usually more accurate. However, you can also plug in a rule-based or entirely custom solution (see here for details).",flexible sentence segmentation,"Spacy's default sentence segmentation is accurate, but users can also plug in a rule-based or custom solution.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015233100242,0.78,0.7,0.63,0.87,0.76,0.61,0.69,0.85,0.71,0.67,0.93,0.68,0.69,0.74,0.38,0.65,0.47,0.68,0.65,0.81,0.52,0.73,0.88,0.59, 94flexible sentence segmentation ,0.84,0.16,0,0
320,94,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,50742516,How to get the index of a token in a sentence in spaCy?,"Is there an elegant way to get the index of a word/token in its sentence? I am aware of the attributes for tokens https://spacy.io/api/token#attributes The i attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.

Example:

""This is an example. This is another example.""
What I need is both ""This"" to be returned as index 0, both ""is"" to be returned as index 1 etc...","A spaCy Doc object also lets you iterate over the doc.sents, which are Span objects of the individual sentence. To get a span's start and end index in the parent document you can look at the start and end attribute. So if you iterate over the sentences and subtract the sentence start index from the token.i, you get the token's relative index within the sentence:

for sent in doc.sents:
    for token in sent:
        print(token.text, token.i - sent.start)
The default sentence segmentation uses the dependency parse, which is usually more accurate. However, you can also plug in a rule-based or entirely custom solution (see here for details).",iterating over spans,"Spacy Doc object allows iteration over doc.sents, which are Span objects of individual sentences.",,1,,,,C20230706015231849942,0.76,0.49,0.77,0.56,0.75,0.63,0.72,0.69,0.62,0.76,0.53,0.55,0.64,0.6,0.45,0.61,0.45,0.61,0.62,0.73,0.78,0.78,0.72,0.74, 94iterating over spans ,0.08,0.92,1,1
321,94,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,50742516,How to get the index of a token in a sentence in spaCy?,"Is there an elegant way to get the index of a word/token in its sentence? I am aware of the attributes for tokens https://spacy.io/api/token#attributes The i attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.

Example:

""This is an example. This is another example.""
What I need is both ""This"" to be returned as index 0, both ""is"" to be returned as index 1 etc...","A spaCy Doc object also lets you iterate over the doc.sents, which are Span objects of the individual sentence. To get a span's start and end index in the parent document you can look at the start and end attribute. So if you iterate over the sentences and subtract the sentence start index from the token.i, you get the token's relative index within the sentence:

for sent in doc.sents:
    for token in sent:
        print(token.text, token.i - sent.start)
The default sentence segmentation uses the dependency parse, which is usually more accurate. However, you can also plug in a rule-based or entirely custom solution (see here for details).",relative index calculation,Subtracting sentence start index from token.i provides the token's relative index within the sentence.,,1,,,,C20230706015232562611,0.79,0.85,0.62,0.92,0.7,0.67,0.81,0.86,0.63,0.9,0.6,0.58,0.65,0.7,0.72,0.92,0.82,0.84,0.74,0.87,0.7,0.89,0.78,0.74, 94relative index calculation ,0.02,0.98,1,1
322,95,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,57779549,Converting Spacy generated dependency into CoNLL format cannot handle more than one ROOT?,"I used the SpaCy library to generate dependencies and save it into a CoNLL format using the code below.

This works, but there are some sentences in my dataset that get splits into two by Spacy because they have two ROOTS. This results in having two fields for one sentence in the CoNLL format.

Example: A random sentence from my dataset is : ""teanna trump probably cleaner twitter hoe but""

Is there a way to save it all in one field instead of two even though it has two ROOTS so that 'but' becomes 7th item in field number 1? Which means it would look like this instead","I'd recommend using (or adapting) the textacy CoNLL exporter to get the right format, see: How to generate .conllu from a Doc object?

Spacy's parser is doing sentence segmentation and you're iterating over doc.sents, so you'll see each sentence it exported separately. If you want to provide your own sentence segmentation, you can do that with a custom component, e.g.:

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ""..."":
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before=""parser"")
Details (especially about how to handle None vs. False vs. True): https://spacy.io/usage/linguistic-features#sbd-custom

Spacy's default models aren't trained on twitter-like text, the parser probably won't perform well with respect to sentence boundaries here.

(Please ask unrelated questions as separate questions, and also take a look at spacy's docs: https://spacy.io/usage/linguistic-features#special-cases)",custom sentence segmentation,"The user can provide their own sentence segmentation using a custom component, which allows for more flexibility and customization of the segmentation process.",,1,,,,C20230706015234550541,0.66,0.77,0.65,0.66,0.59,0.73,0.51,0.77,0.69,0.77,0.82,0.9,0.55,0.6,0.68,0.83,0.43,0.38,0.5,0.72,0.52,0.83,0.74,0.75, 95custom sentence segmentation ,0.2,0.8,1,1
323,95,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,57779549,Converting Spacy generated dependency into CoNLL format cannot handle more than one ROOT?,"I used the SpaCy library to generate dependencies and save it into a CoNLL format using the code below.

This works, but there are some sentences in my dataset that get splits into two by Spacy because they have two ROOTS. This results in having two fields for one sentence in the CoNLL format.

Example: A random sentence from my dataset is : ""teanna trump probably cleaner twitter hoe but""

Is there a way to save it all in one field instead of two even though it has two ROOTS so that 'but' becomes 7th item in field number 1? Which means it would look like this instead","I'd recommend using (or adapting) the textacy CoNLL exporter to get the right format, see: How to generate .conllu from a Doc object?

Spacy's parser is doing sentence segmentation and you're iterating over doc.sents, so you'll see each sentence it exported separately. If you want to provide your own sentence segmentation, you can do that with a custom component, e.g.:

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ""..."":
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before=""parser"")
Details (especially about how to handle None vs. False vs. True): https://spacy.io/usage/linguistic-features#sbd-custom

Spacy's default models aren't trained on twitter-like text, the parser probably won't perform well with respect to sentence boundaries here.

(Please ask unrelated questions as separate questions, and also take a look at spacy's docs: https://spacy.io/usage/linguistic-features#special-cases)",limitations of spacy's sentence segmentation,"Spacy's parser may not perform well with respect to sentence boundaries in certain types of text, such as Twitter-like text.",,1,,,,C20230706015233599059,0.71,0.79,0.63,0.81,0.6,0.75,0.57,0.83,0.81,0.76,0.74,0.94,0.65,0.68,0.61,0.47,0.44,0.83,0.72,0.93,0.72,0.74,0.86,0.74, 95limitations of spacy's sentence segmentation ,0.05,0.95,1,1
324,95,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,57779549,Converting Spacy generated dependency into CoNLL format cannot handle more than one ROOT?,"I used the SpaCy library to generate dependencies and save it into a CoNLL format using the code below.

This works, but there are some sentences in my dataset that get splits into two by Spacy because they have two ROOTS. This results in having two fields for one sentence in the CoNLL format.

Example: A random sentence from my dataset is : ""teanna trump probably cleaner twitter hoe but""

Is there a way to save it all in one field instead of two even though it has two ROOTS so that 'but' becomes 7th item in field number 1? Which means it would look like this instead","I'd recommend using (or adapting) the textacy CoNLL exporter to get the right format, see: How to generate .conllu from a Doc object?

Spacy's parser is doing sentence segmentation and you're iterating over doc.sents, so you'll see each sentence it exported separately. If you want to provide your own sentence segmentation, you can do that with a custom component, e.g.:

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ""..."":
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before=""parser"")
Details (especially about how to handle None vs. False vs. True): https://spacy.io/usage/linguistic-features#sbd-custom

Spacy's default models aren't trained on twitter-like text, the parser probably won't perform well with respect to sentence boundaries here.

(Please ask unrelated questions as separate questions, and also take a look at spacy's docs: https://spacy.io/usage/linguistic-features#special-cases)",solution with textacy conll exporter,The user is advised to use (or adapt) textacy CoNLL exporter to get the right format for the CoNLL export.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015234078447,0.75,0.55,0.66,0.6,0.61,0.66,0.71,0.69,0.71,0.58,0.62,0.72,0.71,0.6,0.51,0.58,0.5,0.49,0.59,0.66,0.55,0.75,0.64,0.56, 95solution with textacy conll exporter ,0.75,0.25,0,0
325,96,Spacy,Feature,Text classification,Text classification feature,48834832,How do I create gold data for TextCategorizer training?,"I want to train a TextCategorizer model with the following (text, label) pairs.

Label COLOR:

The door is brown.
The barn is red.
The flower is yellow.
Label ANIMAL:

The horse is running.
The fish is jumping.
The chicken is asleep.
I am copying the example code in the documentation for TextCategorizer.

textcat = TextCategorizer(nlp.vocab)
losses = {}
optimizer = nlp.begin_training()
textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)
The doc variables will presumably be just nlp(""The door is brown."") and so on. What should be in gold1 and gold2? I'm guessing they should be GoldParse objects, but I don't see how you represent text categorization information in those.","According to this example train_textcat.py it should be something like {'cats': {'ANIMAL': 0, 'COLOR': 1}} if you want to train a multi-label model. Also, if you have only two classes, you can simply use {'cats': {'ANIMAL': 1}} for label ANIMAL and {'cats': {'ANIMAL': 0}} for label COLOR.

You can use the following minimal working example for a one category text classification;",example code provided,"The user is provided with an example code train_textcat.py, which shows how to represent text categorization information in the gold1 and gold2 variables.",,1,,,,C20230706015236047434,0.6,0.62,0.76,0.43,0.38,0.34,0.39,0.53,0.62,0.61,0.48,0.43,0.75,0.06,0.48,0.52,0.52,0.45,0.53,0.05,0.43,0.05,0.35,0.11, 96example code provided ,0.02,0.98,1,1
326,96,Spacy,Feature,Text classification,Text classification feature,48834832,How do I create gold data for TextCategorizer training?,"I want to train a TextCategorizer model with the following (text, label) pairs.

Label COLOR:

The door is brown.
The barn is red.
The flower is yellow.
Label ANIMAL:

The horse is running.
The fish is jumping.
The chicken is asleep.
I am copying the example code in the documentation for TextCategorizer.

textcat = TextCategorizer(nlp.vocab)
losses = {}
optimizer = nlp.begin_training()
textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)
The doc variables will presumably be just nlp(""The door is brown."") and so on. What should be in gold1 and gold2? I'm guessing they should be GoldParse objects, but I don't see how you represent text categorization information in those.","According to this example train_textcat.py it should be something like {'cats': {'ANIMAL': 0, 'COLOR': 1}} if you want to train a multi-label model. Also, if you have only two classes, you can simply use {'cats': {'ANIMAL': 1}} for label ANIMAL and {'cats': {'ANIMAL': 0}} for label COLOR.

You can use the following minimal working example for a one category text classification;",spacy supports textcategorizer,The user is able to train a TextCategorizer model with labeled data pairs using example code from Spacy's documentation.,Question not focused on main topic,0,Misdirected Challenge,Challenger,,C20230706015235056380,0.56,0.68,0.45,0.57,0.61,0.59,0.7,0.82,0.91,0.82,0.72,0.84,0.64,0.64,0.3,0.4,0.37,0.53,0.45,0.73,0.53,0.43,0.42,0.43, 96spacy supports textcategorizer ,0.92,0.08,0,1
327,96,Spacy,Feature,Text classification,Text classification feature,48834832,How do I create gold data for TextCategorizer training?,"I want to train a TextCategorizer model with the following (text, label) pairs.

Label COLOR:

The door is brown.
The barn is red.
The flower is yellow.
Label ANIMAL:

The horse is running.
The fish is jumping.
The chicken is asleep.
I am copying the example code in the documentation for TextCategorizer.

textcat = TextCategorizer(nlp.vocab)
losses = {}
optimizer = nlp.begin_training()
textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)
The doc variables will presumably be just nlp(""The door is brown."") and so on. What should be in gold1 and gold2? I'm guessing they should be GoldParse objects, but I don't see how you represent text categorization information in those.","According to this example train_textcat.py it should be something like {'cats': {'ANIMAL': 0, 'COLOR': 1}} if you want to train a multi-label model. Also, if you have only two classes, you can simply use {'cats': {'ANIMAL': 1}} for label ANIMAL and {'cats': {'ANIMAL': 0}} for label COLOR.

You can use the following minimal working example for a one category text classification;",supports multi-class classification,"The user is able to classify text into categories like ANIMAL and COLOR, indicating that Spacy is capable of supporting multi-class text classification.",,1,,,,C20230706015235520866,0.75,0.65,0.31,0.47,0.63,0.44,0.3,0.72,0.75,0.33,0.36,0.55,0.08,0.49,0.6,0.36,0.57,0.41,0.29,0.48,0.59,0.36,0.14,0.43, 96supports multi-class classification ,0.06,0.94,1,1
328,97,Spacy,Feature,Text classification,Text classification feature,62092445,Is it possible to improve spaCy's similarity results with custom named entities?,"I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.

I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.

Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?

If I was wrong, what's the best approach to improve results? Seems like some sort of custom labeling should help.","I found my solution was nestled in this tutorial: Text Classification in Python Using spaCy, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's CountVectorizer.

I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories. The trick was to set multi_class='auto' on the LogisticRegression linear model, and to use average='micro' on the precision score and precision recall, so all my text data, like entities, were leveraged:

classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
and...

print(""Logistic Regression Accuracy:"",metrics.accuracy_score(y_test, predicted))
print(""Logistic Regression Precision:"",metrics.precision_score(y_test, predicted,average='micro'))
print(""Logistic Regression Recall:"",metrics.recall_score(y_test, predicted,average='micro'))
Hope this helps save someone some time!",custom named entities,SpaCy supports custom named entities for text classification.,,1,,,,C20230706015236572187,0.53,0.43,0.53,0.84,0.73,0.72,0.35,0.38,0.33,0.71,0.81,0.73,0.62,0.55,0.63,0.4,0.92,0.44,0.42,0.39,0.33,0.81,0.78,0.84, 97custom named entities ,0.02,0.98,1,1
329,97,Spacy,Feature,Text classification,Text classification feature,62092445,Is it possible to improve spaCy's similarity results with custom named entities?,"I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.

I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.

Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?

If I was wrong, what's the best approach to improve results? Seems like some sort of custom labeling should help.","I found my solution was nestled in this tutorial: Text Classification in Python Using spaCy, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's CountVectorizer.

I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories. The trick was to set multi_class='auto' on the LogisticRegression linear model, and to use average='micro' on the precision score and precision recall, so all my text data, like entities, were leveraged:

classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
and...

print(""Logistic Regression Accuracy:"",metrics.accuracy_score(y_test, predicted))
print(""Logistic Regression Precision:"",metrics.precision_score(y_test, predicted,average='micro'))
print(""Logistic Regression Recall:"",metrics.recall_score(y_test, predicted,average='micro'))
Hope this helps save someone some time!",effective approach,The user found success with a LogisticRegression model with multi_class='auto' and average='micro' for optimal use of their text data.,,1,,,,C20230706015238138506,0.7,0.52,0.6,0.73,0.9,0.74,0.76,0.84,0.72,0.83,0.85,0.78,0.65,0.46,0.65,0.5,0.95,0.54,0.6,0.79,0.59,0.73,0.96,0.73, 97effective approach ,0.35,0.65,1,1
330,97,Spacy,Feature,Text classification,Text classification feature,62092445,Is it possible to improve spaCy's similarity results with custom named entities?,"I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.

I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.

Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?

If I was wrong, what's the best approach to improve results? Seems like some sort of custom labeling should help.","I found my solution was nestled in this tutorial: Text Classification in Python Using spaCy, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's CountVectorizer.

I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories. The trick was to set multi_class='auto' on the LogisticRegression linear model, and to use average='micro' on the precision score and precision recall, so all my text data, like entities, were leveraged:

classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
and...

print(""Logistic Regression Accuracy:"",metrics.accuracy_score(y_test, predicted))
print(""Logistic Regression Precision:"",metrics.precision_score(y_test, predicted,average='micro'))
print(""Logistic Regression Recall:"",metrics.recall_score(y_test, predicted,average='micro'))
Hope this helps save someone some time!",ineffective approach,Adding custom named entities did not improve similarity results for the user.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015237097167,0.42,0.6,0.66,0.59,0.58,0.74,0.39,0.53,0.46,0.62,0.61,0.83,0.53,0.52,0.4,0.51,0.51,0.58,0.53,0.47,0.5,0.78,0.78,0.87, 97ineffective approach ,0.52,0.48,0,0
331,97,Spacy,Feature,Text classification,Text classification feature,62092445,Is it possible to improve spaCy's similarity results with custom named entities?,"I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.

I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.

Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?

If I was wrong, what's the best approach to improve results? Seems like some sort of custom labeling should help.","I found my solution was nestled in this tutorial: Text Classification in Python Using spaCy, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's CountVectorizer.

I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories. The trick was to set multi_class='auto' on the LogisticRegression linear model, and to use average='micro' on the precision score and precision recall, so all my text data, like entities, were leveraged:

classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
and...

print(""Logistic Regression Accuracy:"",metrics.accuracy_score(y_test, predicted))
print(""Logistic Regression Precision:"",metrics.precision_score(y_test, predicted,average='micro'))
print(""Logistic Regression Recall:"",metrics.recall_score(y_test, predicted,average='micro'))
Hope this helps save someone some time!",text classification tutorial,The user found success with a text classification tutorial using spaCy and SciKit-Learn's CountVectorizer.,Out of Scope Challenge,1,Out of Scope Challenge,Challenger,,C20230706015237619196,0.57,0.7,0.59,0.8,0.73,0.78,0.57,0.78,0.54,0.84,0.57,0.68,0.55,0.62,0.56,0.66,0.56,0.8,0.45,0.61,0.56,0.82,0.81,0.78, 97text classification tutorial ,0.63,0.37,0,0
332,98,Spacy,Feature,Rule-based matching,Rule-based matching feature,72346631,Highlight text parts based on labels,"thanks to fellow stackoverflowrians I have data labels that I would like to high light in the text:

eg. I have product description

Description: Tampered black round grey/natural swing with yellow load-bearing left hook
Features were extracted as

colors=['black','grey','natural','yellow']
shape = ['round']
direction= ['left']
In Spacy it is possible to highlight the features like this

enter image description here

Is there any possibility to highlight it also like this from the data I have as labels? So that I have labels shown in the text too? I dont know if Spacy is the good tool or any other is better?
","I'm not entirely sure what you're asking, but you can put entities of your own on the spaCy Doc object and pass them to Displacy.

To simply set entities manually, you can do this:

doc = nlp(...)
span = doc[0:1] # whatever span of the doc you want to highlight
span.ent_label_ = ""COLOR"" # the label you want
ents = [span] # in reality you could do more than one
doc.ents = ents
If you have word list and need to look for words, you can use rule-based matching with an EntityRuler. Check the rule-based matching guide.",setting entities manually,"Entities can be set manually to highlight specific spans of text, as described in the provided answer.",,1,,,,C20230706015239199193,0.56,0.67,0.76,0.63,0.47,0.53,0.47,0.64,0.45,0.59,0.44,0.72,0.59,0.68,0.63,0.54,0.51,0.6,0.6,0.63,0.5,0.67,0.71,0.67, 98setting entities manually ,0.06,0.94,1,1
333,98,Spacy,Feature,Rule-based matching,Rule-based matching feature,72346631,Highlight text parts based on labels,"thanks to fellow stackoverflowrians I have data labels that I would like to high light in the text:

eg. I have product description

Description: Tampered black round grey/natural swing with yellow load-bearing left hook
Features were extracted as

colors=['black','grey','natural','yellow']
shape = ['round']
direction= ['left']
In Spacy it is possible to highlight the features like this

enter image description here

Is there any possibility to highlight it also like this from the data I have as labels? So that I have labels shown in the text too? I dont know if Spacy is the good tool or any other is better?
","I'm not entirely sure what you're asking, but you can put entities of your own on the spaCy Doc object and pass them to Displacy.

To simply set entities manually, you can do this:

doc = nlp(...)
span = doc[0:1] # whatever span of the doc you want to highlight
span.ent_label_ = ""COLOR"" # the label you want
ents = [span] # in reality you could do more than one
doc.ents = ents
If you have word list and need to look for words, you can use rule-based matching with an EntityRuler. Check the rule-based matching guide.",spacy suitability,"Based on the conversation, Spacy appears to be a suitable tool for the user's needs of highlighting text based on labels, although this may depend on their specific use case.",,1,,,,C20230706015240241771,0.71,0.79,0.74,0.79,0.79,0.79,0.82,0.74,0.78,0.86,0.79,0.75,0.76,0.49,0.68,0.62,0.53,0.58,0.7,0.6,0.62,0.74,0.7,0.67, 98spacy suitability ,0.14,0.86,1,1
334,98,Spacy,Feature,Rule-based matching,Rule-based matching feature,72346631,Highlight text parts based on labels,"thanks to fellow stackoverflowrians I have data labels that I would like to high light in the text:

eg. I have product description

Description: Tampered black round grey/natural swing with yellow load-bearing left hook
Features were extracted as

colors=['black','grey','natural','yellow']
shape = ['round']
direction= ['left']
In Spacy it is possible to highlight the features like this

enter image description here

Is there any possibility to highlight it also like this from the data I have as labels? So that I have labels shown in the text too? I dont know if Spacy is the good tool or any other is better?
","I'm not entirely sure what you're asking, but you can put entities of your own on the spaCy Doc object and pass them to Displacy.

To simply set entities manually, you can do this:

doc = nlp(...)
span = doc[0:1] # whatever span of the doc you want to highlight
span.ent_label_ = ""COLOR"" # the label you want
ents = [span] # in reality you could do more than one
doc.ents = ents
If you have word list and need to look for words, you can use rule-based matching with an EntityRuler. Check the rule-based matching guide.",support for rule-based matching,"Spacy supports rule-based matching features for highlighting text based on labels, as confirmed in the answer.",Limitation of similarity calculation (large text),1,Similary Calculation Issue ,Decider,,C20230706015238685988,0.57,0.64,0.8,0.9,0.54,0.47,0.61,0.68,0.3,0.8,0.61,0.63,0.52,0.64,0.93,0.71,0.28,0.37,0.73,0.71,0.32,0.8,0.41,0.42, 98support for rule-based matching ,0.03,0.97,1,1
335,98,Spacy,Feature,Rule-based matching,Rule-based matching feature,72346631,Highlight text parts based on labels,"thanks to fellow stackoverflowrians I have data labels that I would like to high light in the text:

eg. I have product description

Description: Tampered black round grey/natural swing with yellow load-bearing left hook
Features were extracted as

colors=['black','grey','natural','yellow']
shape = ['round']
direction= ['left']
In Spacy it is possible to highlight the features like this

enter image description here

Is there any possibility to highlight it also like this from the data I have as labels? So that I have labels shown in the text too? I dont know if Spacy is the good tool or any other is better?
","I'm not entirely sure what you're asking, but you can put entities of your own on the spaCy Doc object and pass them to Displacy.

To simply set entities manually, you can do this:

doc = nlp(...)
span = doc[0:1] # whatever span of the doc you want to highlight
span.ent_label_ = ""COLOR"" # the label you want
ents = [span] # in reality you could do more than one
doc.ents = ents
If you have word list and need to look for words, you can use rule-based matching with an EntityRuler. Check the rule-based matching guide.",using an entityruler,"An EntityRuler can be used with rule-based matching to look for specific words in the text, according to the answer.",Question not focused on main topic,1,Misdirected Challenge,Challenger,,C20230706015239715046,0.82,0.69,0.77,0.74,0.59,0.55,0.83,0.7,0.62,0.72,0.64,0.71,0.54,0.61,0.71,0.91,0.47,0.48,0.69,0.64,0.6,0.79,0.54,0.65, 98using an entityruler ,0.12,0.88,1,1
336,99,Spacy,Feature,Rule-based matching,Rule-based matching feature,76011075,"Entity Ruler cannot recognize ""Frankfurt am Main"" as Geo-Political Entity (GPE)","I try to add a new rule in Named Entity Recognition so that Spacy will label the phrase ""Frankfurt am Main"" as GPE.

nlp = spacy.load(""en_core_web_sm"")
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [{""label"": ""ORG"", ""pattern"": ""MyCorp Inc.""},{""label"": ""GPE"", ""pattern"": ""Frankfurt am Main""}]
ruler.add_patterns(patterns)
doc = nlp(""MyCorp Inc. is a company in Frankfurt am Main"")
print([(ent.text, ent.label_) for ent in doc.ents])
But Spacy still label the word ""Frankfurt"" as GPE instead of the whole phrase ""Frankfurt am Main"":

[('MyCorp Inc.', 'ORG'), ('Frankfurt', 'GPE')]
I want to know how I can adjust spacy so that it can label ""Frankfurt am Main"" as GPE.","Put the entity_ruler before ner so that its entity spans have priority over the ner entities.

nlp.add_pipe(""entity_ruler"", before=""ner"")
See: https://spacy.io/usage/rule-based-matching#entityruler-usage",adjusting entity priority,"The suggested solution involves moving the Entity Ruler module before the NER module in the pipeline, so that the Entity Ruler entities take priority over NER entities.",,1,,,,C20230706015242338632,0.76,0.72,0.74,0.55,0.57,0.56,0.8,0.74,0.77,0.72,0.68,0.67,0.6,0.79,0.63,0.61,0.57,0.5,0.76,0.69,0.76,0.72,0.88,0.69, 99adjusting entity priority ,0.04,0.96,1,1
337,99,Spacy,Feature,Rule-based matching,Rule-based matching feature,76011075,"Entity Ruler cannot recognize ""Frankfurt am Main"" as Geo-Political Entity (GPE)","I try to add a new rule in Named Entity Recognition so that Spacy will label the phrase ""Frankfurt am Main"" as GPE.

nlp = spacy.load(""en_core_web_sm"")
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [{""label"": ""ORG"", ""pattern"": ""MyCorp Inc.""},{""label"": ""GPE"", ""pattern"": ""Frankfurt am Main""}]
ruler.add_patterns(patterns)
doc = nlp(""MyCorp Inc. is a company in Frankfurt am Main"")
print([(ent.text, ent.label_) for ent in doc.ents])
But Spacy still label the word ""Frankfurt"" as GPE instead of the whole phrase ""Frankfurt am Main"":

[('MyCorp Inc.', 'ORG'), ('Frankfurt', 'GPE')]
I want to know how I can adjust spacy so that it can label ""Frankfurt am Main"" as GPE.","Put the entity_ruler before ner so that its entity spans have priority over the ner entities.

nlp.add_pipe(""entity_ruler"", before=""ner"")
See: https://spacy.io/usage/rule-based-matching#entityruler-usage",entity ruler limitations,"The user encountered an issue where the Entity Ruler was unable to recognize 'Frankfurt am Main' as a GPE entity, indicating some limitations in the feature.",,1,,,,C20230706015241507160,0.53,0.58,0.63,0.22,0.49,0.37,0.39,0.61,0.33,0.42,0.53,0.44,0.53,0.61,0.45,0.28,0.41,0.32,0.55,0.63,0.59,0.84,0.82,0.82, 99entity ruler limitations ,0.02,0.98,1,1
338,100,Spacy,Feature,Rule-based matching,Rule-based matching feature,57479028,spaCy NLP custom rule matcher,"I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,

I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given below the sample sentence,

""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics""
I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.","Spacy provides Rule-based matching which you could use.

They can be used as follows:

import spacy
from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['Czech Republic', 'Slovakia']
ruler = EntityRuler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern =[{'ENT_TYPE': 'country'}, {'lower': 'won'},{""IS_DIGIT"": True}]
matcher.add('medal', None, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)

output:

Czech Republic won 5
Slovakia won 0
The above code should get you started. Naturally, you will have to write your own more complex rules so that you can handle cases like: ""Czech Republic unsurprisingly won 5 gold medals at olympics in 1995."" And other more complex sentence structures.",code shared for example,The given code shows an example of how to use EntityRuler and Matcher for matching and extracting information from the given text.,,1,,,,C20230706015315705969,0.74,0.67,0.72,0.5,0.44,0.39,0.59,0.31,0.53,0.55,0.53,0.48,0.71,0.64,0.55,0.43,0.81,0.36,0.59,0.38,0.42,0.67,0.44,0.34, 100code shared for example ,0.07,0.93,1,1
339,100,Spacy,Feature,Rule-based matching,Rule-based matching feature,57479028,spaCy NLP custom rule matcher,"I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,

I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given below the sample sentence,

""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics""
I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.","Spacy provides Rule-based matching which you could use.

They can be used as follows:

import spacy
from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['Czech Republic', 'Slovakia']
ruler = EntityRuler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern =[{'ENT_TYPE': 'country'}, {'lower': 'won'},{""IS_DIGIT"": True}]
matcher.add('medal', None, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)

output:

Czech Republic won 5
Slovakia won 0
The above code should get you started. Naturally, you will have to write your own more complex rules so that you can handle cases like: ""Czech Republic unsurprisingly won 5 gold medals at olympics in 1995."" And other more complex sentence structures.",entityruler for pattern matching,EntityRuler can be used to create patterns for entity matching in the given text. Users can add multiple patterns using this.,,1,,,,C20230706015243442482,0.8,0.55,0.61,0.69,0.67,0.64,0.55,0.61,0.51,0.35,0.65,0.39,0.46,0.59,0.66,0.89,0.48,0.49,0.41,0.67,0.6,0.58,0.39,0.7, 100entityruler for pattern matching ,0.03,0.97,1,1
340,100,Spacy,Feature,Rule-based matching,Rule-based matching feature,57479028,spaCy NLP custom rule matcher,"I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,

I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given below the sample sentence,

""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics""
I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.","Spacy provides Rule-based matching which you could use.

They can be used as follows:

import spacy
from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['Czech Republic', 'Slovakia']
ruler = EntityRuler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern =[{'ENT_TYPE': 'country'}, {'lower': 'won'},{""IS_DIGIT"": True}]
matcher.add('medal', None, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)

output:

Czech Republic won 5
Slovakia won 0
The above code should get you started. Naturally, you will have to write your own more complex rules so that you can handle cases like: ""Czech Republic unsurprisingly won 5 gold medals at olympics in 1995."" And other more complex sentence structures.",matcher for rule matching,Matcher can be used to define complex rules for matching entities in the text based on the defined patterns.,,1,,,,C20230706015243910447,0.82,0.69,0.45,0.64,0.49,0.53,0.61,0.71,0.42,0.49,0.29,0.46,0.67,0.66,0.69,0.66,0.66,0.84,0.65,0.66,0.71,0.45,0.55,0.78, 100matcher for rule matching ,0.05,0.95,1,1
341,100,Spacy,Feature,Rule-based matching,Rule-based matching feature,57479028,spaCy NLP custom rule matcher,"I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,

I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given below the sample sentence,

""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics""
I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.","Spacy provides Rule-based matching which you could use.

They can be used as follows:

import spacy
from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['Czech Republic', 'Slovakia']
ruler = EntityRuler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern =[{'ENT_TYPE': 'country'}, {'lower': 'won'},{""IS_DIGIT"": True}]
matcher.add('medal', None, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)

output:

Czech Republic won 5
Slovakia won 0
The above code should get you started. Naturally, you will have to write your own more complex rules so that you can handle cases like: ""Czech Republic unsurprisingly won 5 gold medals at olympics in 1995."" And other more complex sentence structures.",spacy supports rule-based matching,Spacy provides the Rule-based matching feature that allows users to create patterns and match them in the given text.,,1,,,,C20230706015242861629,0.71,0.61,0.63,0.56,0.68,0.57,0.59,0.49,0.5,0.55,0.37,0.39,0.64,0.66,0.56,0.48,0.87,0.49,0.67,0.49,0.57,0.49,0.62,0.7, 100spacy supports rule-based matching ,0.01,0.99,1,1