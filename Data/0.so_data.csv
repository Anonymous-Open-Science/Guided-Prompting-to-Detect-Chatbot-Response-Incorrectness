id,library,aspect,search_word,aspect_question,so_id,so_question_title,so_question_body,so_answer,status,question_approach,question,answer,expected_answer,callid
1,Spacy,Ease of use,simple,Ease of use aspect,62766608,Is there a simple way to get the position of a token in sequence with spacy?,"I want to the start index and the end index of every token in a sequence. Is there a simple way to do that with spacy?
","import spacy

text = 'Brown is a nice guy'
nlp = spacy.load(""en_core_web_sm"")
doc = nlp(text)

for token in doc:
  print(token.text, token.idx, token.idx + len(token.text) - 1)",,,,,,
2,Spacy,Ease of use,easy,Ease of use aspect,56694713,In spaCy is there a way to extract the sentence the entity has been extracted from?,"considering the following sentence:

""the quick brown fox jumps over the lazy dog""
and if I want to extract brown, extracting only brown is fairly easy to do however I want the following output:

""brown"" , ""the quick brown fox jumps over the lazy dog""","Yes, there is a way to do this in Spacy. You have to iterate over the entity Span objects and extract the sentence from each Span object. Here is an example:

    doc = nlp(""John and Claire live in London. They have a dog. Claire walks her 
    dog everyday."")
    for entity in doc.ents:
       print('Entity extracted : ', entity.text)
       print('Sentence extracted from : ', entity.sent)",,,,,,
3,Spacy,Ease of use,easy,Ease of use aspect,74799295,How to extract cities with Spacy / Can't load French model,"I know it's perhaps an easy question but i'm not very familiar with Spacy.

So i'm trying to extract cities in a text file.","I just skimmed some of the source code for locationtagger, and it appears that it hardcodes usage of the en_core_web_sm model. It likely does not form correct parses of your input text.

I would not use nltk or locationtagger for this task.

Instead, download a proper spaCy model for French:

python3 -m spacy download fr_core_news_{sm|md|lg|trf}
Read spaCy's documentation on named entity recognition [1]. This includes information about identifying geopolitical entities (""GPE"").

The default spaCy models will tag cities, states/provinces/districts, and countries under the ""GPE"" tag. If you are interested only in the cities, then, you should filter the found GPEs against the data in locationtagger's City-Region-Locations.csv.

Additionally, you may wish to segment the text by paragraph and use spaCy's nlp.pipe to process paragraphs in parallel.",,,,,,
4,Spacy,Ease of use,simple,Ease of use aspect,57199811,Create a Spacy pipeline with my own Tokeniser?,"What puzzles me is that a spacy ""pipeline"" is a sequence of processing steps, but the very first step which converts the original data into a spacy document is ""hidden"". Unlike all the other steps it cannot be accessed from the pipeline, I think. So what if I want to replace it, change it, or create a pipeline with my own Tokenizer, what is the proper way to do this?","The reason the tokenizer isn't part of the regular pipeline is because it's ""special"": There can only really be one, and while all other pipeline components take a Doc and return it, the tokenizer takes a string of text and turns it into a Doc.

However, nlp.tokenizer is writable, so you can either create your own Tokenizer class from scratch, or even replace it with an entirely custom function. Here's a super simple example that shows the idea:
",,,,,,
5,Spacy,Ease of use,difficult,Ease of use aspect,67789544,Given a word can we get all possible lemmas for it using Spacy?,"The input word is standalone and not part of a sentence but I would like to get all of its possible lemmas as if the input word were in different sentences with all possible POS tags. I would also like to get the lookup version of the word's lemma.

Why am I doing this?

I have extracted lemmas from all the documents and I have also calculated the number of dependency links between lemmas. Both of which I have done using en_core_web_sm. Now, given an input word, I would like to return the lemmas that are linked most frequently to all the possible lemmas of the input word.

So in short, I would like to replicate the behaviour of token._lemma for the input word with all possible POS tags to maintain consistency with the lemma links I have counted.","I found it difficult to get lemmas and inflections directly out of spaCy without first constructing an example sentence to give it context. This wasn't ideal, so I looked further and found LemmaInflect did this very well.",,,,,,
6,Spacy,Ease of use,difficult,Ease of use aspect,48980120,Is it possible to parse emojis using spaCy?,"Is it possible to tokenize emojis like :), :(, ;~( properly using the spaCy Python library? e.g. If I run the following code:

import spacy

nlp = spacy.load('en')
doc = nlp(""Hello bright world :)"")
And then visualize the doc with displaCy:
It incorrectly parses world :) as one token. How can I modify spaCy so it recognizes these additional symbols? Thanks.

edit: Found the following: https://github.com/ines/spacymoji but I think it only supports Unicode emojis like ✨ and not ASCII ones like :)","Yes, spaCy actually includes a pretty comprehensive list of text-based emoticons as part of its tokenizer exceptions. So using your example above and printing the individual tokens, the emoticon is tokenized correctly:

doc = nlp(""Hello bright world :)"")
print([token.text for token in doc])
# ['Hello', 'bright', 'world', ':)']
I think what happens here is that you actually came across an interesting (maybe non-ideal) edge case with the displacy defaults. To avoid very long dependency arcs for punctuation, the collapse_punct setting defaults to True. This means that when the visualisation is rendered, punctuation is merged onto the preceding token. Punctuation is identified by checking whether the token's is_punct attribute returns True – which also happens to be the case for "":)"".

In your example, you can work around this by setting collapse_punct to False in the options passed to displacy.serve:

displacy.serve(doc, style='dep', options={'collapse_punct': False})
(The displaCy visualizer should probably include an exception for emoticons when merging punctuation. This is currently difficult, because spaCy doesn't have an is_emoji or is_symbol flag. However, it might be a nice addition in the future – you can vote for it on this thread.)",,,,,,
7,Spacy,Ease of use,easy,Ease of use aspect,55087101,How to get all noun phrases in Spacy(Python),"I would like to extract ""all"" the noun phrases from a sentence. I'm wondering how I can do it. I have the following code:","Spacy clearly defines a noun chunk as:

A base noun phrase, or ""NP chunk"", is a noun phrase that does not permit other NPs to be nested within it – so no NP-level coordination, no prepositional phrases, and no relative clauses."" (https://spacy.io/api/doc#noun_chunks)
If you process the dependency parse differently, allowing prepositional modifiers and nested phrases/chunks, then you can end up with what you're looking for.

I bet you could modify the existing spacy code fairly easily to do what you want:

https://github.com/explosion/spaCy/blob/06c6dc6fbcb8fbb78a61a2e42c1b782974bd43bd/spacy/lang/en/syntax_iterators.py",,,,,,
8,Spacy,Ease of use,complex,Ease of use aspect,65850018,processing text with spacy nlp.pipe,"I'm procerssing 40,000 abstracts with spacy nlp.pipe using the code below and its taking 8 mins. Is there a way to speed this up further? I've also disabled ner.","Try tuning batch_size and n_process params :
Note as well, by joining on "" "" you may have some surprises, as spaCy's splitting rules are a bit more complex than that.",,,,,,
9,Spacy,Documentation,documentation,Documentation aspect,52855178,Discrepancy documentation and implementation of spaCy vectors for German words?,"According to documentation:

spaCy's small models (all packages that end in sm) don't ship with word vectors, and only include context-sensitive tensors. [...] individual tokens won't have any vectors assigned.
But when I use the de_core_news_sm model, the tokens Do have entries for x.vector and x.has_vector=True.

It looks like these are context_vectors, but as far as I understood the documentation only word vectors are accessible through the vector attribute and sm models should have none. Why does this work for a ""small model""?","has_vector behaves differently than you expect.

This is discussed in the comments on an issue raised on github. The gist is, since vectors are available, it is True, even though those vectors are context vectors. Note that you can still use them, eg to compute similarity.

Quote from spaCy contributor Ines:

We've been going back and forth on how the has_vector should behave in cases like this. There is a vector, so having it return False would be misleading. Similarly, if the model doesn't come with a pre-trained vocab, technically all lexemes are OOV.
Version 2.1.0 has been announced to include German word vectors.",,,,,,
10,Spacy,Documentation,documentation,Documentation aspect,48200524,Named entity recognition in Spacy,"I am trying to find Named entities for a sentence as below

import spacy.lang.en
parser = spacy.lang.en.English()
ParsedSentence = parser(u""Alphabet is a new startup in China"")
for Entity in  ParsedSentence.ents:  
    print (Entity.label, Entity.label_, ' '.join(t.orth_ for t in Entity))
I am expecting to get the result ""Alphabet"",""China"" but I am getting an empty set as result. What am I doing wrong here","As per spacy documentation for Name Entity Recognition here is the way to extract name entity

import spacy
nlp = spacy.load('en') # install 'en' model (python3 -m spacy download en)
doc = nlp(""Alphabet is a new startup in China"")
print('Name Entity: {0}'.format(doc.ents))
Result
Name Entity:  (China,) ",,,,,,
11,Spacy,Documentation,documentation,Documentation aspect,58215855,"How to get full list of POS, Tag, and Dep in spaCy?","The POS, TAG, and DEP values used in spaCy are common ones of NLP, but I believe there are some differences depending on the corpus database.

For example, Universal Dependencies Contributors has listed 37 syntactic dependencies. Does spaCy use all of these 37 dependencies? And nothing more?

Is there a command to output all POS, TAG, and DEP values, spaCy may provide?","A specific pipeline component can show its labels:

nlp = spacy.load('en')
nlp.get_pipe(""tagger"").labels
# ('$', ""''"", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``')
spacy.explain(label) will give a short definition for most labels, e.g.:

spacy.explain(""NN"")
# 'noun, singular or mass'
spacy.explain(""PERSON"")
# 'People, including fictional'
The detailed documentation is here: https://spacy.io/api/annotation",,,,,,
12,Spacy,Documentation,documentation,Documentation aspect,50189238,How to re-train an existing spacy NER model for currency,"I am trying to update an existing spacy model ""en_core_web_sm"" with some different country currency such as ""euro"", ""rupees"", ""eu"", ""Rs."", ""INR"" etc. How can I achieve that ? The spacy tutorial didn't quite help me as training a fixed string such as ""horses"" as ""ANIMAL"" seems different than my requirements. The reason is I can have currency value indifferent formats : ""1 million euros"", ""Rs. 10,000"", ""INR 1 thousand"" etc. My sample dataset contains around 1000 samples with the following format :

TRAIN_DATA = [      
 ("" You have activated International transaction limit for Debit Card ending XXXX1137 on 2017-07-05 12:48:20.0 via NetBanking. The new limit is Rs. 250,000.00"", {'entities' : [(140, 154, 'MONEY')] }),...
]
Can anyone please help me out with this with the data format, training size or any other relevant information ?","The example from the documentation should work for you. I altered it a little to match your variable name.

optimizer = nlp.begin_training()

for itn in range(100):
    random.shuffle(train_data)
    for raw_text, entity_offsets in TRAIN_DATA:
        doc = nlp.make_doc(raw_text)
        gold = GoldParse(doc, entities=entity_offsets)
        nlp.update([doc], [gold], drop=0.5, sgd=optimizer)
nlp.to_disk('/model')
Link to Documentation",,,,,,
13,Spacy,Documentation,documentation,Documentation aspect,74550422,Concatenate two spacy docs together?,"How do I concatenate two spacy docs together? To merge them into one?

import spacy

nlp = spacy.load('en')
doc1 = nlp(u'This is the doc number one.')
doc2 = nlp(u'And this is the doc number two.')
new_doc = doc1+doc2
Of course that will return an error as a doc object is not concatenable by default. Is there a straightforward solution to do that?

I looked at this: https://github.com/explosion/spaCy/issues/2229 The issue seems closed so it sounds like they have implemented a solution but I cannot find a simple example of that being used.","What about this:
For ""a lot"" of different sentences, it might be better to use nlp.pipe as shown in the documentation.

Hope it helps.",,,,,,
14,Spacy,Documentation,documentation,Documentation aspect,57206701,Spacy tokenizer rule for exceptions that contain whitespace?,"When I create a pipeline with the default tokenizer for say English, I can then call the method for adding a special case:

tokenizer.add_special_case(""don't"", case)
The tokenizer will happily accept a special case that contains whitespace:

tokenizer.add_special_case(""some odd case"", case)
but it appears that does not actually change the behavior of the tokenizer or will never match?

More generally, what is the best way of extending an existing tokenizer so that the some patterns which normally would result in multiple tokens only create one token? For example something like [A-Za-z]+\([A-Za-z0-9]+\)[A-Za-z]+ should not result in three tokens because of the parentheses but in a single token, e.g. for asdf(a33b)xyz while the normal English rules should still apply if that pattern does not match.

Is this something that can be done somehow by augmenting an existing tokenizer or would I have to first tokenize, then find entities that match the corresponding token patterns and then merge the entity tokens?","As you found, Tokenizer.add_special_case() doesn't work for handling tokens that contain whitespace. That's for adding strings like ""o'clock"" and "":-)"", or expanding e.g. ""don't"" to ""do not"".

Modifying the prefix, suffix and infix rules (either by setting them on an existing tokenizer or creating a new tokenizer with custom parameters) also doesn't work since those are applied after whitespace splitting.

To override the whitespace splitting behavior, you have four options:

Merge after tokenization. You use Retokenizer.merge(), or possibly merge_entities or merge_noun_chunks. The relevant documentation is here: https://spacy.io/usage/linguistic-features#retokenization and https://spacy.io/api/pipeline-functions#merge_entities and https://spacy.io/api/pipeline-functions#merge_noun_chunks

This is your best bet for keeping as much of the default behavior as possible.
Subclass Tokenizer and override __call__. Sample code:

Implement a completely new tokenizer (without subclassing Tokenizer). Relevant docs here: https://spacy.io/usage/linguistic-features#custom-tokenizer-example
Tokenize externally and instantiate Doc with words. Relevant docs here: https://spacy.io/usage/linguistic-features#own-annotations
To answer the second part of your question, if you don't need to change whitespace splitting behavior, you have two other options:

Add to the default prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizer-additions

Note from https://stackoverflow.com/a/58112065/594211: ""You can add new patterns without defining a custom tokenizer, but there's no way to remove a pattern without defining a custom tokenizer.""
Instantiate Tokenizer with custom prefix, suffix and infix rules. The relevant documentation is here: https://spacy.io/usage/linguistic-features#native-tokenizers

To get the default rules, you read the existing tokenizer's attributes (as shown above) or use the nlp object’s Defaults. There are code samples for the latter approach in https://stackoverflow.com/a/47502839/594211 and https://stackoverflow.com/a/58112065/594211.",,,,,,
15,Spacy,Documentation,documentation,Documentation aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",,,,,,
16,Spacy,Documentation,documentation,Documentation aspect,67198877,Cannot import biluo_tags_from_offsets from spacy.gold,"I am trying to import biluo_tags_from_offsets from spacy.gold but cannot do it. Getting ModuleNotFoundError.

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-21-d17a54331c7a> in <module>
      3 
      4 import spacy
----> 5 from spacy.gold import biluo_tags_from_offsets
      6 nlp = spacy.load(""en_core_web_lg"")
      7 

ModuleNotFoundError: No module named 'spacy.gold'
How can I use this spacy.gold module. I have already installed spacy. I am an amateur, sorry if this question does not make any sense. Any kind of help is appreciated. Thanks!","As the documentation says, spacy.gold was disabled in spaCy 3.0. If you have the latest spaCy version, that is why you are getting this error.

You need to replace from spacy.gold import biluo_tags_from_offsets with from spacy.training import offsets_to_biluo_tags.",,,,,,
17,Spacy,Documentation,documentation,Documentation aspect,61241351,"Install specific version of spacy - working with pip, but not with conda","I am revisiting some code I wrote months ago with Spacy Version 2.1.8. I need this version of spacy. pip install is working but Anaconda does not even show that spacy is installed. And

conda install spacy=2.1.8
gives the PackagesNotFoundError error.","As explained in spaCy documentation, you can take advantage of the conda-forge community repository to install spacy

$ conda install -c conda-forge spacy=2.1.8
This version should be available according to spaCy's page on Conda Forge.",,,,,,
18,Spacy,Documentation,documentation,Documentation aspect,68083466,How to use spacy train to add entities to an existing custom NER model? (Spacy v3.0),"I am currently implementing a custom NER model interface where a user can interact with a frontend application to add custom entities to train a spacy model.

I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.

For example, let's say I have a model that is already trained for a custom entity of FOOD. (Pizza, Pasta, Bread, etc…). Now I want to take this existing model, and train it for a new entity called DRINKS with keywords like Coca-Cola, Pepsi, Juice, etc… Using spacy train command for spacy v3.0.

The spacy train command that I am using currently is as follows:

> python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./train.spacy
I load the model for prediction using:

> nlp1 = spacy.load(R"".\output\model-best"")
As of now, I was training the model for new entities manually. Below is the code to find keywords in my training data and output a JSON format for training data (old format).","""I want to use spacy train (CLI) to take an existing model (custom NER model) and add the keyword and entity specified by the user, to that model. (Instead of training the whole model again). I can't find this anywhere in the documentation.""
What you are describing is called ""online learning"" and the default spaCy models don't support it. Most modern neural NER methods, even outside of spaCy, have no support for it at all.

You cannot fix this by using a custom training loop.

Your options are to use rule-based matching, so you can only match things explicitly in a list, or to retrain models on the fly.

Rule-based matching should be easy to set up but has the obvious issue that it can't learn things not explicitly in the list.

Training things on the fly may sound like it'll take too long, but you can train a small model quite quickly. What you can do is train a small model for a small number of iterations while the user is working interactively, and after they've confirmed the model is more or less working correctly you can use the same training data for a larger model with longer training.",,,,,,
19,Spacy,Performance,performance,Performance aspect,58724562,When creating a Doc using the standard constructor the model is not loaded ,"I'm trying to use SpaCY and instantiate the Doc object using the constructor:
but when I do that, if I try to use the dependency parser:
I get the error:

ValueError: [E029] noun_chunks requires the dependency parse, which requires a statistical model to be installed and loaded. For more info, see the documentation:
While if I use the method nlp(""Hello world!"") that does not happens.

The reason I do that, is because I use the entity extraction from a third party application I want to supply to SpaCy my tokenisation and my entities.
What should I do? load the pipeline by myself in the document, and exclude the tokeniser for example?","nlp() returns a Doc where the tokenizer and all the pipeline components in nlp.pipeline have been applied to the document.

If you create a Doc by hand, the tokenizer and the pipeline components are not loaded or applied at any point.

After creating a Doc by hand, you can still apply individual pipeline components from a loaded model:

nlp = spacy.load('en_core_web_sm')
nlp.tagger(doc)
nlp.parser(doc)
Then you can add your own entities to the document. (Note that if your tokenizer is very different from the default tokenizer used when training a model, the performance may not be as good.)",,,,,,
20,Spacy,Performance,performance,Performance aspect,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",,,,,,
21,Spacy,Performance,performance,Performance aspect,48169545,Does spacy take as input a list of tokens?,"I would like to use spacy's POS tagging, NER, and dependency parsing without using word tokenization. Indeed, my input is a list of tokens representing a sentence, and I would like to respect the user's tokenization. Is this possible at all, either with spacy or any other NLP package ?

For now, I am using this spacy-based function to put a sentence (a unicode string) in the Conll format:
I would like to do the same while having as input a list of tokens...","You can run Spacy's processing pipeline against already tokenised text. You need to understand, though, that the underlying statistical models have been trained on a reference corpus that has been tokenised using some strategy and if your tokenisation strategy is significantly different, you may expect some performance degradation.

Here's how to go about it using Spacy 2.0.5 and Python 3. If using Python 2, you may need to use unicode literals.",,,,,,
22,Spacy,Performance,performance,Performance aspect,57049798,Can we find sentences around an entity tagged via NER?,"We have a model ready which identifies a custom named entity. The problem is if the whole doc is given then the model does not work as per expecation if only a few sentences are given, it is giving amazing results.

I want to select two sentences before and after a tagged entity.

eg. If a part of the doc has world Colombo(which is tagged as GPE), I need to select two sentences before the tag and 2 sentences after the tag. I tried a couple of approaches but the complexity is too high.

Is there a built-in way in spacy with which we can address this problem?

I am using python and spacy.

I have tried parsing the doc by identifying the index of the tag. But that approach is really slow.","It might be worth it to see if you can improve the custom named entity recognizer, because it should be unusual for extra context to hurt performance and potentially if you fix that issue it will work better overall.

However, regarding your concrete question about surrounding sentences:

A Token or a Span (an entity is a Span) has a .sent attribute that gives you the covering sentence as a Span. If you look at the tokens right before/after a given sentence's start/end tokens, you can get the previous/next sentences for any token in a document.",,,,,,
23,Spacy,Performance,performance,Performance aspect,72413618,How can I make it so that I process each sentence to look for and replace with a synonym for the words that match?,"I am currently working with spacy and have a corpus (containing 960,256 words) that looks like this:
I have a function that looks for the synonym of a word (using spacy):

Which returns an array of answers like so:
What I want is to grab the corpus, feed it sentence by sentence and word by word to most_similar so I can save the list of words to replace and do so by using replace_wordthe thing is that I'm not sure how to do this. I've tried for a while but it always fails somehow (either won't take batches so I can't do it at once, the words end up being empty vectors if I simply split each sentence by .split("" "") so...could you help me out please?","I hope I understood what you need correctly. I'm guessing you want to:

Iterate over a corpus
Find specific tokens using the matcher
Find synonyms of the matched tokens
Return a new list of sentences but with the replaced tokens.
If that's the case then what you need is a valid similarity function (I tried the one above but it didn't work for me properly) but you can try this:
You also mentioned that you want this to run on a corpus. I recommend that you use the nlp.pipe() method for performance gains combined with the set_extension method. You can do it like this:",,,,,,
24,Spacy,Performance,performance,Performance aspect,66433304,spacy- why nlp() works for single string while nlp.pipe() works fine for a list of strings?,"I recently ran into a strange behavior while using spacy, which is when I process string,

in case the string is a single string object, I have to use nlp(string),

while I have to use nlp.pipe(a list) for a list made of strings elements.
Why is this? I assume this must be something to do with nlp.pipe() behavior which is generator.

What is the reason?","Spacy does this because generators are more efficient. Since generators are consumed only once they are more memory efficient than a list.

According to their documentation instead of processing texts one-by-one and applying nlp pipeline it processes texts in batches.

Furthermore, you can configure batch size in nlp.pipe to optimize performance according to your system

Process the texts as a stream using nlp.pipe and buffer them in batches, instead of one-by-one. This is usually much more efficient.
If your goal is to process large streams of data using nlp.pipe it would be much more efficient to write a streamer/generator to produce results as you need them from database/filesystem than loading everything in memory and then processing them one by one.",,,,,,
25,Spacy,Performance,fast,Performance aspect,58212589,How to check if a sentence is a question with spacy?,"I am using spacy library to build a chat bot. How do I check if a document is a question with a certain confidence? I know how to do relevance, but not sure how to filter statements from questions.

I am looking for something like below:

spacy.load('en_core_web_lg')('Is this a question?').is_question","My first response is to suggest looking for question marks at the end of the sentence.

Otherwise, most questions start with {is, does, do, what, when, where, who, why, what, how}.

There is a more complex answer involving the inclusion of auxiliary verbs and their placement relative to the verb, but if your data is well-formed, this may be sufficient (and fast).",,,,,,
26,Spacy,Performance,fast,Performance aspect,59830899,sense2vec performance improvemets,"I am using Sense2Vec (builds upon Spacy, a fantastic NLP library/tool) and testing the similarity between various words. Unfortunately the performance is quite slow (execution is over a second on a regular MacBook Pro).

Does anyone have a way to speed this up?

Here is the code:

term = ""dog|NOUN""
sense2vec = Sense2Vec().from_disk(""path-to-s2v_reddit_2019_lg"")
sims = sense2vec.most_similar(term, n=top)
I suspect some caching would speed things, but I am not sure which elements should be cached.
","There is indeed a way to speed up Sense2Vec.most_similar. There is a script available from https://github.com/explosion/sense2vec/blob/master/scripts/06_precompute_cache.py, which precomputes a cache of the nearest neighbors. It will then save that cache with the component on disk, making the data larger but the queries fast. If a query is not covered by the cache, most_similar falls back to using the normal computation.",,,,,,
27,Spacy,Performance,fast,Performance aspect,48199353,How to use spacy in large dataset with short sentences efficiently?,"I choose spacy to process kinds of text because of the performance of it's lemmatation compared with nltk. But When I process millions short text, it always consumed all of my memory(32G) and crashed. Without it just a few minutes and less than 10G mem is consumed.

Is something wrong with the usage of this method? is there any better solution to improve the performance? Thanks!","You can use multithreading in spacy to create a fast tokenization and data ingestion pipeline.

Rewriting your code block and functionality using the nlp.pipe method would look something like this:
This way puts all your filtering into the token_filter function, which takes in a spacy token and returns True only if it is not punctuation, a space, a stopword, and 4 or less characters. Then, you use this function as you pass through each token in each document, where it will return the lemma only if it meets all of those conditions. Then, filtered_tokens is a list of your tokenized documents.

Some helpful references for customizing this pipeline would be:

Token attributes
Language.pipe",,,,,,
28,Spacy,Performance,fast,Performance aspect,40615717,Spacy.io multithreading with custom pipelines,"I'm trying to use Spacy.io with custom pipelines for language processing, but it seems that when I add a custom function to the pipeline the process runs only on one thread. With default pipeline process uses all the threads specified.

This is how I defined pipeline:

nlp = spacy.load(language, create_pipeline=custom_pipeline)
This is custom_pipeline function:

def custom_pipeline(nlp):
    return (nlp.tagger, score_vocab_pipe)
This is how I run the pipeline:

nlp.pipe(texts, batch_size=1000, n_threads=8)
Are there any requisites for custom pipeline functions to support multithreading with spacy?","This post might give you a better understanding of how the multi-threading is implemented. The multi-threading is described in the docs here.

In short: the tagger doesn't currently release the GIL, so the tagger.pipe() method is just a generator that applies the tagger one-by-one. The tagger should be quite fast for most workloads with one thread per process, especially since it doesn't use much memory. You can see the recipe for multi-processing batch jobs here.

We could release the GIL around the tagger as well, to allow efficient multi-threading. If you'd like to work on this, we can talk about it on the tracker or the spaCy Gitter.",,,,,,
29,Spacy,Performance,fast,Performance aspect,69738938,How to use existing huggingface-transformers model into spacy?,"I'm here to ask you guys if it is possible to use an existing trained huggingface-transformers model with spacy.

My first naive attempt was to load it via spacy.load('bert-base-uncased'), it didn't work because spacy demands a certain structure, which is understandable.

Now I'm trying to figure out how to use the spacy-transformers library to load the model, create the spacy structure, and use it from that point as a normal spacy-aware model.

I don't know if it is even possible as I couldn't find anything regarding the subject. I've tried to read the documentation but all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place? I can believe someone has to train everything again with spacy.
","What you do is add a Transformer component to your pipeline and give the name of your HuggingFace model as a parameter to that. This is covered in the docs, though people do have trouble finding it. It's important to understand that a Transformer is only one piece of a spaCy pipeline, and you should understand how it all fits together.

To pull from the docs, this is how you specify a custom model in a config:

[components.transformer.model]
@architectures = ""spacy-transformers.TransformerModel.v3""
# XXX You can change the model name here
name = ""bert-base-cased""
tokenizer_config = {""use_fast"": true}
Going back to why you need to understand spaCy's structure, it's very important to understand that in spaCy, Transformers are only sources of features. If your HuggingFace model has an NER head or something it will not work. So if you use a custom model, you'll need to train other components, like NER, on top of it.

Also note that spaCy has a variety of non-Transformers built-in models. These are very fast to train and in many situations will give performance comparable to Transformers; even if they aren't as accurate, you can use the built-in models to get your pipeline configured and then just swap in a Transformer.

all guides, examples, and posts I found, start from a spacy structured model like spacy/en_core_web_sm, but how did that model was created in the first place?",,,,,,
30,Spacy,Performance,speed,Performance aspect,70925238,Sharing Spacy model between processes,"My code is using Python's multiprocessing for parallel computation. As part of the computation Spacy is used. Is it safe to create a single spacy object with nlp = spacy.load(""de_core_news_lg"") and access it by multiple processes for named entity recognition?","You can take advantange of multiprocessing with spaCy by passing the n_process argument to nlp.pipe. For example:

docs = [""This is the first doc"", ""this is the second doc""]

nlp = spacy.load(""en_core_web_sm"")  # use your model here

docs_tokens = []
for doc in nlp.pipe(docs, n_process=2):
    tokens = [t.text for t in doc]
    docs_tokens.append(tokens)
There's more about this in the spaCy documentation, as well as this Speed FAQ.",,,,,,
31,Spacy,Performance,speed,Performance aspect,52897492,spaCy: optimizing tokenization,"I'm currently trying to tokenize a text file where each line is the body text of a tweet:
The file is 59,397 lines long (a day's worth of data) and I'm using spaCy for pre-processing/tokenization. It's currently taking me around 8.5 minutes and I was wondering if there were any way of optimising the following code to be quicker as 8.5 minutes seems awfully long for this process:Although it says files, it's currently only looping over 1 file.

Just to note, I only need this to tokenize the content; I don't need any extra tagging etc.","It sounds like you haven't optimised the pipeline yet. You'll get a significant speed up from disabling the pipeline components you don't need, like so:

nlp = spacy.load('en', disable=['parser', 'tagger', 'ner'])    
This should get you down to about the two-minute mark, or better, on its own.

If you need a further speed up, you can look at multi-threading using nlp.pipe. Docs for multi-threading are here: https://spacy.io/usage/processing-pipelines#section-multithreading",,,,,,
32,Spacy,Performance,speed,Performance aspect,57697374,List most similar words in spaCy in pretrained model,"With Gensim, after I've trained my own model, I can use model.wv.most_similar('cat', topn=5) and get a list of the 5 words that are closest to cat in the vector space. For example:

With spaCy, as per the documentation, I can do:

which gives similarity for tokens in a specified string. But combing through the docs and searching, I can't figure out if there is a gensim-type way of listing all similar words for a preloaded model with either nlp = spacy.load('en_core_web_lg') or nlp = spacy.load('en_vectors_web_lg'). Is there a way to do this?","I used Andy's response and it worked correctly but slowly. To resolve that I took the approach below.

SpaCy uses the cosine similarity, in the backend, to compute .similarity. Therefore, I decided to replace word.similarity(w) with its optimized counterpart. The optimized method that I worked with was cosine_similarity_numba(w.vector, word.vector), shown below, that uses the Numba library to speed up computations. You should replace line 12 in the most_similar method with the line below.
The method became 2-3 times faster which was essential for me.
I explained it in more details in this article: How to Build a Fast “Most-Similar Words” Method in SpaCy",,,,,,
33,Spacy,Stability,bug,Stability aspect,40595828,"Retrieving the start and end character indices in the original document, for those sentences returned by Spacy","I am using something similar to the following pattern to retrieve the start and end indices of Spacy's sentences in the original document:
This seems to work for those few test docs I used. But worried if I am overlooking any 'gotchas' like spacy sometimes stripping off some characters that I am not aware of . Am I?

PS: If it helps, I need these indices to compare with indices I have from Brat's annotation file.","You should just be able to use the sent.start_char and sent.end_char attributes. These give exactly the indices you're after: https://spacy.io/docs/api/span#attributes

Also doc.text should always equal the original full text. If it doesn't, please submit a bug report.",,,,,,
34,Spacy,Stability,bug,Stability aspect,42215762,How to get Spacy to tokenize am/pm expressions correctly?,"Spacy seems unable to correctly tokenize am/pm expressions where the number and the am/pm part is not separated by whitespace.

Example: ""Something happened at 9am"" produces an unexpected token set: List of Token objects: [Something, happened, at, LIKE_URL, am]

While ""Something happened at 9 am"" works just fine: List of Token objects: [Something, happened, at, 9, am]

I notice that if you drill down into the Token object, the 'orth' representation of the token is a 9. Likewise, if I try '8am', the token reads 'IS_UPPER' and the 'orth' representation of the token is 8.

In particular this is a problem for me because I need to track token spans accross a wider context, and a token that shows up as ""8"" in the text but ""IS_UPPER"" in token form causes my spans to be off by 7. (IE the actual document-level index (token.idx) of the next token is off by 7)

How can I process a document with spacy that contains am/pm expressions sometimes denoted in the format described above, and retrieve accurate token spans?

I'm using Python 2.7.12 with Spacy 1.4.0","Turns out, this was reported as a bug about a month ago: https://github.com/explosion/spaCy/issues/736

Upgrading to Spacy 1.6.0 resolves the issue.",,,,,,
35,Spacy,Stability,bug,Stability aspect,52263757,Spacy is_stop doesn't identify stop words?,"When I use SpaCy to identify stopwords, it doesn't work if I use the en_core_web_lg corpus, but it does work when I use en_core_web_sm. Is this a bug, or am I doing something wrong?
However, when I change this line to use the en_core_web_smcorpus, I get different results:","The issue you have is a documented bug. The suggested workaround is the following:

",,,,,,
36,Spacy,Stability,bug,Stability aspect,62796437,Spacy French langage gives NoneType error,"Here is the code :

import spacy
nlp = spacy.load('fr_core_news_sm')
doc = nlp('Demain je travaille à la maison')
for token in doc:
    print(token.text)
It gives the error :

File ""c:\users\ab\appdata\local\programs\python\python37\lib\site-packages\spacy\lang\fr\lemmatizer.py"", line 49, in call if self.is_base_form(univ_pos, morphology): TypeError: 'NoneType' object is not callable
If I change 'fr_core_news_sm' to 'en_core_web_sm', it works well.

My python version is 3.7.7. And Spacy version is 2.3.1.","This seems to be a bug at spaCy 2.3.1: https://github.com/explosion/spaCy/issues/5728

Downgrade to 2.3.0 and it should work: pip install spacy==2.3.0",,,,,,
37,Spacy,Stability,bug,Stability aspect,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.",,,,,,
38,Spacy,Stability,bug,Stability aspect,64164360,How can I add a specific substring to tokenize on in spaCy?,"I am using spaCy to tokenize a string, and the string is likely to contain a specific substring. If the substring is present, I would like spaCy to treat the substring as a token, regardless of any other rules it has. I would like to keep all other rules intact. Is this possible?

To provide a concrete example, suppose the substring of interest is 'banana'; I want 'I like bananabread.' to be tokenized as ['I', 'like', 'banana', 'bread', '.'].

Where do I go from here (keeping in mind that I would like to keep the rest of the tokenizer rules intact)? I have tried adding 'banana' to the prefixes, suffixes, and infixes, with no success.","Adding the string as a prefix, suffix, and infix should work, but depending on which version of spacy you're using, you may have run into a caching bug while testing. This bug is fixed in v2.2+.

With spacy v2.3.2:

(In v2.1 or earlier, the tokenizer customization still works on a newly loaded nlp, but if you've already processed some texts with the nlp pipeline and then modify the settings, the bug was that it would use the stored tokenization from the cache rather than the new settings.)",,,,,,
39,Spacy,Stability,bug,Stability aspect,75724665,How to maximize performance of spaCy on an M1 Mac (currently much slower than Intel),"I've observed that nlp.pipe is 30-40% slower on my almost brand new M1 Pro Macbook than on my old Macbook Pro from 2017. Most other functions are faster on the M1 by a similar margin, so this is not the performance I would expect.

For a benchmark, I'm running the following code (with scispacy):

My M1 Mac takes over 75 seconds to complete the task, while my 2017 Intel Mac can do it in 46 seconds.

I don't know whether spacy uses numpy, but I installed a fast version of numpy using ABarrier's answer to this question. That made numpy faster, but made no difference for spacy. I'm assuming that somewhere there is an unoptimized binary being used, but I don't know how to figure out what it is.
","Install the package thinc-apple-ops:

pip install thinc-apple-ops
Or through the spacy extra:

pip install 'spacy[apple]'
(The unoptimized package is blis. We'd like to be able to switch to a newer version of BLIS with M1 support, but there are still some open bugs.)",,,,,,
40,Spacy,Stability,bug,Stability aspect,57109492,PhraseMatcher to match in a different token attribute,"We would like to match a set of phrases using PhraseMatcher. However we whould like to match not only on the verbatim text, but a normalized version of the input. For instance, lower case, with the accents removed, etc.

We have tried to add a custom attibute to the Token, and use it in the init of the PhraseMatcher to match it but, it did not work.

We could transform the text using a custom pipeline but we want to keep the original text to be able to use other components of spacy.
What is the way to do this?","Since EntityRuler is based on PhraseMatcher, I copy here a working example with Spacy v2.2.0. Follow the comments to understand how to work with ""NORM"" attribute from tokens.

At the end, you can see how the word ""FÁCIL"" matches the pattern ""facil"", since it has been normalized.
This bug was fixed in release v2.1.8 https://github.com/explosion/spaCy/issues/4002",,,,,,
41,Spacy,Stability,bug,Stability aspect,68559878,Force 'parser' to not segment sentences?,"Is there an easy way to tell the ""parser"" pipe not to change the value of Token.is_sent_start ?

So, here is the story: I am working with documents that are pre-sentencized (1 line = 1 sentence), this segmentation is all I need. I realized the parser's segmentation is not always the same as in my documents, so I don't want to rely on the segmentation made by it.

I can't change the segmentation after the parser has done it, so I cannot correct it when it makes mistakes (you get an error). And if I segment the text myself and then apply the parser, it overrules the segmentation I've just made, so it doesn't work.

So, to force keeping the original segmentation and still use a pretrained transformer model (fr_dep_news_trf), I either :

disable the parser,
add a custom Pipe to nlp to set Token.is_sent_start how I want,
create the Doc with nlp(""an example"")
or, I simply create a Doc with

doc = Doc(words=[""an"", ""example""], sent_starts=[True, False])
and then I apply every element of the pipeline except the parser.

However, if I still do need the parser at some point (which I do, because I need to know some subtrees), If I simply apply it on my Doc, it overrules the segmentation already in place, so, in some cases, the segmentation is incorrect. So I do the following workaround:

Keep the correct segmentation in a list sentences = list(doc.sents)
Apply the parser on the doc
Work with whatever syntactic information the parser computed
Retrieve whatever sentencial information I need from the list I previously made, as I now cannot trust Token.is_sent_start.
It works, but it doesn't really feel right imho, it feels a bit messy. Is there an easier, cleaner way I missed ?

Something else I am considering is setting a custom extension, so that I would, for instance, use Token._.is_sent_start instead of the default Token.is_sent_start, and a custom Doc._.sents, but I fear it might be more confusing than helpful ...

Some user suggested using span.merge() for a pretty similar topic, but the function doesn't seem to exist in recent releases of spaCy (Preventing spaCy splitting paragraph numbers into sentences)","The parser is supposed to respect sentence boundaries if they are set in advance. There is one outstanding bug where this doesn't happen, but that was only in the case where some tokens had their sentence boundaries left unset.

If you set all the token boundaries to True or False (not None) and then run the parser, does it overwrite your values? If so it'd be great to have a specific example of that, because that sounds like a bug.

Given that, if you use a custom component to set your true sentence boundaries before the parser, it should work.

Regarding some of your other points...

I don't think it makes any sense to keep your sentence boundaries separate from the parser's - if you do that you can end up with subtrees that span multiple sentences, which will just be weird and unhelpful.

You didn't mention this in your question, but is treating each sentence/line as a separate doc an option? (It's not clear if you're combining multiple lines and the sentence boundaries are wrong, or if you're passing in a single line but it's turning into multiple sentences.)

Thanks a million for you answer! It made me go back to my documents and check every output of every element in the pipeline... Well, I feel terribly stupid, but I simply cannot reproduce my observation of a ""deffective parser"", everything seems fine now. I guess after my initial struggle with forcing my own sentence boundaries (which wasn't even actually that complicated ...), I convinced myself somehow it wasn't working even after my own boundaries were set ? Or I fixed something else in my code ? Terribly sorry. I'll keep an eye on it, might post here again if I find a mistake again",,,,,,
42,Spacy,Stability,bug,Stability aspect,62737680,Spacy tagger loss is zero while training,"I use this snippet of code to train a tagger in spacy 2.3.0.
The problem is that the loss value is always zero. What am I doing wrong?","Sorry, this is a bug in v2.3.0. It will be fixed in the upcoming v2.3.1. You can train a tagger with spacy train instead or use v2.2.4 in the meanwhile.

If you'd like to have this fix sooner, you can also install from source in the current master branch (the fix is in commit b7107ac8).",,,,,,
43,Spacy,Stability,backwards compatibility,Stability aspect,54201004,Multithreading with spacy: Is joblib necessary?,"In this part of the documentation, it is mentioned that nlp.pipe() works in parallel and the following example is given:

for doc in nlp.pipe(texts, batch_size=10000, n_threads=3):
    pass
After that, another, longer example is given that makes use of joblib. I don't quite understand the relationship between the two. As I understand the documentation, if I simply want to parallelize the tokenisation of many documents, the above, simple for loop will work and I do not have to use joblib, right?

My pipeline looks like this:

nlp = spacy.load('en', disable=['parser', 'ner', 'textcat'])
When would I need to use joblib?","Based on an answer in Spacy github issues:

We kept the n_threads argument to avoid breaking people's code, but unfortunately the implementation doesn't currently release the GIL, the way we did in v1. In v2 the neural network model is more complicated and more subject to change, so we haven't implemented it in Cython. We might at a later date.

In v2.1.0 (you can get an alpha by installing spacy-nightly, the matrix multiplications are now single-threaded. This makes it safe to launch multiple processes for the pipeline, so we can look at doing that internally. In the meantime, the n_threads argument sits idle...Which I agree is confusing, but removing it and breaking backwards compatibility seems worse.
Thus, to summarize: n_threads doesn't work in v2.1. What I'm doing now is using Spacy with joblib to read a dataset in minibatches.

Spacy released an example for that: Spacy Multiprocessing, and it works perfectly.

I have a dataset with almost ~4M short text. Without using the example they released, it took almost 23 hours to finish parsing them, but using joblib with spacy, it took 1 hour and half to finish!

To reference the readers of this question to Spacy Multiprocessing example: Spacy Multiprocessing",,,,,,
44,Spacy,Stability,backwards,Stability aspect,52598788,spaCy: custom attributes not matching correctly?,"I have been having problems using custom extension attributes with the recently improved Matcher (spaCy 2.012). Even a simple example (mostly copied from here) is not working as I expected:
In other words, the rule correctly matches on the span 'have' (1, 2), but incorrectly matches 'have' (5, 6) and 'had' (6, 7). Furthermore, the callback function is not called. The custom attribute appears to be ignored.
The first rule functions as above. Then the second rule triggers, along with the callback function (which prints the message). There is an additional correct match for the new pattern along with the correct and erroneous matches from the first rule.

So, I have a few questions:

why does pattern1 match incorrectly? (i.e. why does the _ custom attribute constraint not apply?)
why does the callback function not work on the first call?
why does it work upon addition of a new rule?
In my own code, when using custom attributes as constraints in subsequent patterns, these patterns match on ALL tokens. I assume this is related to the behaviour exhibited by the code above.","Sorry if this was confusing – but the GitHub thread you're referring to is still only the spec and proposal, i.e. the planned implementation. The changes will hopefully ship with spaCy v2.1.0 (since some of the changes to the Matcher internals are not fully backwards compatible).

While the custom attribute matching isn't implemented yet, the basic improvements to the Matcher engine are already available on the develop branch and in the alpha version via spacy-nightly (pip install spacy-nightly). Those updates likely also resolve the inconsistent behaviour you observed with the callback function.",,,,,,
45,Spacy,Stability,backwards,Stability aspect,66503956,"spaCy 3.0, installed files are different from the files in github repo","I installed spaCy 3.0 on my ubuntu. I use ctrl+B to find the definition of class ""sentencizer"" which is in sentencizer.py file:
Why there is no content in the functions defined in sentencizer.py. In the spaCy github repo, there is no sentencizer.py file and the class ""sentencizer"" is defined in sentencizer.pyx:
Why the installed files are different from the github repo? Thanks!","When developing a Python library, small changes are saved in Git as they're made, but they're only released to PyPI when the maintainer intentionally makes a release. So it's normal for the files on your computer to be a little different from the files in a git repo, even if you have a very recent release.

I was really confused about the sentencizer.py code you posted, since there doesn't seem to have ever been a file with that name in spaCy, but it looks like that is a magic PyCharm feature - it's not showing you the actual source code, it's doing some sort of decompilation.

You noticed spaCy has the sentencizer.pyx file. That's compiled into a binary .so file that Python runs when you use the code. PyCharm is presumably working backwards from the .so file.",,,,,,
46,Spacy,Stability,version,Stability aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",,,,,,
47,Spacy,Stability,version,Stability aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",,,,,,
48,Spacy,Stability,version,Stability aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server – this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release – so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",,,,,,
49,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,51037475,spaCy language module not downloading,"I am working on Ubuntu 16.04. I downloaded spaCy using conda and am working on jupyter notebook. I tried

python -m spacy download en
which gives the following error:

/home/nikita/anaconda3/bin/python: No module named spacy.main; 'spacy' is a package and cannot be directly executed

So I tried

python -m spacy.en.download
which gives these errors:

Then I tried

python3 -m spacy.en.download --force all
which gives the same errors as the 2nd one. Please help!","For some reason, conda seems to have installed an older version of spaCy. To make sure you're installing the latest version (or any other specific one), you can use the following command:

conda install -c conda-forge spacy=2.0.11
(Disclaimer: I'm one of the spaCy maintainers. We're still investigating why this happens to some users, and whether it's related to how dependencies are resolved, an old Anaconda distribution that ships with spaCy 0.101.0 or something entirely different. The relevant thread on the issue tracker is here.)",,,,,,
50,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,46826541,Methods for creating training data for SpaCy models?,"I recently began a NLP journey using SpaCy, and I have ~5,500 strings which I want to label up. For the first 100, I did this using a spreadsheet with custom columns, which was then run through a script to generate Python dictionaries. In the sheet, I have strored the string, label type, label value. The script then works out the position of the label value from within the string.

It's rather time consuming to product training data in this way, and it's open to error.

Are there any tools available to assist with this? I literally just need the ability to highlight a substring, and then choose the label type. I could build it myself, but I feel it may already exist.","I'm one of the maintainers of spaCy and we've actually been thinking about this problem a lot! So we've built Prodigy, an annotation tool that integrates with spaCy and puts the model in the loop to help you train and evaluate models faster. It's currently in beta, but you can sign up for a free invite. Prodigy takes a slightly different approach to the click-drag-highlight-select concept of other annotation tools. It uses the model in the loop to suggest annotations with the most relevant gradient for training, and only asks you for a simple binary feedback: accept or reject. This lets you move through examples quickly. As you annotate, the model in the loop is updated, and its predictions will influence what Prodigy asks next.

This works especially well if you're looking to improve existing entity types present in your spaCy model, or if you're working with a large corpus of example text you want to use for annotation.

If you're looking for a tool more specifically for highlighting and annotating spans of text, you should also check out Brat. I'm not 100% sure what the output looks like, but you should definitely be able to convert it to spaCy's training format. There's also a trainable version of the displaCy ENT visualizer, developed by someone from the community.",,,,,,
51,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44980966,Spacy model download issue,"I am using Spacy 1.3

However, it looks like I cannot download the model now.

20/29 : RUN python -m spacy.en.download all
The command '/bin/sh -c python -m spacy.en.download all' returned a non-zero code","There seems to be a problem with the download server – this will be fixed asap. (I'm one of the spaCy maintainers btw. Sorry about the inconvenience.)

All models are also attached as archives to the v1.6 release – so in the meantime, you can always download them manually from there, unzip the archive and place the contained folder in spacy/data.

If you don't have to use v1.3, I'd also recommend checking out the newer versions and upgrading to spaCy v1.7+. Models are now hosted on GitHub, which makes the downloading process more transparent and doesn't rely on a separate download server. They're also wrapped as native Python packages, which lets you install them via pip, add them to your project's requirements.txt and even import them as a module at the top of your file. You can read more about this in the models documentation.

This makes it easier to manage model dependencies, especially as more models become available. If you're using spaCy 1.8+, you'll be able to use models for English, German, French and Spanish. If you have trained your own models and decide to upgrade, note that you will have to retrain your models with the input from the new version. Models trained on spaCy <v1.7 are not compatible with v1.7+.",,,,,,
52,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,55852115,Token extension versus matcher versus phrase matcher vs entity ruler in spaCy,"I am trying to figure out the best way (fast) to extract entities, e.g. a month. I have come up with 5 different approaches using spaCy.

Initial setup

For each solution I start with an initial setup
Conclusion

The custom attributes with is limited to single token matching and the token matcher seems to be faster so that seems to be preferable. The EntityRuler seems to be the slowest which isn't surprising since it is changing the Doc.ents. It is however quite convenient that you have your matches in Doc.ents so you might want to consider this method still.

I was quite surprised that the token matcher outperforms the phrase matcher. I thought it would be opposite:

If you need to match large terminology lists, you can also use the PhraseMatcher and create Doc objects instead of token patterns, which is much more efficient overall
Question

Am I missing something important here or can I trust this analysis on a larger scale?","I think ultimately, it all comes down to finding the optimal tradeoff between speed, maintainability of the code and the way this piece of logic fits into the larger picture of your application. Finding a few strings in a text is unlikely to be the end goal of what you're trying to do – otherwise, you probably wouldn't be using spaCy and would stick to regular expressions. How your application needs to ""consume"" the result of the matching and what the matches mean in the larger context should motivate the approach you choose.

As you mention in the conclusion, if your matches are ""named entities"" by definition, adding them to the doc.ents makes a lot of sense and will even give you an easy way to combine your logic with statistical predictions. Even if it adds slightly more overhead, it'll likely still outperform any scaffolding you'd otherwise have to write around it yourself.",,,,,,
53,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,44176829,Python Spacy error: RuntimeError: Language not supported,"I am going to add new entities to the own spacy data model ""mymodel"". Before I have installed ""mymodel"" using this tutorial and it was working good. When I want to use ""mymodel"" to add new entities I have a misunderstanding. Please, help me
","The problem here is that spacy.load() currently expects either a language ID (e.g. 'en'), or a shortcut link to a model that tells spaCy where to find the data. Because spaCy can't find a shortcut link, it assumes that 'my_model' is a language, which obviously doesn't exist.

You can set up a link for your model like this:

python -m spacy link my_model my_model # if it's installed via pip, or:
python -m spacy link /path/to/my_model/data my_model
This will create a symlink in the /spacy/data directory, so you should run it with admin permissions.

Alternatively, if you've created a model package that can be installed via pip, you can simply install and import it, and then call its load() method with no arguments:

import my_model
nlp = my_model.load()
In some cases, this way of loading models is actually more convenient, as it's cleaner and lets you debug your code more easily. For example, if a model doesn't exist, Python will raise an ImportError immediately. Similarly, if loading fails, you know there's likely a problem with the model's own loading and meta.

Btw: I'm one of the spaCy maintainers and I agree that the way spacy.load() currently works is definitely unideal and confusing. We're looking forward to finally changing this with the next major release. We're very close to releasing the first alpha of v2.0, which will solve this problem more elegantly, and will also include a lot of improvements to the training process and documentation.",,,,,,
54,Spacy,Active Maintenance,Maintain,Active Maintenance aspect,72068918,"Python NLP Spacy : improve bi-gram extraction from a dataframe, and with named entities?","I am using Python and spaCy as my NLP library, working on a big dataframe that contains feedback about different cars, which looks like this:

enter image description here

'feedback' column contains natural language text to be processed,
'lemmatized' column contains lemmatized version of the feedback text,
'entities' column contains named entities extracted from the feedback text (I've trained the pipeline so that it will recognise car models and brands, labelling these as 'CAR_BRAND' and 'CAR_MODEL')
I then created the following function, which applies the Spacy nlp token to each row of my dataframe and extract any [noun + verb], [verb + noun], [adj + noun], [adj+ proper noun] combinations.
This is where I have a problem...

This is producing only one bigram per row maximum. How can I tweak the code so that more than one bigram can be extracted and put in a column? (Also are there more linguistic combinations I should try?)
Is there a possibility to find out what people are saying about 'CAR_BRAND' and 'CAR_MODEL' named entities extracted in the 'entities' column? For example 'Cool Porsche' - Some brands or models are made of more than two words so it's tricky to tackle.
I am very new to NLP.. If there is a more efficient way to tackle this, any advice will be super helpful! Many thanks for your help in advance.","spaCy has a built-in pattern matching engine that's perfect for your application – it's documented here and in a more extensive usage guide. It allows you to define patterns in a readable and easy-to-maintain way, as lists of dictionaries that define the properties of the tokens to be matched.

Some ideas for improvement

Named entity recognition should be able to detect multi-word expressions, so brand and/or model names that consist of more than one token shouldn't be an issue if everything is set up correctly
Matching dependency patterns instead of linear patterns might slightly improve your results
That being said, what you're trying to do – kind of sentiment analysis -is quite a difficult task that's normally engaged with machine learning approaches and heaps of training data. So don't expect too much from simple heuristics.
",,,,,,
55,Spacy,Active Maintenance,github,Active Maintenance aspect,58327580,How to add new lemma rule to existing language for spacy,"I want to add a new lemmatiser rule for an existing language, i.e. lemmatise all nouns ending with ""z"" to ending with """".

In the case of individual words, spaCy gives the opportunity to add a tokeniser exception to an existing language after loading using

nlp.tokenizer.add_special_case(""adidas"", [{ORTH: 'adidas', LEMMA: 'Adidas', POS: 'NOUN', TAG: 'NNP'}])
The above sets the lemma, pos and tag of the new word and this is not altered. The default English lemmatiser returned ""adida"" as the lemma.

Now, I am trying to ""lemmatise"" nouns ""wordz"" to ""word"", ""windowz"" to ""window"" etc without setting all cases as exceptions but rather add a new rule: Noun ending with ""z"" has lemma the noun without the trailing ""z"".

I understand that it will depend on the tagger output as the rules that exist in _lemma_rules.py are pos dependent.

Is there a way to add the rule without creating a new language as a copy of an existing with just one modified file?","Since my question was very specific, I had to communicate with the spaCy developer team and got a working answer.

Actually it is does not work for the fake example in English but it works in real case scenario while using the Greek models as Greek lemmatisation is mainly rule based.

The proposed solution is to use the Lookups Api, which is only available in versions 2.2 and later.

As they mention,

nlp.vocab.lookups.get_table(""lemma_rules"") 
returns a dict-like table that you can write to.

Full answer in spaCy GitHub",,,,,,
56,Spacy,Active Maintenance,github,Active Maintenance aspect,51855137,Rasa NLU: Tensorflow embedding with entity extraction,"Tensorflow embedding has a very good intent recogition.

In my experience it improved test results from about 45% confidence to 95% for simple inputs such as ""Hello"" compared to spacy.

But out of the box it does not have any entity extraction.

Is there any Pipeline configuration to solve this?","Config

The configuration below contains ner_crf for entity extraction.
Version

Rasa NLU: >=0.13.0

credit: github issue",,,,,,
57,Spacy,Active Maintenance,github,Active Maintenance aspect,46765198,Unable to load spacy English model - 'WindowsPath' object has no attribute 'read',"I installed spacy using pip and then downloaded the English model using  
$ python -m spacy download en which after downloading gave me the message

You can now load the model via spacy.load('en')
I have the English model files(en_core_web_sm) downloaded to the working directory, am I missing something? Do I need to set a path variable? Any help is much appreciated, thanks!",If anybody else receives this error : I opened this as an issue with spaCy's developers on Github. I was suggested using Python 3.6 instead of 2.7 for the moment as there is no alternate workaround to the problem. The next spaCy version should cover this bugfix (I'm told).,,,,,,
58,Spacy,Active Maintenance,github,Active Maintenance aspect,53728770,Cannot install es_core_news_sm from Spacy,"When I try to install es_core_news_sm with this commmand

python -m spacy download es_core_news_sm
with conda I get this error

No module name spacy._main_;'spacy'is a package and cannot be directly executed. 
Thank you so much!","This could happen if conda installed an older version of spaCy. See the following issue on spaCy's github.

The fix as stated in the issue is as follows:

Find the most recent version of spaCy on conda:conda search -f spacy
Then run: conda install spacy=NEWEST_VERSION
The newest version of spaCy on Conda is 2.0.18",,,,,,
59,Spacy,Active Maintenance,github,Active Maintenance aspect,61943409,Spacy's BERT model doesn't learn,"I've been trying to use spaCy's pretrained BERT model de_trf_bertbasecased_lg to increase accuracy in my classification project. I used to build a model from scratch using de_core_news_sm and everything worked fine: I had an accuracy around 70%. But now I am using BERT pretrained model instead and I'm getting 0% accuracy. I don't believe that it's working so bad, so I'm assuming that there is just a problem with my code. I might have missed something important but I can't figure out what. I used the code in this article as an example.

Function get_data() opens files with different categories, creates a tuple like this one (text, {'cats' : {'category1': 0, 'category2':1, ...}}), gathers all these tuples into one array, which is then being returned to the main function.

Function test(nlp) opens the file with test data, predicts categories for each line in the file and checks, whether the prediction was correct.

Again, everything worked just fine with de_core_news_sm, so I'm pretty sure that functions get_data() and test(nlp) are working fine. Code above looks like in example but still 0% accuracy.I don't understand what I'm doing wrong.","Received an answer to my question on GitHub and it looks like there must be some optimizer parameters specified, just like in this example.",,,,,,
60,Spacy,Security,leak,Security aspect,55841087,Python: Spacy and memory consumption,"1 - THE PROBLEM

I'm using ""spacy"" on python for text documents lemmatization. There are 500,000 documents having size up to 20 Mb of clean text.

The problem is the following: spacy memory consuming is growing in time till the whole memory is used.

2 - BACKGROUND

My hardware configuration: CPU: Intel I7-8700K 3.7 GHz (12 cores) Memory: 16 Gb SSD: 1 Tb GPU is onboard but is not used for this task

I'm using ""multiprocessing"" to split the task among several processes (workers). Each worker receives a list of documents to process. The main process performs monitoring of child processes. I initiate ""spacy"" in each child process once and use this one spacy instance to handle the whole list of documents in the worker.

3 - EXPECTATIONS

I have seen a good recommendation to build a separated server-client solution [here]Is possible to keep spacy in memory to reduce the load time?

Is it possible to keep memory consumption under control using ""multiprocessing"" approach?
","Memory problems when processing large amounts of data seem to be a known issue, see some relevant github issues:

https://github.com/explosion/spaCy/issues/3623
https://github.com/explosion/spaCy/issues/3556
Unfortunately, it doesn't look like there's a good solution yet.",,,,,,
61,Spacy,Security,leak,Security aspect,58956171,"Dataflow job failed after more than 6 hours with ""The worker lost contact with the service""?","I am using Dataflow to read data from BigQuery and then do NLP preprocessing using python. I am using Python 3 and SDK 2.16.0. I am using 100 workers (provite IP, private access and Cloud NAT) with workers in europe-west6 and endpoint in europe-west1. The BigQuery tables are in US. Test jobs were working without any issue but when trying to process the full table (32 GB), the job failed after 6h 40 min and it is hard to fully understand what is the underlying error.","After some test and after few plots for monitoring, it was clear that even if the length of the text was the sime the processing time started to augment rapidly (botton right plot)
Then it became clear that the issue was with SpaCy 2.1.8 (memory leak).

Using Spacy 2.2.3 fix the issue. Now the 32 Gb of data are processed in 4h30 without any issue.",,,,,,
62,GSON,Security,Security,Security aspect,13267258,Gson SecurityException: Can not make Class Constructor accessible,"I am using Gson to serialize/deserialize a class that contains a static nested class. The class looks like below.

public class ClassA {
private NestedClass nestedClass;

public NestedClass getNestedClass() {
return nestedClass;
}

public void setNestedClass(NestedClass nestedClass) {
this.nestedClass = nestedClass;
}

public static class NestedClass implements Serializable {

public NestedClass() {
}
}
}
The serialization works fine, but when i am trying to deserialize the json string back into an object i get the following SecurityException.

The exception seems to be coming from Java while trying to reflect the inner class. Has anyone been in that situation before?

Any help appreciated.",It appears you're running Gson on a JVM with a SecurityManager enabled. You'll need to disable the security manager to use Gson.,,,,,,
63,GSON,Security,Security,Security aspect,45793451,Kotlin not nullable value can be null?,"I have backend that return me some json.

I parse it to my class:

class SomeData(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String
)
Use gson converter factory:

Retrofit retrofit = new Retrofit.Builder()
  .baseUrl(ENDPOINT)
  .client(okHttpClient)
  .addConverterFactory(GsonConverterFactory.create(gson))
  .addCallAdapterFactory(RxJava2CallAdapterFactory.create())
  .build();
And in my interface:

interface MyAPI {
    @GET(""get_data"")
    Observable<List<SomeData>> getSomeData();
}
Then I retrieve data from the server (with rxJava) without any error. But I expected an error because I thought I should do something like this (to prevent GSON converter error, because notNullableValue is not present in my JSON response):

class SomeData @JvmOverloads constructor(
  @SerializedName(""user_name"") val name: String,
  @SerializedName(""user_city"") val city: String,
  var notNullableValue: String = """"
)
After the data is received from backend and parsed to my SomeData class with constructor without def value, the value of the notNullableValue == null.

As I understand not nullable value can be null in Kotlin?","Yes, that is because you're giving it a default value. Ofcourse it will never be null. That's the whole point of a default value.

Remove ="""" from constructor and you will get an error.

Edit: Found the issue. GSON uses the magic sun.misc.Unsafe class which has an allocateInstance method which is obviously considered very unsafe because what it does is skip initialization (constructors/field initializers and the like) and security checks. So there is your answer why a Kotlin non-nullable field can be null. Offending code is in com/google/gson/internal/ConstructorConstructor.java:223

Some interesting details about the Unsafe class: http://mishadoff.com/blog/java-magic-part-4-sun-dot-misc-dot-unsafe/",,,,,,
64,GSON,Security,Security,Security aspect,8403277,GSON vulnerabilities or exploits on deserialization to avoid,"I am planning to use gson's fromJson() method to parse a string coming from the browser. Are there any potential vulnerabilities associated with doing that? The data type I am converting to is relatively simple, a List and a boolean. But since gson uses reflection, is there something for me to watch out for?

For example, with older jvm (pre 6.24), a DOS attack could have been used against integers, where the integer parser would hang.

Can some clever json cause gson to start loading classes it should be?","The thing to watch with Gson is what type of builder you are using (see custom deserializer / serializer)

Gson has another weakness, when you are deserializing, (let's say using a custom one) you better check for the type of object you are passing (use instanceof).

Other main point: Gson will automatically convert the variable based on the type being passed.

ie. { ""var1"":1 , ""var2"":""1""} The first one will be converted into an integer, the second based to a string, therefore I would watch out on your objects transformation.",,,,,,
65,GSON,Security,Security,Security aspect,4981101,Gson Security concerns,I plan to use Gson (http://code.google.com/p/google-gson/) on my server and transfer JavaScript objects from the browser to Java objects on the server. I will use Ajax to POST the request. I am a little concerned that everybody will be able to post Java objects to my server VM. A Java object that runs in the server process will have the potential to do whatever it wants. How does Gson address that?,"gson/json is data only, not code. So noone can post code to your server, just data.

When you're deserializing json, you specify which class (or you just extract primitives or collections of primitives directly) to populate with data - noone can send you json that aribtarly instantiate any of your classes living on the server.",,,,,,
66,GSON,Security,Security,Security aspect,39176229,Deserializing to a templated class with Gson,"I have a class with a nested subclass
Currently I would like to be able to have a function that takes as a parameter the held class, and returns an appropriate instance of GenericClassHolder, i.e. something like my current code:
But this does not work in my current implementation, with the error

Unable to invoke no-args constructor for GenericClassHolder<T>. Register an InstanceCreator with Gson for this type may fix this problem.
How can I succinctly achieve my aforementioned goal of having a function that takes as input the specification for a class that may change and parses to this object accordingly? If it is not possible to achieve an elegant solution through a function, how can I achieve this goal without one?","I deployed this code in a Google Appengine instance, and due to some security reasons, I could not deserialize the way I was deserializing and replicate the same way on my local machine. The solution was to make the deserialized classes static; they were nested inside of an outer encapsulating class as well",,,,,,
67,GSON,Security,attack,Security aspect,53661408,Retrofit2 strange combination of different braces in front of request Body,"I am using Retrofit2 and I got stuck on the problem. I wrote simple entity for body:
also I wrote custom serializer for it:
DateTimeUtil:
and in request body somehow appears this:
How this "")]}'"" could be attached at front of my ""date"" json in request?
Only that I found is after JsonWriter do some magic in buffer.readByteString() it stores broken body.","I found where was the problem comes from:

Here my custom GsonBuilder:

private fun getGsonFactory(): Gson {
        return GsonBuilder()
            .setLenient()
            .serializeSpecialFloatingPointValues()
            .setLongSerializationPolicy(LongSerializationPolicy.DEFAULT)
            .generateNonExecutableJson()
            .enableComplexMapKeySerialization()
            .serializeNulls()
            .setPrettyPrinting()
            .registerTypeAdapter(CleaningProgress::class.java, CleaningProgressDeserializer())
            .create()
    }
The line of code below, add that "")]}'"" at front of request. Why am I add it to builder? I trust official documentation about this method:

Makes the output JSON non-executable in Javascript by prefixing the generated JSON with some special text. This prevents attacks from third-party sites through script sourcing.",,,,,,
68,Spacy,Feature,tokenization,tokenization feature,59500498,spacy tokenizer: is there a way to use regex as a key in custom exceptions for update_exc,"It is possible to add custom exceptions to spacy tokenizer. And these exceptions work fine. However, as far as I know, it's possible to use only strings as keys to match for those exceptions. It's done this way:
The only clue I found is: https://github.com/explosion/spaCy/issues/840

In that revision of tokenizer_exceptions.py there was some way to use regexps as keys for tokenizer exceptions(however, I haven't found any examples to do so)

But in current revisions, at least initial analysis hasn't shown any ways to do s

So is there a way to solve this task?

(input: regex as a key for exception, output - phone numbers with spaces inside)","No, there's no way to have regular expressions as tokenizer exceptions. The tokenizer only looks for exceptions as exact string matches, mainly for reasons of speed. The other difficulty for this kind of example is that tokenizer exceptions currently can't contain spaces. (Support for spaces is planned for a future version of spacy, but not regexes, which would still be too slow.)

I think the best way to do this would be to add a custom pipeline component at the beginning of the pipeline that retokenizes the document with the retokenizer: https://spacy.io/api/doc#retokenize. You can provide any required attributes like lemmas while retokenizing.",,,,,,
69,Spacy,Feature,tokenization,tokenization feature,50719732,Keeping all white spaces as tokens,"I have a question about whether there is a way to keep single white space as an independent token in spaCy tokenization.

For example if I ran:

import spacy

nlp = spacy.load(""en_core_web_sm"")
doc = nlp(""This is easy."")
toks = [w.text for w in doc]
toks
The result is

['This', 'is', 'easy', '.']
Instead, I would like to have something like

['This', ' ', 'is', ' ', 'easy', '.']
Is there are a simple way to do that?","spaCy exposes the token's whitespace as the whitespace_ attribute. So if you only need a list of strings, you could do:

token_texts = []
for token in doc:
   token_texts.append(token.text)
   if token.whitespace_:  # filter out empty strings
       token_texts.append(token.whitespace_)
If you want to create an actual Doc object out of those tokens, that's possible, too. Doc objects can be constructed with a words keyword argument (a list of strings to add as tokens). However, I'm not sure how useful that would be.",,,,,,
70,Spacy,Feature,tokenization,tokenization feature,50908667,Tokenization not working the same for both case.,"I have a document

doc = nlp('x-xxmessage-id:')
When I want to extract the tokens of this one I get 'x', 'xx', 'message' and 'id', ':'. Everything goes well. Then I create a new document

test_doc = nlp('id')
If I try to extract the tokens of test_doc, I will get 'i' and 'd'. Is there any way to get past this problem? Because I want to get the same token as above and this is creating problems in the text processing.","Just like language itself, tokenization is context-dependent and the language-specific data defines rules that tell spaCy how to split the text based on the surrounding characters. spaCy's defaults are also optimised for general-purpose text, like news text, web texts and other modern writing.

In your example, you've come across an interesting case: the abstract string ""x-xxmessage-id:"" is split on punctuation, while the isolated lowercase string ""id"" is split into ""i"" and ""d"", because in written text, it's most commonly an alternate spelling of ""I'd"" or ""i'd"" (""I could"", ""I would"" etc.). You can find the respective rules here.

If you're dealing with specific texts that are substantially different from regular natural language texts, you usually want to customise the tokenization rules or possibly even add a Language subclass for your own custom ""dialect"". If there's a fixed number of cases you want to tokenize differently that can be expressed by rules, another option would be to add a component to your pipeline that merges the split tokens back together.

Finally, you could also try using the language-independent xx / MultiLanguage class instead. It still includes very basic tokenization rules, like splitting on punctuation, but none of the rules specific to the English language.

from spacy.lang.xx import MultiLanguage
nlp = MultiLanguage()",,,,,,
71,Spacy,Feature,tokenization,tokenization feature,50330455,How to detokenize spacy text without doc context?,"I have a sequence to sequence model trained on tokens formed by spacy's tokenization. This is both encoder and decoder.

The output is a stream of tokens from a seq2seq model. I want to detokenize the text to form natural text.

Example:

Input to Seq2Seq: Some text

Output from Seq2Seq: This does n't work .

Is there any API in spacy to reverse tokenization done by rules in its tokenizer?","TL;DR I've written a code that attempts to do it, the snippet is below.

Another approach, with a computational complexity of O(n^2) * would be to use a function I just wrote. The main thought was ""What spaCy splits, shall be rejoined once more!""
Downsides:

In this approach you may easily merge ""do"" and ""nt"", as well as strip space between the dot ""."" and preceding word. This method is not perfect, as there are multiple possible combinations of sentences that lead to specific spaCy tokenization.

I am not sure if there is a method to fully detokenize a sentence when all you have is spaCy separated text, but this is the best I've got.

After having searched for hours on Google, only a few answers came along, with this very stack question being opened on 3 of my tabs on chrome ;), and all it wrote was basically ""don't use spaCy, use revtok"". As I couldn't change the tokenization other researchers chose, I had to develop my own solution. Hope it helps someone ;)",,,,,,
72,Spacy,Feature,POS,POS tagging feature,42830248,how to write spacy matcher of POS regex,"Spacy has two features I'd like to combine - part-of-speech (POS) and rule-based matching.

How can I combine them in a neat way?

For example - let's say input is a single sentence and I'd like to verify it meets some POS ordering condition - for example the verb is after the noun (something like noun**verb regex). result should be true or false. Is that doable? or the matcher is specific like in the example

Rule-based matching can have POS rules?

If not - here is my current plan - gather everything in one string and apply regex","Sure, simply use the POS attribute.

import spacy
nlp = spacy.load('en')
from spacy.matcher import Matcher
from spacy.attrs import POS
matcher = Matcher(nlp.vocab)
matcher.add_pattern(""Adjective and noun"", [{POS: 'ADJ'}, {POS: 'NOUN'}])

doc = nlp(u'what are the main issues')
matches = matcher(doc)",,,,,,
73,Spacy,Feature,POS,POS tagging feature,47856247,Extract verb phrases using Spacy,I have been using Spacy for noun chunks extraction using Doc.noun_chunks property provided by Spacy. How could I extract verb phrases from input text using Spacy library (of the form 'VERB ? ADV * VERB +' )?,"This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
This might help you.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The author is writing a new book.'
pattern = r'<VERB>?<ADV>*<VERB>+'
doc = textacy.Doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.pos_regex_matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

is writing
On how to highlight the verb phrases do check the link below.

Highlight verb phrases using spacy and html

Another Approach:

Recently observed Textacy has made some changes to regex matches. Based on that approach i tried this way.

from __future__ import unicode_literals
import spacy,en_core_web_sm
import textacy
nlp = en_core_web_sm.load()
sentence = 'The cat sat on the mat. He dog jumped into the water. The author is writing a book.'
pattern = [{'POS': 'VERB', 'OP': '?'},
           {'POS': 'ADV', 'OP': '*'},
           {'POS': 'VERB', 'OP': '+'}]
doc = textacy.make_spacy_doc(sentence, lang='en_core_web_sm')
lists = textacy.extract.matches(doc, pattern)
for list in lists:
    print(list.text)
Output:

sat
jumped
writing
I checked the POS matches in this links seems the result is not the intended one.

[https://explosion.ai/demos/matcher][1]

Did anybody try framing POS tags instead of Regexp pattern for finding Verb phrases?",,,,,,
74,Spacy,Feature,POS,POS tagging feature,58136715,Is there a way to determine part of speech patterns in a dataset of sentences?,I have a pandas dataframe with sentences and I'd like to determine part of speech patterns in each sentence. What's a good way to do this?,"The two most popular POS taggers for Python are NLTK and spaCy. spaCy is both faster and easier to use, and as you need to do more, spaCy also provides a nice API for interacting with wordnet, word Vectors, and other more advanced tools.

This is how you do POS reading with spaCy:

import spacy

nlp = spacy.load('en_core_web_sm')
doc = nlp(u'Apple is looking at buying U.K. startup for $1 billion')

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,
            token.shape_, token.is_alpha, token.is_stop)",,,,,,
75,Spacy,Feature,POS,POS tagging feature,34102420,POS tagging using spaCy,"I am trying to do POS tagging using the spaCy module in Python.
Here it returns crispy as a noun instead of an adjective. However, if I use a test sentence like

a=""we had crispy fries""
It recognizes that crispy is an adjective. Here is the output:

[(we , u'PRP'), (had , u'VBD'), (crispy , u'JJ'), (fries, u'NNS')]
I think the primary reason why crispy wasn't tagged as an adjective in the first case was because dosa was tagged as 'NN' whereas fries was tagged as 'NNS' in the second case.

Is there any way I can get crispy to be tagged as an adjective in the second case too?","TL;DR: You should accept the occasional error.

Details:

Spacy's tagger is statistical, meaning that the tags you get are its best estimate based on the data it was shown during training. I would guess those data did not contain the word dosa. The tagger had to guess, and guessed wrong. There isn't an easy way to correct its output, because it is not using rules or anything you can modify easily. The model has been trained on a standard corpus of English, which may be quite different to the kind of language you are using it for (domain). If error rate is too high for your purposes, you can re-train the model using domain-specific data. This will be very laborious though. Ask yourself what you are trying to achieve and whether 3% error rate in PoS tagging is the worst of your problems.

In general, you shouldn't judge the performance of a statistical system on a case-by-case basis. The accuracy of modern English PoS taggers is around 97%, which is roughly the same as the average human. You will inevitably get some errors. However, the errors of the model will not be the same as the human errors, as the two have ""learnt"" how to solve the problem in a different way. Sometimes the model will get confused by things you and I consider obvious, e.g. your example. This doesn't mean it is bad overall, or that PoS tagging is your real problem.",,,,,,
76,Spacy,Feature,dependency parsing,dependency parsing feature,55521858,"spacy noun-chunking creates unexpected lemma, pos, tag, and dep","I am using spacy to parse documents and unfortunately I am unable to process noun chunks the way I would have expected them to be processed. 

I expected a ""lemma"" would be the phrase ""the big dog"" with plural form changed to singular and the phrase would be ""pos"" of ""NOUN"", a ""tag"" of ""NNS"", and a ""dep"" of ""nsubj"".

Is this the correct behaviour, or am I using spacy incorrectly? If I am using spacy incorrectly, please let me know the correct manner in which to perform this task.","There are a few things to consider here

Lemmatisation is token based
POS tagging and dependency parsing is predictive
You probably will get the big dog if you take the lemma_ attribute for each token. It does not update the token pos of you use the attribute.

Also, since dependency parsing and POS tagging is trained in a predictive model, it is not guaranteed to always be ""right"" from a human linguistic perspective.

Other than the lemma issue, it seems you are using spacy correct",,,,,,
77,Spacy,Feature,dependency parsing,dependency parsing feature,43397502,Rule-based matcher of entities with spacy,"I want to use the python library spacy for matching tokens in a text (adding a labels as a semantic reference). Then, I want to use the matches to extract relations between the tokens. My first was to make use of the matcher.add and matcher.add_pattern of spacy. The matcher.add is working fine and I can find tokens, my code until now:

I have tried in different ways to make this works but is obviously not, I guess there is something wrong with my understanding of matcher.add_pattern.

Can some please put me in the right direction how to do this with spacy?
Is it possible to add regex here to find patterns, how?
How can I add more than one token with the same label, or somehow create a list of tokens for the same label, eg. ""FINANCE""?
I will appreciate any comments.","Your matcher would identify the tokens, but to find relations between them you will have to do dependency parsing. Here is visual example from spacy :
You can then traverse the tree to find relations between the tokens. https://spacy.io/docs/usage/dependency-parse#navigating

The dep (enum) and dep_ (verbose name) attribute of each token would give you the relationships with its child",,,,,,
78,Spacy,Feature,dependency parsing,dependency parsing feature,39258476,Collocations with spaCy,"I've been using NLTK for finding collocations, or n-grams, and have recently discovered the spaCy module for NLP. I've only just begun familiarizing myself with it and have, thus far, seen little mention for supported collocation functions.

Can spaCy be used to find collocations directly?

I have read through the documentation, but haven't seen mention.","Collocations detection also can be based on dependency parsing, but spaCy do not have support to do it. You can use spaCy as part of an approach, but not directly.

may you also consider gensim: https://radimrehurek.com/gensim/models/phrases.html

I hope it can help you",,,,,,
79,Spacy,Feature,dependency parsing,dependency parsing feature,55498881,Vectorized form of cleaning function for NLP,"I made the following function to clean the text notes of my dataset :
Problem is when I want to clean all my dataset text, it take hour and hour. (my dataset is 70k row and between 100 to 5000 words per row)

I tried to use swifter to run the apply method on multiplethread like that : data.note_line_comment.swifter.apply(clean)

But it doesn't made really better as it took almost one hour.

I was wondering if there is any way to make a vectorized form of my function or maybe and other way to speed up the process. Any idea ?","Short answer

This type of problem inherently takes time.

Long answer

Use regular expressions
Change the spacy pipeline
The more information about the strings you need to make a decision, the longer it will take.

Good news is, if your cleaning of the text is relatively simplified, a few regular expressions might do the trick.

Otherwise you are using the spacy pipeline to help remove bits of text which is costly since it does many things by default:

Tokenisation
Lemmatisation
Dependency parsing
NER
Chunking
Alternatively, you can try your task again and turn off the aspects of the spacy pipeline you don't want which may speed it up quite a bit.

For example, maybe turn off named entity recognition, tagging and dependency parsing...

nlp = spacy.load(""en"", disable=[""parser"", ""tagger"", ""ner""])
Then try again, it will speed up.",,,,,,
80,Spacy,Feature,Lemmatization,Lemmatization feature,60638828,SpaCy use Lemmatizer as stand-alone component,"I want to use SpaCy's lemmatizer as a standalone component (because I have pre-tokenized text, and I don't want to re-concatenate it and run the full pipeline because SpaCy will most likely tokenize differently in some cases).

I found the lemmatizer in the package but I somehow needs to load the dictionaries with the rules to initialize this Lemmatizer. These files must be somewhere in the model of the English or German model, right? I couldn't find them there.

from spacy.lemmatizer import Lemmatizer
where do the LEMMA_INDEX, etc. files are comming from?
lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)
I found a similar question here: Spacy lemmatizer issue/consistency but this one did not entirely answer how to get these dictionary files from the model. The spacy.lang.* parameter seems to no longer exist in newer versions.","Here's an extracted bit of code I had, that used the SpaCy lemmatizer by itself. I'm not somewhere I can run it so it might have a small bug or two if I made an editing mistake.

Note that in general, you need to know the upos for the word in order to lemmatize correctly. This code will return all the possible lemmas but I would advise modifying it to pass in the correct upos for your word.

class SpacyLemmatizer(object):
    def __init__(self, smodel):
        import spacy
        self.lemmatizer = spacy.load(smodel).vocab.morphology.lemmatizer

    # get the lemmas for every upos
    def getLemmas(self, entry):
        possible_lemmas = set()
        for upos in ('NOUN', 'VERB', 'ADJ', 'ADV'):
            lemmas = self.lemmatizer(entry, upos, morphology=None)
            lemma = lemmas[0]    # See morphology.pyx::lemmatize
            possible_lemmas.add( lemma )
        return possible_lemmas",,,,,,
81,Spacy,Feature,Lemmatization,Lemmatization feature,59636002,Spacy lemmatization of a single word,"I am trying to get the lemmatized version of a single word. Is there a way using ""spacy"" (fantastic python NLP library) to do this.
Below is the code I have tried but this does not work):

from spacy.lemmatizer import Lemmatizer
from spacy.lookups import Lookups
lookups = Lookups()
lemmatizer = Lemmatizer(lookups)
word = ""ducks""
lemmas = lemmatizer.lookup(word)
print(lemmas)
The result I was hoping for was that the word ""ducks"" (plural) would result in ""duck"" (singular). Unfortunately, ""ducks"" (plural) is returned.

Is there a way of doing this?

NOTE: I realize that I could process an entire string of words from a document (nlp(document)) and then find the required token and then get its lemma (token.lemma_), but the word(s) I need to lemmatize are somewhat dynamic and are not able to be processed as a large document.","Otherwise you can keep using spaCy, but after disabling parser and NER pipeline components:

Start by downloading a 12M small model (English multi-task CNN trained on OntoNotes)
$ python -m spacy download en_core_web_sm
Python code
import spacy
nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner']) # just keep tagger for lemmatization
"" "".join([token.lemma_ for token in nlp('ducks')])
Output

> duck",,,,,,
82,Spacy,Feature,Lemmatization,Lemmatization feature,74175424,"Is Spacy lemmatization not working properly or does it not lemmatize all words ending with ""-ing""?","When I run the spacy lemmatizer, it does not lemmatize the word ""consulting"" and therefore I suspect it is failing.

Here is my code:

nlp = spacy.load('en_core_web_trf', disable=['parser', 'ner'])
lemmatizer = nlp.get_pipe('lemmatizer')
doc = nlp('consulting')
print([token.lemma_ for token in doc])
And my output:

['consulting']","The spaCy lemmatizer is not failing, it's performing as expected. Lemmatization depends heavily on the Part of Speech (PoS) tag assigned to the token, and PoS tagger models are trained on sentences/documents, not single tokens (words). For example, parts-of-speech.info which is based on the Stanford PoS tagger, does not allow you to enter single words.

In your case, the single word ""consulting"" is being tagged as a noun, and the spaCy model you are using deems ""consulting"" to be the appropriate lemma for this case. You'll see if you change your string instead to ""consulting tomorrow"", spaCy will lemmatize ""consulting"" to ""consult"" as it is tagged as a verb (see output from the code below). In short, I recommend not trying to perform lemmatization on single tokens, instead, use the model on sentences/documents as it was intended.",,,,,,
83,Spacy,Feature,Lemmatization,Lemmatization feature,63693463,SpaCy lemmatizer removes capitalization,"I would like to lemmatize some textual data in Hungarian language and encountered a strange feature in spaCy. The token.lemma_ function works well in terms of lemmatization, however, it returns some of the sentences without first letter capitalization. This is quite annoying, as my next function, unnest_stences (R) requires first capital letters in order to identify and break the text down into individual sentences. 

When I pass my original data to the function, it returns some sentences with upercase first letters, others with lowercase letters. For some strange reason I couldn't reproduce that pattern above, but I guess the main point is visible. The function does not work as expected.

Any ideas how I could fix this?

I'm using Jupyter Notebook, Python 2.7, Win 7 and a Toshiba laptop (Portégé Z830-10R i3-2367M).","Lowercasing is the expected behavior of spaCy's lemmatizer for non-proper-noun tokens.

One workaround is to check if each token is titlecased, and convert to original casing after lemmatizing (only applies to the first character).

import spacy

nlp = spacy.load('en_core_web_sm')

text = 'This is a test sentence.'
doc = nlp(text)
newtext = ' '.join([tok.lemma_.title() if tok.is_title else tok.lemma_ for tok in doc])
print(newtext)
# This be a test sentence .",,,,,,
84,Spacy,Feature,NER,NER feature,57536896,Custom entity ruler with SpaCy did not return a match,"This link shows how to create custom entity ruler.

I basically copied and modified the code for another custom entity ruler and used it to find a match in a doc as follows:
However, when I run the code (in Jupyter notebook), nothing returned.

Could you please tell me:

If the code returned nothing, did it mean no match was found?
Why couldn't my code find a match although it's almost identical to the original (except for the patterns added to the ruler)? What did I do wrong?","The problem is an interaction between the NER component provided in the English model and your EntityRuler component. The NER component finds 2 as a number (CARDINAL) and there's a restriction that entities aren't allowed to overlap, so the EntityRuler component doesn't find any matches.

You can either add your EntityRuler before the NER component:

nlp.add_pipe(ruler, before='ner')
Or tell the EntityRuler that it's allowed to overwrite existing entities:

ruler = EntityRuler(nlp, overwrite_ents=True)",,,,,,
85,Spacy,Feature,NER,NER feature,65220447,Add new named entity to Spacy's en_core_web_sm model?,"I'm following the example here on training a new entity type:

https://spacy.io/usage/training#example-new-entity-type

It works fine when I don't pass an existing model to it, and will correctly create a new model, which recognizes my new named entity e.g.

 python.exe train-new-entity-type.py
It also works fine when I pass an existing model to it (created by running it once before), and will correctly load the model in the dir/my_model dir, which still recognizes my new named entity e.g.

 python.exe train-new-entity-type.py -m dir/my_model
However, I want to train a new entity type, and add that to spacy's existing model, so that spacy will recognize it's own supported named entities as well as my new entity type, so I tried:

 python.exe train-new-entity-type.py -m en_core_web_sm
However, this didn't seem to work. Spacy's own supported named entities were recognized but they were not correct (vs just using the en_core_web_sm on its own without adding my new entity type to it), and my new entity type was no longer recognized at all.

Am I doing something wrong? Is this possible (adding named entities to en_core_web_sm)?","Read about the ""catastrophic forgetting"" problem when updating an existing model: https://spacy.io/usage/training#ner

It can be tricky to update an existing model, so it might be easier to train a separate model for your new entity type and add the NER component to the en_core_web_sm pipeline with a custom name. The main thing to watch out for is that you need to make sure the models are loaded with the same vocab so that you don't run into problems with the string store:

import spacy
nlp = spacy.load(""en_core_web_sm"")
custom_nlp = spacy.load(""my_model"", vocab=nlp.vocab)
nlp.add_pipe(custom_nlp.get_pipe(""ner""), name=""my_ner"", before=""ner"")
Where you add it in the pipeline (before/after the existing ner) will determine which entity spans have priority, since the ner component won't modify existing entity spans.",,,,,,
86,Spacy,Feature,NER,NER feature,68682465,pattern match issue with SpaCy,"I have the following script using a pattern to match the postcode. However, depending on how I supply the postcode string seems to affect how it is matched.

It would appear that for some reason the first postcode is never matched. In the last example both ""RH5 0XE"" & ""PO01 0IO"" should match, as you can see ""RH5 0XE"" matched in the previous run.

Any ideas on how I can improve the match rate?","Put the entity_ruler before ner in the pipeline so that its matches have priority over the CARDINAL spans from ner, or alternatively you can set it to overwrite overlapping entities with the overwrite_ents setting.",,,,,,
87,Spacy,Feature,NER,NER feature,46934523,How to get spaCy NER probability,"I want to combine spaCy's NER engine with a separate NER engine (a BoW model). I'm currently comparing outputs from the two engines, trying to figure out what the optimal combination of the two would be. Both perform decently, but quite often spaCy finds entities that the BoW engine misses, and vice versa. What I would like is to access a probability score (or something similar) from spaCy whenever it finds an entity that is not found by the BoW engine. Can I get spaCy to print out its own probability score for a given entity it has found? As in, ""Hi, I'm spaCy. I've found this token (or combination of tokens) that I'm X% certain is an entity of type BLAH."" I want to know that number X every time spaCy finds an entity. I imagine there must be such a number somewhere internally in spaCy's NER engine, plus a threshold value below which the possible entity is not flagged as an entity, and I'd like to know how to get my hands on that number. Thanks in advance.","Actually, there is an issue for that.

The author of the library, suggests there (among others) the following solution:

Beam search with global objective. This is the standard solution: use a global objective, so that the parser model is trained to prefer parses that are better overall. Keep N different candidates, and output the best one. This can be used to support confidence by looking at the alternate analyses in the beam. If an entity occurs in every analysis, the NER is more confident it's correct.

mportant note: The outputs you will get here are probably different from the outputs you would get using the Standard NER and not the beam search alternative. However, the beam search alternative provides you a metric of confidence that as I understand from your question is useful for your case.
",,,,,,
88,Spacy,Feature,Entity linking,Entity linking feature,59316859,Displaying the description of entity from kb id in spacy entity linking,"I have successfully trained a spacy entity linking model(obviously by limiting the data).

my question is how to display the description of entity from kb as output?

import spacy
nlp = spacy.load(r""D:\el model\nlp"")
doc = nlp(""Amir Khan is a great boxer"")
ents = [(e.text, e.label_, e.kb_id_) for e in doc.ents]
print(ents) ","As said by Sofie Van Landeghem(Spacy Entity Linking Representative). The descriptions are currently not stored in the KB itself because of performance reasons. However, from the intermediary results during processing, you should have a file entity_descriptions.csv that maps the WikiData ID to its description in a simple tabular format.",,,,,,
89,Spacy,Feature,Entity linking,Entity linking feature,59050554,Error running Spacy entity linking example,"I was trying the entity linking example in spacy.
Using this example to train the entity linker and generating the knowledge base for the same with this example.

I can create a knowledge base with the available en_core_web_md, this is the output for the same.
When I try to train the entity linker with the knowledge base from above, I get this error.
File ""pipes.pyx"", line 1219, in spacy.pipeline.pipes.EntityLinker.update
I did follow the instructions specified here. I used the en_core_web_md to create the knowledge base since I do not have a pre-trained model.

I did not write any custom code just trying to run this example, Can someone point me to the right direction.
","This was asked and answered in the following issue on spaCy's GitHub.

It looks like the script no longer worked after a refactor of the entity linking pipeline as it now expects either a statistical or rule-based NER component in the pipeline.

The new script adds such an EntityRuler to the pipeline as an example. I.e.,

# Add a custom component to recognize ""Russ Cochran"" as an entity for the example training data.
# Note that in a realistic application, an actual NER algorithm should be used instead.
ruler = EntityRuler(nlp)
patterns = [{""label"": ""PERSON"", ""pattern"": [{""LOWER"": ""russ""}, {""LOWER"": ""cochran""}]}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, this can be replaced with your own statistical NER model.",,,,,,
90,Spacy,Feature,Entity linking,Entity linking feature,60922171,Spacy - entity linker - why is the predict score a combination of prob and cosine sim?,"I was going through the predict method for the entity linker pipe under spacy, and for some reason the score is defined as the following :

scores = prior_probs + sims - (prior_probs*sims)

Link here

Anybody has experience with this / knows where this formula comes from?

Thanks!","It is taken from Entity Linking via Joint Encoding of Types, Descriptions, and Context section 4 equation 2.

I don't feel confident enough though in explaining the formula in detail, on overall the purpose is to combine probability scores for entitiy candidates derived from external knowledge based resources (KB in the paper), which are the prior probabilities, and scores estimated with a sentence encoder, used to encode the mention to link along with its context, sims in the formula because they compute cosine similarity between the encoded mention vector and all entity candidates (which is why this formula is used only if ""incl_context"" is true).",,,,,,
91,Spacy,Feature,Entity linking,Entity linking feature,60809394,Can spaCy link only named entities?,"I'd would like to extract important entities and link them to Wikipedia entities. I've trained spaCy on a small sample of Wikipedia/WikiData and run entity linking on the review:

[('Tofu', 'PERSON', 'Q177378'), 
('Nyquil', 'WORK_OF_ART', 'NIL')]
I'd like other entities to be extracted and linked as well, e.g.:

kimchi -> Kimchi
cold -> Common cold
healing -> medicine 
medically -> medicine
It looks like spaCy can link only named entities. I've tried to explicitly list other entities as named (which obviously does not scale well):

ruler = EntityRuler(nlp)
patterns = [{""label"": ""ORG"", ""pattern"": ""kimchi""}, {""label"": ""ORG"", ""pattern"": ""cold""}]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler)
However, spaCy does not seem to link new entities at all:

[ ('Tofu', 'PERSON', 'Q177378'),
  ('cold', 'ORG', ''),
  ('Nyquil', 'WORK_OF_ART', 'NIL'),
  ('kimchi', 'ORG', '')]
How can I make Spacy recognize also other entities?
Should this be done before training entity linking model or can be done with already trained model?
Is spaCy the right tool for my task at all?","In theory it's possible. First, you'll need to make sure you have a component that tags these kind of entities. You could train an NER model for this, but be aware that its performance might not be as good on things like ""cold"" than it would be for actual named entities like ""London"".

To create the Knowledge Base and the Entity Linker from Wikipedia/Wikidata, the example scripts are not limited to named entities - they attempt to parse anything that appears in an intra-wiki link. If the word ""cold"" gets linked to the page ""Common cold"", it should be able to learn it. The exact entities that are stored in the KB and that are used for training the EL model, depend on which entities are found by your entity recognizer component. So if you adjust that according to your use-case, the entity linking component will follow automatically.",,,,,,
92,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,56416641,separate texts into sentences NLTK vs spaCy,"I want to separate texts into sentences.

looking in stack overflow I found:

WITH NLTK

from nltk.tokenize import sent_tokenize
text=""""""Hello Mr. Smith, how are you doing today? The weathe is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard""""""
tokenized_text=sent_tokenize(text)
print(tokenized_text)
WITH SPACY

from spacy.lang.en import English # updated

raw_text = 'Hello, world. Here are two sentences.'
nlp = English()
nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated
doc = nlp(raw_text)
sentences = [sent.string.strip() for sent in doc.sents]
The question is what in the background for spacy having to do it differently with a so called create_pipe. Sentences are important for training your own word embedings for NLP. There should be a reason why spaCy does not include directly out of the box a sentence tokenizer.

Thanks.

NOTE: Be aware that a simply .split(.) does not work, there are several decimal numbers in the text and other kind of tokens containing '.'","By default, spaCy uses its dependency parser to do sentence segmentation, which requires loading a statistical model. The sentencizer is a rule-based sentence segmenter that you can use to define your own sentence segmentation rules without loading a model.

If you don't mind leaving the parser activated, you can use the following code:

import spacy
nlp = spacy.load('en_core_web_sm') # or whatever model you have installed
raw_text = 'Hello, world. Here are two sentences.'
doc = nlp(raw_text)
sentences = [sent.text.strip() for sent in doc.sents]",,,,,,
93,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,51725599,training sentence tokenizer in spaCy,"I'm trying to tokenize sentences using spacy.

The text includes lots of abbreviations and comments which ends with a period. Also, the text was obtained with OCR and sometimes there are line breaks in the middle of sentences. Spacy doesn't seem to be performing so well in these situations.

I have extracted some examples of how I want these sentences to be split. Is there any way to train spacy's sentence tokenizer?","Spacy is a little unusual in that the default sentence segmentation comes from the dependency parser, so you can't train a sentence boundary detector directly as such, but you can add your own custom component to the pipeline or pre-insert some boundaries that the parser will respect. See their documentation with examples: Spacy Sentence Segmentation

For the cases you're describing it would potentially be useful also be able to specify that a particular position is NOT a sentence boundary, but as far as I can tell that's not currently possible.",,,,,,
94,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,50742516,How to get the index of a token in a sentence in spaCy?,"Is there an elegant way to get the index of a word/token in its sentence? I am aware of the attributes for tokens https://spacy.io/api/token#attributes The i attribute returns the index within the whole parent document. But the parent document can contain multiple sentences.

Example:

""This is an example. This is another example.""
What I need is both ""This"" to be returned as index 0, both ""is"" to be returned as index 1 etc...","A spaCy Doc object also lets you iterate over the doc.sents, which are Span objects of the individual sentence. To get a span's start and end index in the parent document you can look at the start and end attribute. So if you iterate over the sentences and subtract the sentence start index from the token.i, you get the token's relative index within the sentence:

for sent in doc.sents:
    for token in sent:
        print(token.text, token.i - sent.start)
The default sentence segmentation uses the dependency parse, which is usually more accurate. However, you can also plug in a rule-based or entirely custom solution (see here for details).",,,,,,
95,Spacy,Feature,Sentence segmentation,Sentence segmentation feature,57779549,Converting Spacy generated dependency into CoNLL format cannot handle more than one ROOT?,"I used the SpaCy library to generate dependencies and save it into a CoNLL format using the code below.

This works, but there are some sentences in my dataset that get splits into two by Spacy because they have two ROOTS. This results in having two fields for one sentence in the CoNLL format.

Example: A random sentence from my dataset is : ""teanna trump probably cleaner twitter hoe but""

Is there a way to save it all in one field instead of two even though it has two ROOTS so that 'but' becomes 7th item in field number 1? Which means it would look like this instead","I'd recommend using (or adapting) the textacy CoNLL exporter to get the right format, see: How to generate .conllu from a Doc object?

Spacy's parser is doing sentence segmentation and you're iterating over doc.sents, so you'll see each sentence it exported separately. If you want to provide your own sentence segmentation, you can do that with a custom component, e.g.:

def set_custom_boundaries(doc):
    for token in doc[:-1]:
        if token.text == ""..."":
            doc[token.i+1].is_sent_start = True
    return doc

nlp.add_pipe(set_custom_boundaries, before=""parser"")
Details (especially about how to handle None vs. False vs. True): https://spacy.io/usage/linguistic-features#sbd-custom

Spacy's default models aren't trained on twitter-like text, the parser probably won't perform well with respect to sentence boundaries here.

(Please ask unrelated questions as separate questions, and also take a look at spacy's docs: https://spacy.io/usage/linguistic-features#special-cases)",,,,,,
96,Spacy,Feature,Text classification,Text classification feature,48834832,How do I create gold data for TextCategorizer training?,"I want to train a TextCategorizer model with the following (text, label) pairs.

Label COLOR:

The door is brown.
The barn is red.
The flower is yellow.
Label ANIMAL:

The horse is running.
The fish is jumping.
The chicken is asleep.
I am copying the example code in the documentation for TextCategorizer.

textcat = TextCategorizer(nlp.vocab)
losses = {}
optimizer = nlp.begin_training()
textcat.update([doc1, doc2], [gold1, gold2], losses=losses, sgd=optimizer)
The doc variables will presumably be just nlp(""The door is brown."") and so on. What should be in gold1 and gold2? I'm guessing they should be GoldParse objects, but I don't see how you represent text categorization information in those.","According to this example train_textcat.py it should be something like {'cats': {'ANIMAL': 0, 'COLOR': 1}} if you want to train a multi-label model. Also, if you have only two classes, you can simply use {'cats': {'ANIMAL': 1}} for label ANIMAL and {'cats': {'ANIMAL': 0}} for label COLOR.

You can use the following minimal working example for a one category text classification;",,,,,,
97,Spacy,Feature,Text classification,Text classification feature,62092445,Is it possible to improve spaCy's similarity results with custom named entities?,"I've found that spaCy's similarity does a decent job of comparing my documents using ""en_core_web_lg"" out-of-the-box.

I'd like to tighten up relationships in some areas and thought adding custom NER labels to the model would help, but my results before and after show no improvements, even though I've been able to create a test set of custom entities.

Now I'm wondering, was my theory completely wrong, or could I simply be missing something in my pipeline?

If I was wrong, what's the best approach to improve results? Seems like some sort of custom labeling should help.","I found my solution was nestled in this tutorial: Text Classification in Python Using spaCy, which generates a BoW matrix for spaCy's text data by using SciKit-Learn's CountVectorizer.

I avoided sentiment analysis tutorials, due to binary classification, since I need support for multiple categories. The trick was to set multi_class='auto' on the LogisticRegression linear model, and to use average='micro' on the precision score and precision recall, so all my text data, like entities, were leveraged:

classifier = LogisticRegression(solver='lbfgs', multi_class='auto')
and...

print(""Logistic Regression Accuracy:"",metrics.accuracy_score(y_test, predicted))
print(""Logistic Regression Precision:"",metrics.precision_score(y_test, predicted,average='micro'))
print(""Logistic Regression Recall:"",metrics.recall_score(y_test, predicted,average='micro'))
Hope this helps save someone some time!",,,,,,
98,Spacy,Feature,Rule-based matching,Rule-based matching feature,72346631,Highlight text parts based on labels,"thanks to fellow stackoverflowrians I have data labels that I would like to high light in the text:

eg. I have product description

Description: Tampered black round grey/natural swing with yellow load-bearing left hook
Features were extracted as

colors=['black','grey','natural','yellow']
shape = ['round']
direction= ['left']
In Spacy it is possible to highlight the features like this

enter image description here

Is there any possibility to highlight it also like this from the data I have as labels? So that I have labels shown in the text too? I dont know if Spacy is the good tool or any other is better?
","I'm not entirely sure what you're asking, but you can put entities of your own on the spaCy Doc object and pass them to Displacy.

To simply set entities manually, you can do this:

doc = nlp(...)
span = doc[0:1] # whatever span of the doc you want to highlight
span.ent_label_ = ""COLOR"" # the label you want
ents = [span] # in reality you could do more than one
doc.ents = ents
If you have word list and need to look for words, you can use rule-based matching with an EntityRuler. Check the rule-based matching guide.",,,,,,
99,Spacy,Feature,Rule-based matching,Rule-based matching feature,76011075,"Entity Ruler cannot recognize ""Frankfurt am Main"" as Geo-Political Entity (GPE)","I try to add a new rule in Named Entity Recognition so that Spacy will label the phrase ""Frankfurt am Main"" as GPE.

nlp = spacy.load(""en_core_web_sm"")
ruler = nlp.add_pipe(""entity_ruler"")
patterns = [{""label"": ""ORG"", ""pattern"": ""MyCorp Inc.""},{""label"": ""GPE"", ""pattern"": ""Frankfurt am Main""}]
ruler.add_patterns(patterns)
doc = nlp(""MyCorp Inc. is a company in Frankfurt am Main"")
print([(ent.text, ent.label_) for ent in doc.ents])
But Spacy still label the word ""Frankfurt"" as GPE instead of the whole phrase ""Frankfurt am Main"":

[('MyCorp Inc.', 'ORG'), ('Frankfurt', 'GPE')]
I want to know how I can adjust spacy so that it can label ""Frankfurt am Main"" as GPE.","Put the entity_ruler before ner so that its entity spans have priority over the ner entities.

nlp.add_pipe(""entity_ruler"", before=""ner"")
See: https://spacy.io/usage/rule-based-matching#entityruler-usage",,,,,,
100,Spacy,Feature,Rule-based matching,Rule-based matching feature,57479028,spaCy NLP custom rule matcher,"I am begginer with NLP. I am using spaCy python library for my NLP project. Here is my requirement,

I have a JSON File with all country names. Now i need to parse and get goldmedal count for the each countries in the document. Given below the sample sentence,

""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics""
I am able to fetch country names but not it medal count. Given below my code. Please help to proceed further.","Spacy provides Rule-based matching which you could use.

They can be used as follows:

import spacy
from spacy.pipeline import EntityRuler
nlp = spacy.load('en_core_web_sm', disable=[""ner"", ""parser""])

countries = ['Czech Republic', 'Slovakia']
ruler = EntityRuler(nlp)
for a in countries:
    ruler.add_patterns([{""label"": ""country"", ""pattern"": a}])
nlp.add_pipe(ruler)


doc = nlp(""Czech Republic won 5 gold medals at olympics. Slovakia won 0 medals olympics"")

with doc.retokenize() as retokenizer:
    for ent in doc.ents:
        retokenizer.merge(doc[ent.start:ent.end])


from spacy.matcher import Matcher
matcher = Matcher(nlp.vocab)
pattern =[{'ENT_TYPE': 'country'}, {'lower': 'won'},{""IS_DIGIT"": True}]
matcher.add('medal', None, pattern)
matches = matcher(doc)


for match_id, start, end in matches:
    span = doc[start:end]
    print(span)

output:

Czech Republic won 5
Slovakia won 0
The above code should get you started. Naturally, you will have to write your own more complex rules so that you can handle cases like: ""Czech Republic unsurprisingly won 5 gold medals at olympics in 1995."" And other more complex sentence structures.",,,,,,